v.srinivasa rajkumar educational technology	0.0	0.0	0.0	2.0	0.0000000000	False
rajkumar educational technology i.i.t.delhi	0.0	0.0	0.0	2.0	0.0000000000	False
educational technology i.i.t.delhi presents	0.0	0.0	0.0	2.0	0.0000000000	False
i.i.t.delhi presents a video	0.0	0.0	0.0	2.0	0.0000000000	False
video course on programming	0.0	0.0	0.0	2.0	0.0000000000	False
languages by dr.s.arun kumar	0.0	0.0	0.0	2.0	0.0000000000	False
high level programming languages	0.0	1.0	0.0	2.0	0.0000000000	True
imperative and functional languages	0.0	0.0	0.0	2.0	0.0000000000	False
respects so imperative language	0.0	0.0	0.0	2.0	0.0000000000	False
based languages where state	0.0	0.0	0.0	2.0	0.0000000000	False
languages where state updation	0.0	0.0	0.0	2.0	0.0000000000	False
languages are really value	0.0	0.0	0.0	2.0	0.0000000000	False
languages in the notion	0.0	0.0	0.0	2.0	0.0000000000	False
variables in functional languages	0.0	0.0	0.0	2.0	0.0000000000	False
mathematics whereas the notion	0.0	0.0	0.0	2.0	0.0000000000	False
notion of the variables	0.0	0.0	0.0	4.0	0.0000000000	False
physics which can change	0.0	0.0	0.0	2.0	0.0000000000	False
quantities like acceleration velocity	0.0	0.0	0.0	2.0	0.0000000000	False
state based languages means	0.0	0.0	0.0	2.0	0.0000000000	False
trimaxes though unlike physics	0.0	0.0	0.0	2.0	0.0000000000	False
case of functional languages	0.0	0.0	0.0	2.0	0.0000000000	False
functional languages the notion	0.0	0.0	0.0	2.0	0.0000000000	False
execution of a program	0.0	0.0	0.0	2.0	0.0000000000	False
languages is really concentrated	0.0	0.0	0.0	2.0	0.0000000000	False
hundreds of programming languages	0.0	0.0	0.0	4.0	0.0000000000	False
history you will find	0.0	0.0	0.0	2.0	0.0000000000	False
sixties a large portion	0.0	0.0	0.0	2.0	0.0000000000	False
features so which means	0.0	0.0	0.0	2.0	0.0000000000	False
means these these represent	0.0	0.0	0.0	2.0	0.0000000000	False
represent the basic control	0.0	0.0	0.0	2.0	0.0000000000	False
control structures the exploration	0.0	0.0	0.0	2.0	0.0000000000	False
data structures in order	0.0	0.0	0.0	2.0	0.0000000000	False
obtain clean readable programs	0.0	0.0	0.0	2.0	0.0000000000	False
efficiently implement able programs	0.0	0.0	0.0	2.0	0.0000000000	False
programs um efficient running	0.0	0.0	0.0	2.0	0.0000000000	False
fixed in the seventies	0.0	0.0	0.0	2.0	0.0000000000	False
exploration of programming languages	0.0	0.0	0.0	2.0	0.0000000000	False
languages was in terms	0.0	0.0	0.0	2.0	0.0000000000	False
pascal most important feature	0.0	0.0	0.0	2.0	0.0000000000	False
combines the module features	0.0	0.0	0.0	2.0	0.0000000000	False
module features of modula	0.0	0.0	0.0	2.0	0.0000000000	False
important feature exception handling	0.0	0.0	0.0	2.0	0.0000000000	False
feature exception handling generics	0.0	0.0	0.0	2.0	0.0000000000	False
handling generics or polymorphism	0.0	0.0	0.0	2.0	0.0000000000	False
clu is a module	0.0	0.0	0.0	2.0	0.0000000000	False
sense the basic control	0.0	0.0	0.0	2.0	0.0000000000	False
basic control structure remain	0.0	0.0	0.0	2.0	0.0000000000	False
structures in these languages	0.0	0.0	0.0	2.0	0.0000000000	False
mark denote the decendency	0.0	0.0	0.0	2.0	0.0000000000	False
structures so from simula	0.0	0.0	0.0	2.0	0.0000000000	False
small talk eighty control	0.0	0.0	0.0	2.0	0.0000000000	False
talk eighty control structures	0.0	0.0	0.0	2.0	0.0000000000	False
control structures or syntax	0.0	0.0	0.0	2.0	0.0000000000	False
simula which was extended	0.0	0.0	0.0	2.0	0.0000000000	False
features are these kinds	0.0	0.0	0.0	2.0	0.0000000000	False
nowadays biolarge their exploration	0.0	0.0	0.0	2.0	0.0000000000	False
terms of new features	0.0	0.0	0.0	2.0	0.0000000000	False
kind of new features	0.0	0.0	0.0	2.0	0.0000000000	False
current state of art	0.0	0.0	0.0	2.0	0.0000000000	False
large amount of work	0.0	0.0	0.0	2.0	0.0000000000	False
efficient trying to make	0.0	0.0	0.0	2.0	0.0000000000	False
data structures and controls	0.0	0.0	0.0	2.0	0.0000000000	False
controls and control structures	0.0	0.0	0.0	2.0	0.0000000000	False
control structures then languages	0.0	0.0	0.0	2.0	0.0000000000	False
languages like ml caml	0.0	0.0	0.0	2.0	0.0000000000	False
addition of new features	0.0	0.0	0.0	2.0	0.0000000000	False
syntax of ml expressions	0.0	0.0	0.0	2.0	0.0000000000	False
lisp in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
features like the introduction	0.0	0.0	0.0	2.0	0.0000000000	False
introduction of modules introduction	0.0	0.0	0.0	2.0	0.0000000000	False
introduction of exceptional handling	0.0	0.0	0.0	2.0	0.0000000000	False
exceptional handling the introduction	0.0	0.0	0.0	2.0	0.0000000000	False
powerful data abstraction mechanisms	0.0	0.0	0.0	2.0	0.0000000000	False
mechanisms and a type	0.0	0.0	0.0	2.0	0.0000000000	False
checking ok so lisp	0.0	0.0	0.0	2.0	0.0000000000	False
lisp has no type	0.0	0.0	0.0	2.0	0.0000000000	False
checking at all right	0.0	0.0	0.0	2.0	0.0000000000	False
study the basic features	0.0	0.0	0.0	2.0	0.0000000000	False
basic features of languages	0.0	0.0	0.0	2.0	0.0000000000	True
thing like language design	0.0	0.0	0.0	2.0	0.0000000000	False
issue that the implementation	0.0	0.0	0.0	2.0	0.0000000000	False
implementation that the language	0.0	0.0	0.0	2.0	0.0000000000	False
language pascal has taught	0.0	0.0	0.0	2.0	0.0000000000	False
idea for a language	0.0	0.0	0.0	2.0	0.0000000000	False
unified primitives for expressing	0.0	0.0	0.0	2.0	0.0000000000	False
basically all your algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms and data structures	0.0	0.0	0.0	2.0	0.0000000000	False
algol sixty like languages	0.0	0.0	0.0	2.0	0.0000000000	False
variations of the dialect	0.0	0.0	0.0	2.0	0.0000000000	False
pascal or algol system	0.0	0.0	0.0	2.0	0.0000000000	False
set of primitive operations	0.0	0.0	0.0	2.0	0.0000000000	False
read the source code	0.0	0.0	5.9978021978	6.0	0.0000000000	False
code of the program	0.0	0.0	0.0	2.0	0.0000000000	False
program like a book	0.0	0.0	0.0	2.0	0.0000000000	False
software that you write	0.0	0.0	0.0	2.0	0.0000000000	False
permanently fixed what hsppens	0.0	0.0	0.0	2.0	0.0000000000	False
bugs might be detected	0.0	0.0	0.0	2.0	0.0000000000	False
detected years and years	0.0	0.0	0.0	2.0	0.0000000000	False
years after their software	0.0	0.0	0.0	2.0	0.0000000000	False
software is been commissioned	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms of the source	0.0	0.0	0.0	2.0	0.0000000000	False
contained so in fact	0.0	0.0	0.0	2.0	0.0000000000	False
efficiency and such consideration	0.0	0.0	0.0	2.0	0.0000000000	False
thing because it includes	0.0	0.0	0.0	2.0	0.0000000000	False
maintainability of the software	0.0	0.0	0.0	2.0	0.0000000000	False
persons who have written	0.0	0.0	0.0	2.0	0.0000000000	False
software so the software	0.0	0.0	0.0	2.0	0.0000000000	False
means that the source	0.0	0.0	0.0	2.0	0.0000000000	False
thing the second thing	0.0	0.0	0.0	2.0	0.0000000000	False
users use their piece	0.0	0.0	0.0	2.0	0.0000000000	False
adding some more conveniences	0.0	0.0	0.0	2.0	0.0000000000	False
part of the maintainability	0.0	0.0	0.0	2.0	0.0000000000	False
maintainability of a piece	0.0	0.0	0.0	2.0	0.0000000000	False
extensibility of the software	0.0	0.0	0.0	2.0	0.0000000000	False
original team that wrote	0.0	0.0	0.0	2.0	0.0000000000	False
abstraction the basic abstraction	0.0	0.0	0.0	2.0	0.0000000000	False
aware of the control	0.0	0.0	0.0	2.0	0.0000000000	False
control abstractions or things	0.0	0.0	0.0	2.0	0.0000000000	False
things like procedures functions	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of control abstractions	0.0	0.0	0.0	2.0	0.0000000000	False
primitive kind of data	0.0	0.0	0.0	2.0	0.0000000000	False
kind of data abstraction	0.0	0.0	0.0	2.0	0.0000000000	False
arrays are one data	0.0	0.0	0.0	2.0	0.0000000000	False
data abstraction that means	0.0	0.0	0.0	2.0	0.0000000000	False
logical unit um records	0.0	0.0	0.0	2.0	0.0000000000	False
unit um records variant	0.0	0.0	0.0	2.0	0.0000000000	False
ability to take sets	0.0	0.0	0.0	2.0	0.0000000000	False
single unit so combinations	0.0	0.0	0.0	2.0	0.0000000000	False
operations and data abstractions	0.0	0.0	0.0	2.0	0.0000000000	False
languages like ml ada	0.0	0.0	0.0	2.0	0.0000000000	False
ada provide the notion	0.0	0.0	0.0	2.0	0.0000000000	False
variables and change types	0.0	0.0	0.0	2.0	0.0000000000	False
change types and instantiate	0.0	0.0	0.0	2.0	0.0000000000	False
instantiate the same kinds	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms for example stacks	0.0	0.0	0.0	2.0	0.0000000000	False
talking of a stack	0.0	0.0	0.0	2.0	0.0000000000	False
stack of some record	0.0	0.0	0.0	2.0	0.0000000000	False
things the basic operation	0.0	0.0	0.0	2.0	0.0000000000	False
basic operation on stacks	0.0	0.0	0.0	2.0	0.0000000000	False
stacks are like pop	0.0	0.0	0.0	2.0	0.0000000000	False
push checking for emptiness	0.0	0.0	0.0	2.0	0.0000000000	False
repeat the code depending	0.0	0.0	0.0	2.0	0.0000000000	False
depending on the type	0.0	0.0	0.0	2.0	0.0000000000	False
type of the element	0.0	0.0	0.0	2.0	0.0000000000	False
element of the stack	0.0	0.0	0.0	2.0	0.0000000000	False
code carefully written verified	0.0	0.0	0.0	2.0	0.0000000000	False
right so the support	0.0	0.0	0.0	2.0	0.0000000000	False
important modern language design	0.0	0.0	0.0	2.0	0.0000000000	False
modern language design issue	0.0	0.0	0.0	2.0	0.0000000000	False
programs so the language	0.0	0.0	0.0	2.0	0.0000000000	False
language should also provide	0.0	0.0	0.0	2.0	0.0000000000	False
provide support for verification	0.0	0.0	0.0	2.0	0.0000000000	False
support for verification provability	0.0	0.0	0.0	2.0	0.0000000000	False
verification provability of programs	0.0	0.0	0.0	2.0	0.0000000000	False
programs not necessarily machine	0.0	0.0	0.0	2.0	0.0000000000	False
necessarily machine based provability	0.0	0.0	0.0	2.0	0.0000000000	False
provability but possibly hand	0.0	0.0	0.0	2.0	0.0000000000	False
possibly hand based provability	0.0	0.0	0.0	2.0	0.0000000000	False
provability or a mixture	0.0	0.0	0.0	2.0	0.0000000000	False
mixture or a user	0.0	0.0	0.0	2.0	0.0000000000	False
interactive provability of programs	0.0	0.0	0.0	2.0	0.0000000000	False
sixties where a lot	0.0	0.0	0.0	2.0	0.0000000000	False
expended in the case	0.0	0.0	0.0	2.0	0.0000000000	False
fortan and cobol compilers	0.0	0.0	0.0	2.0	0.0000000000	False
cobol compilers was portability	0.0	0.0	0.0	2.0	0.0000000000	False
means what it means	0.0	0.0	0.0	2.0	0.0000000000	False
oriented towards an end	0.0	0.0	0.0	2.0	0.0000000000	False
architecture or machine independent	0.0	0.0	0.0	4.0	0.0000000000	False
machine independent or machine	0.0	0.0	0.0	2.0	0.0000000000	False
kind of assembly instruction	0.0	0.0	0.0	2.0	0.0000000000	False
related to the machine	0.0	0.0	0.0	2.0	0.0000000000	False
machines if you ensure	0.0	0.0	0.0	2.0	0.0000000000	False
ensure that you language	0.0	0.0	0.0	2.0	0.0000000000	False
language is really architecture	0.0	0.0	0.0	2.0	0.0000000000	False
independent in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that your main	0.0	0.0	0.0	2.0	0.0000000000	False
convenience the abstractions required	0.0	0.0	0.0	2.0	0.0000000000	False
required for the user	0.0	0.0	0.0	2.0	0.0000000000	False
specific to particular machine	0.0	0.0	0.0	2.0	0.0000000000	False
sets of particular architecture	0.0	0.0	0.0	2.0	0.0000000000	False
based architectures or stack	0.0	0.0	0.0	2.0	0.0000000000	False
architectures or stack based	0.0	0.0	0.0	2.0	0.0000000000	False
architecture specific or machine	0.0	0.0	0.0	2.0	0.0000000000	False
move the entire language	0.0	0.0	0.0	2.0	0.0000000000	False
language to another machine	0.0	0.0	0.0	2.0	0.0000000000	False
minimum amount of effort	0.0	0.0	0.0	2.0	0.0000000000	False
changed when i move	0.0	0.0	0.0	2.0	0.0000000000	False
move an entire language	0.0	0.0	0.0	2.0	0.0000000000	False
implementation from one machine	0.0	0.0	0.0	2.0	0.0000000000	False
idea of the language	0.0	0.0	0.0	2.0	0.0000000000	False
design or the design	0.0	0.0	0.0	2.0	0.0000000000	False
design of its implementation	0.0	0.0	0.0	2.0	0.0000000000	False
ease of the implementation	0.0	0.0	0.0	4.0	0.0000000000	False
availability of ready algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
ready algorithms for implementing	0.0	0.0	0.0	2.0	0.0000000000	False
reason for the success	0.0	0.0	0.0	2.0	0.0000000000	False
reason c c programs	0.0	0.0	0.0	2.0	0.0000000000	False
implementers trying to implement	0.0	0.0	0.0	2.0	0.0000000000	False
syntax common clear semantics	0.0	0.0	0.0	2.0	0.0000000000	False
semantics or the specification	0.0	0.0	0.0	2.0	0.0000000000	False
effects of each language	0.0	0.0	0.0	2.0	0.0000000000	False
language construct each construct	0.0	0.0	0.0	2.0	0.0000000000	False
construct in the language	0.0	0.0	0.0	2.0	0.0000000000	False
programs have to run	0.0	0.0	0.0	2.0	0.0000000000	False
efficiency of the implementation	0.0	0.0	0.0	2.0	0.0000000000	False
means compile time efficiency	0.0	0.0	0.0	2.0	0.0000000000	False
fast can you compile	0.0	0.0	0.0	2.0	0.0000000000	False
written in the language	0.0	0.0	3.99633699634	10.0	0.3824362606	False
language whereas this runtime	0.0	0.0	0.0	2.0	0.0000000000	False
support and the program	0.0	0.0	0.0	2.0	0.0000000000	False
fast as the programs	0.0	0.0	0.0	2.0	0.0000000000	False
run fast yeah ease	0.0	0.0	0.0	2.0	0.0000000000	False
talking about the maintenance	0.0	0.0	0.0	2.0	0.0000000000	False
maintenance of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language by an implementation	0.0	0.0	0.0	2.0	0.0000000000	False
maintenance of the implementation	0.0	0.0	0.0	2.0	0.0000000000	False
implementation of the language	0.0	0.0	0.0	4.0	0.0000000000	False
bugs in the language	0.0	0.0	0.0	2.0	0.0000000000	False
compilation translation and support	0.0	0.0	0.0	2.0	0.0000000000	False
language should support subsets	0.0	0.0	0.0	2.0	0.0000000000	False
subsets of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
smaller set of operations	0.0	0.0	0.0	2.0	0.0000000000	False
features of the language	0.0	0.0	0.0	2.0	0.0000000000	False
divide up the language	0.0	0.0	0.0	2.0	0.0000000000	False
large sort of extensions	0.0	0.0	0.0	2.0	0.0000000000	False
kernel any way run	0.0	0.0	0.0	2.0	0.0000000000	False
run on all machines	0.0	0.0	0.0	2.0	0.0000000000	False
subsets and their reasons	0.0	0.0	0.0	2.0	0.0000000000	False
portability of programs written	0.0	0.0	0.0	2.0	0.0000000000	False
important language for embedded	0.0	0.0	0.0	2.0	0.0000000000	False
language for embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
systems that control sensors	0.0	0.0	0.0	2.0	0.0000000000	False
control sensors various kinds	0.0	0.0	0.0	2.0	0.0000000000	False
affects it could affect	0.0	0.0	0.0	2.0	0.0000000000	False
program to another implementation	0.0	0.0	0.0	2.0	0.0000000000	False
feature like your programs	0.0	0.0	0.0	2.0	0.0000000000	False
right so the portability	0.0	0.0	0.0	2.0	0.0000000000	False
portability of actually programs	0.0	0.0	0.0	2.0	0.0000000000	False
divide up the study	0.0	0.0	0.0	2.0	0.0000000000	False
study of programming languages	0.0	0.0	0.0	2.0	0.0000000000	False
theory of programming languages	0.0	0.0	0.0	4.0	0.0000000000	False
general theory of programming	0.0	0.0	0.0	2.0	0.0000000000	False
programming languages is based	0.0	0.0	0.0	2.0	0.0000000000	False
based on three things	0.0	0.0	0.0	2.0	0.0000000000	False
syntax of the language	0.0	0.0	0.0	2.0	0.0000000000	True
highly simplified natural language	0.0	0.0	0.0	2.0	0.0000000000	False
right so which means	0.0	0.0	0.0	2.0	0.0000000000	False
sentences of the language	0.0	0.0	2.99633699634	10.0	0.4251968504	False
written in that language	0.0	0.0	0.0	2.0	0.0000000000	False
languageit has various parts	0.0	0.0	0.0	2.0	0.0000000000	False
languages all natural languages	0.0	0.0	0.0	2.0	0.0000000000	False
natural languages one thing	0.0	0.0	0.0	2.0	0.0000000000	False
syntactic category called predicates	0.0	0.0	0.0	2.0	0.0000000000	False
clause or a phrase	0.0	0.0	0.0	2.0	0.0000000000	False
subject too in addition	0.0	0.0	0.0	2.0	0.0000000000	False
sentence in natural language	0.0	0.0	0.0	2.0	0.0000000000	False
predicate no complete sentence	0.0	0.0	0.0	2.0	0.0000000000	False
predicate ok so run	0.0	0.0	0.0	2.0	0.0000000000	False
subject well subject phrases	0.0	0.0	0.0	2.0	0.0000000000	False
phrases may be noun	0.0	0.0	0.0	2.0	0.0000000000	False
noun phrases which means	0.0	0.0	0.0	2.0	0.0000000000	False
nouns qualified by object	0.0	0.0	0.0	2.0	0.0000000000	False
sentence of this programming	0.0	0.0	0.0	2.0	0.0000000000	False
programming language and parse	0.0	0.0	0.0	2.0	0.0000000000	False
meaning of these things	0.0	0.0	0.0	2.0	0.0000000000	False
similarities with natural language	0.0	0.0	0.0	2.0	0.0000000000	False
lot of the theory	0.0	0.0	0.0	2.0	0.0000000000	False
theory of the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
syntax was actually inspired	0.0	0.0	0.0	2.0	0.0000000000	False
inspired by natural languages	0.0	0.0	0.0	2.0	0.0000000000	False
languages where the construction	0.0	0.0	0.0	2.0	0.0000000000	False
construction of artificial languages	0.0	0.0	0.0	2.0	0.0000000000	False
semantics of a programming	0.0	0.0	0.0	2.0	0.0000000000	False
study of pascal programming	0.0	0.0	0.0	2.0	0.0000000000	False
iso standard pascal reference	0.0	0.0	0.0	2.0	0.0000000000	False
reference manual by janson	0.0	0.0	0.0	2.0	0.0000000000	False
manual by janson edward	0.0	0.0	0.0	2.0	0.0000000000	False
reference manual really specifies	0.0	0.0	0.0	2.0	0.0000000000	False
effect to be expected	0.0	0.0	0.0	2.0	0.0000000000	False
executing that syntactic entity	0.0	0.0	0.0	2.0	0.0000000000	False
natural language the notion	0.0	0.0	0.0	2.0	0.0000000000	False
mathematically in a machine	0.0	0.0	0.0	2.0	0.0000000000	False
talking about this semantics	0.0	0.0	0.0	2.0	0.0000000000	False
semantics of this programming	0.0	0.0	0.0	2.0	0.0000000000	False
language we are talking	0.0	0.0	0.0	2.0	0.0000000000	False
talking about a pure	0.0	0.0	0.0	2.0	0.0000000000	False
general we are talking	0.0	0.0	0.0	2.0	0.0000000000	False
meanings in a abstract	0.0	0.0	0.0	2.0	0.0000000000	False
settings in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that you assume	0.0	0.0	0.0	2.0	0.0000000000	False
restrictions on the word	0.0	0.0	0.0	2.0	0.0000000000	False
restrictions on computational power	0.0	0.0	0.0	2.0	0.0000000000	False
finite number of operations	0.0	0.0	5.9978021978	6.0	0.0000000000	False
operations at any instance	0.0	0.0	0.0	2.0	0.0000000000	False
right so you assume	0.0	0.0	0.0	2.0	0.0000000000	False
semantic the programming language	0.0	0.0	0.0	2.0	0.0000000000	False
thinking of some kind	0.0	0.0	0.0	2.0	0.0000000000	False
kind of an ideal	0.0	0.0	0.0	2.0	0.0000000000	False
restrictions except one restriction	0.0	0.0	0.0	2.0	0.0000000000	False
operations can be performed	0.0	0.0	0.0	2.0	0.0000000000	False
infinite number of operations	0.0	0.0	0.0	2.0	0.0000000000	False
entity in some ideal	0.0	0.0	0.0	2.0	0.0000000000	False
constructs of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language in that ideal	0.0	0.0	0.0	2.0	0.0000000000	False
independent of any machine	0.0	0.0	0.0	2.0	0.0000000000	False
language as an entity	0.0	0.0	0.0	2.0	0.0000000000	False
devoid of any machine	0.0	0.0	0.0	2.0	0.0000000000	False
general the semantics follow	0.0	0.0	0.0	2.0	0.0000000000	False
semantics follow the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
semantics so the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
elements and some compound	0.0	0.0	0.0	2.0	0.0000000000	False
operations they are connectives	0.0	0.0	0.0	2.0	0.0000000000	False
meanings of the connectives	0.0	0.0	0.0	2.0	0.0000000000	False
infinite set of programs	0.0	0.0	0.0	2.0	0.0000000000	False
discipline that you express	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the effect	0.0	0.0	0.0	2.0	0.0000000000	False
elements of the language	0.0	0.0	0.0	4.0	0.0000000000	False
general possible to predict	0.0	0.0	0.0	2.0	0.0000000000	False
behavior of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language of a program	0.0	0.0	0.0	2.0	0.0000000000	False
language unless you follow	0.0	0.0	0.0	2.0	0.0000000000	False
feature of any kind	0.0	0.0	0.0	2.0	0.0000000000	False
derivation of the meanings	0.0	0.0	0.0	2.0	0.0000000000	False
infinite number of programs	0.0	0.0	0.0	2.0	0.0000000000	False
meaning of the program	0.0	0.0	0.0	4.0	0.0000000000	False
complex elements are formed	0.0	0.0	0.0	2.0	0.0000000000	False
formed so the meaning	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the meanings	0.0	0.0	0.0	2.0	0.0000000000	False
elements and the meanings	0.0	0.0	0.0	2.0	0.0000000000	False
formed a complex element	0.0	0.0	0.0	2.0	0.0000000000	False
means is that semantics	0.0	0.0	0.0	2.0	0.0000000000	False
related to the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
set of allowable objects	0.0	0.0	0.0	4.0	0.0000000000	False
objects are the programs	0.0	0.0	0.0	2.0	0.0000000000	False
programs of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language or the sentences	0.0	0.0	0.0	2.0	0.0000000000	False
language and the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
syntax gives you finitary	0.0	0.0	0.0	2.0	0.0000000000	False
notation right the set	0.0	0.0	0.0	2.0	0.0000000000	False
right the set builder	0.0	0.0	0.0	2.0	0.0000000000	False
give a finitary specification	0.0	0.0	0.0	2.0	0.0000000000	False
program is its structure	0.0	0.0	0.0	2.0	0.0000000000	False
terms of its syntax	0.0	0.0	0.0	2.0	0.0000000000	False
syntax the finitary specification	0.0	0.0	0.0	2.0	0.0000000000	False
specification so the meaning	0.0	0.0	0.0	2.0	0.0000000000	False
semantics of a language	0.0	0.0	0.0	2.0	0.0000000000	False
sort of an ideal	0.0	0.0	0.0	2.0	0.0000000000	False
worry about machine constraints	0.0	0.0	0.0	2.0	0.0000000000	False
worry about word lengths	0.0	0.0	0.0	2.0	0.0000000000	False
worry about limits don	0.0	0.0	0.0	2.0	0.0000000000	False
worry about memory constraints	0.0	0.0	0.0	2.0	0.0000000000	False
constraints assume infinite amount	0.0	0.0	0.0	2.0	0.0000000000	False
infinite amount of memory	0.0	0.0	0.0	2.0	0.0000000000	False
features are really pragmatics	0.0	0.0	0.0	2.0	0.0000000000	False
associate a disc file	0.0	0.0	0.0	2.0	0.0000000000	False
file variable in side	0.0	0.0	0.0	2.0	0.0000000000	False
feature which will vary	0.0	0.0	0.0	2.0	0.0000000000	False
depending upon the operating	0.0	0.0	0.0	2.0	0.0000000000	False
firstly um firstly involves	0.0	0.0	0.0	2.0	0.0000000000	False
machine and architectural constraints	0.0	0.0	0.0	2.0	0.0000000000	False
maxint the maximum integer	0.0	0.0	0.0	2.0	0.0000000000	False
typically implementation dependant feature	0.0	0.0	0.0	2.0	0.0000000000	False
depends upon word length	0.0	0.0	0.0	2.0	0.0000000000	False
word length or byte	0.0	0.0	0.0	2.0	0.0000000000	False
length or byte length	0.0	0.0	0.0	2.0	0.0000000000	False
length or um byte	0.0	0.0	0.0	2.0	0.0000000000	False
two bytes for representing	0.0	0.0	0.0	2.0	0.0000000000	False
right so the value	0.0	0.0	0.0	2.0	0.0000000000	False
value of the maxint	0.0	0.0	0.0	2.0	0.0000000000	False
machine or a register	0.0	0.0	0.0	2.0	0.0000000000	False
machine um those things	0.0	0.0	0.0	2.0	0.0000000000	False
implement um those things	0.0	0.0	0.0	2.0	0.0000000000	False
things are implementation dependent	0.0	0.0	0.0	2.0	0.0000000000	False
portable we will separate	0.0	0.0	0.0	2.0	0.0000000000	False
out the basic algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
basic algorithms of implementation	0.0	0.0	0.0	2.0	0.0000000000	False
nature of the implementation	0.0	0.0	0.0	2.0	0.0000000000	False
features the actual code	0.0	0.0	0.0	2.0	0.0000000000	False
nature of the input	0.0	0.0	0.0	2.0	0.0000000000	False
file based terminal based	0.0	0.0	0.0	2.0	0.0000000000	False
based terminal based sensor	0.0	0.0	0.0	2.0	0.0000000000	False
terminal based sensor based	0.0	0.0	0.0	2.0	0.0000000000	False
includes the file server	0.0	0.0	0.0	2.0	0.0000000000	False
server how the language	0.0	0.0	0.0	2.0	0.0000000000	False
language has to interact	0.0	0.0	0.0	4.0	0.0000000000	False
interact with the file	0.0	0.0	0.0	2.0	0.0000000000	False
general with the directory	0.0	0.0	0.0	2.0	0.0000000000	False
service of the machine	0.0	0.0	0.0	2.0	0.0000000000	False
errors and by errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors i mean errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors written by errors	0.0	0.0	0.0	2.0	0.0000000000	False
written by errors introduced	0.0	0.0	0.0	2.0	0.0000000000	False
errors introduced by users	0.0	0.0	0.0	2.0	0.0000000000	False
users in their programs	0.0	0.0	0.0	2.0	0.0000000000	False
errors as a blanket	0.0	0.0	0.0	2.0	0.0000000000	False
nature of the error	0.0	0.0	0.0	2.0	0.0000000000	False
throw out the program	0.0	0.0	0.0	2.0	0.0000000000	False
out all the errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors in the program	0.0	0.0	0.0	2.0	0.0000000000	False
things that are errors	0.0	0.0	0.0	2.0	0.0000000000	False
amount of compilation effort	0.0	0.0	0.0	2.0	0.0000000000	False
decent error reporting mechanism	0.0	0.0	0.0	2.0	0.0000000000	False
reporting mechanism some error	0.0	0.0	0.0	2.0	0.0000000000	False
mechanism some error handling	0.0	0.0	0.0	2.0	0.0000000000	False
policy and different languages	0.0	0.0	0.0	2.0	0.0000000000	False
implementations take different attitudes	0.0	0.0	0.0	2.0	0.0000000000	False
study all three issues	0.0	0.0	0.0	2.0	0.0000000000	False
semantic and pragmatic issues	0.0	0.0	0.0	2.0	0.0000000000	False
issues will closely depend	0.0	0.0	0.0	2.0	0.0000000000	False
depend upon the syntax	0.0	0.0	0.0	4.0	0.0000000000	False
preferable that they depend	0.0	0.0	0.0	2.0	0.0000000000	False
possibly an abstract object	0.0	0.0	0.0	2.0	0.0000000000	False
century attitude towards numbers	0.0	0.0	0.0	2.0	0.0000000000	False
conception of the mind	0.0	0.0	0.0	2.0	0.0000000000	False
representation in the form	0.0	0.0	0.0	2.0	0.0000000000	False
representing the same number	0.0	0.0	0.0	2.0	0.0000000000	False
hexadecimal and i hope	0.0	0.0	0.0	2.0	0.0000000000	False
representations of the number	0.0	0.0	0.0	2.0	0.0000000000	False
differs from the theonagri	0.0	0.0	0.0	2.0	0.0000000000	False
representation in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that the characters	0.0	0.0	0.0	2.0	0.0000000000	False
positional representation of numbers	0.0	0.0	0.0	2.0	0.0000000000	False
unified by the fact	0.0	0.0	0.0	2.0	0.0000000000	False
representations yeah positional representations	0.0	0.0	0.0	2.0	0.0000000000	False
positional representations i hope	0.0	0.0	0.0	2.0	0.0000000000	False
numerals which is non	0.0	0.0	0.0	2.0	0.0000000000	False
alphabet you could represent	0.0	0.0	0.0	2.0	0.0000000000	False
difference about this representation	0.0	0.0	0.0	2.0	0.0000000000	False
change in character set	0.0	0.0	0.0	2.0	0.0000000000	False
character set what makes	0.0	0.0	0.0	2.0	0.0000000000	False
two ok what make	0.0	0.0	0.0	2.0	0.0000000000	False
makes these two classes	0.0	0.0	0.0	2.0	0.0000000000	False
numeral of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language used for representing	0.0	0.0	0.0	2.0	0.0000000000	False
identical the character sets	0.0	0.0	0.0	2.0	0.0000000000	False
sets but the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
compound forms from simpler	0.0	0.0	0.0	2.0	0.0000000000	False
forms from simpler forms	0.0	0.0	0.0	2.0	0.0000000000	False
roman and arabic case	0.0	0.0	0.0	2.0	0.0000000000	False
setting of the programming	0.0	0.0	0.0	2.0	0.0000000000	False
call a complete dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
complete dictionary of words	0.0	0.0	0.0	2.0	0.0000000000	False
words of the language	0.0	0.0	0.0	4.0	0.0000000000	False
language and a word	0.0	0.0	0.0	2.0	0.0000000000	False
word of a language	0.0	0.0	0.0	2.0	0.0000000000	False
formed from a character	0.0	0.0	0.0	2.0	0.0000000000	False
set from a fixed	0.0	0.0	0.0	2.0	0.0000000000	False
words as allowable words	0.0	0.0	0.0	2.0	0.0000000000	False
allowable words as part	0.0	0.0	0.0	2.0	0.0000000000	False
vocabulary of the language	0.0	0.0	0.0	4.0	0.0000000000	False
dictionary of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language is what constitutes	0.0	0.0	0.0	4.0	0.0000000000	False
natural languages like konkani	0.0	0.0	0.0	2.0	0.0000000000	False
written by different people	0.0	0.0	0.0	2.0	0.0000000000	False
people some people write	0.0	0.0	0.0	2.0	0.0000000000	False
people write in devanagri	0.0	0.0	0.0	2.0	0.0000000000	False
devanagri some people write	0.0	0.0	0.0	2.0	0.0000000000	False
script the arabic script	0.0	0.0	0.0	2.0	0.0000000000	False
collection of the words	0.0	0.0	0.0	2.0	0.0000000000	False
person who knows devanagri	0.0	0.0	0.0	2.0	0.0000000000	False
communicate with the person	0.0	0.0	0.0	2.0	0.0000000000	False
urdu script by speech	0.0	0.0	0.0	2.0	0.0000000000	False
speech because the words	0.0	0.0	0.0	2.0	0.0000000000	False
fixed collection of words	0.0	0.0	0.0	2.0	0.0000000000	False
words whose actual form	0.0	0.0	0.0	2.0	0.0000000000	False
actual form might depend	0.0	0.0	0.0	2.0	0.0000000000	False
depend on the character	0.0	0.0	0.0	2.0	0.0000000000	False
vocabulary there are ways	0.0	0.0	0.0	2.0	0.0000000000	False
ways of combining words	0.0	0.0	0.0	2.0	0.0000000000	False
language to form sentences	0.0	0.0	0.0	2.0	0.0000000000	False
finite set of formation	0.0	0.0	0.0	2.0	0.0000000000	False
set of formation rules	0.0	0.0	0.0	2.0	0.0000000000	False
rules are called productions	0.0	0.0	0.0	2.0	0.0000000000	False
generate all possible sentences	0.0	0.0	0.0	2.0	0.0000000000	False
sentences in the language	0.0	0.0	0.0	2.0	0.0000000000	False
character set really depends	0.0	0.0	0.0	2.0	0.0000000000	False
depends upon the kind	0.0	0.0	0.0	2.0	0.0000000000	False
codes you use nowadays	0.0	0.0	0.0	2.0	0.0000000000	False
pcs and seven bit	0.0	0.0	0.0	2.0	0.0000000000	False
main frame the character	0.0	0.0	0.0	2.0	0.0000000000	False
frame the character sets	0.0	0.0	0.0	2.0	0.0000000000	False
constitutes the a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
four tuple of objects	0.0	0.0	0.0	2.0	0.0000000000	False
set of non terminals	0.0	0.0	3.9978021978	6.0	0.0000000000	False
terminals and this set	0.0	0.0	0.0	2.0	0.0000000000	False
non terminals really specifies	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of grammatical categories	0.0	0.0	0.0	2.0	0.0000000000	False
categories of the language	0.0	0.0	0.0	2.0	0.0000000000	False
parts of speech noun	0.0	0.0	0.0	2.0	0.0000000000	False
speech noun phrase verb	0.0	0.0	0.0	2.0	0.0000000000	False
noun phrase verb phrase	0.0	0.0	0.0	2.0	0.0000000000	False
phrase verb phrase adjectival	0.0	0.0	0.0	2.0	0.0000000000	False
verb phrase adjectival phrase	0.0	0.0	0.0	2.0	0.0000000000	False
phrase adjectival phrase noun	0.0	0.0	0.0	2.0	0.0000000000	False
adjectival phrase noun clause	0.0	0.0	0.0	2.0	0.0000000000	False
phrase noun clause subject	0.0	0.0	0.0	2.0	0.0000000000	False
noun clause subject clauses	0.0	0.0	0.0	2.0	0.0000000000	False
clause subject clauses subject	0.0	0.0	0.0	2.0	0.0000000000	False
subject clauses subject phrases	0.0	0.0	0.0	2.0	0.0000000000	False
clauses subject phrases object	0.0	0.0	0.0	2.0	0.0000000000	False
subject phrases object clauses	0.0	0.0	0.0	2.0	0.0000000000	False
phrases object clauses predicates	0.0	0.0	0.0	2.0	0.0000000000	False
symbols or terminal words	0.0	0.0	0.0	2.0	0.0000000000	False
collection of formation rules	0.0	0.0	0.0	2.0	0.0000000000	False
represents a grammatical category	0.0	0.0	0.0	2.0	0.0000000000	False
grammar specifying boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
symbol s every grammar	0.0	0.0	0.0	2.0	0.0000000000	False
categories the grammatical categories	0.0	0.0	0.0	2.0	0.0000000000	False
stand for an add	0.0	0.0	0.0	2.0	0.0000000000	False
expression or a complement	0.0	0.0	0.0	2.0	0.0000000000	False
vocabulary of this language	0.0	0.0	0.0	2.0	0.0000000000	False
left and right parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
productions from the start	0.0	0.0	0.0	2.0	0.0000000000	False
belonging to this set	0.0	0.0	0.0	2.0	0.0000000000	False
set of boolean variables	0.0	0.0	0.0	2.0	0.0000000000	False
expression so the sentence	0.0	0.0	0.0	2.0	0.0000000000	False
sentence of this languages	0.0	0.0	0.0	2.0	0.0000000000	False
languages are boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
fully parenthesized boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
expressions an and clause	0.0	0.0	0.0	2.0	0.0000000000	False
two boolean expressions enclosed	0.0	0.0	0.0	4.0	0.0000000000	False
expressions enclosed in parenthesis	0.0	0.0	0.0	4.0	0.0000000000	False
separated by the word	0.0	0.0	0.0	2.0	0.0000000000	False
variables whenever you find	0.0	0.0	0.0	2.0	0.0000000000	False
sentence generation you start	0.0	0.0	0.0	2.0	0.0000000000	False
replacing i have circled	0.0	0.0	0.0	2.0	0.0000000000	False
replacing this s leaving	0.0	0.0	0.0	2.0	0.0000000000	False
leaving everything else intact	0.0	0.0	0.0	2.0	0.0000000000	False
chosen here to replace	0.0	0.0	0.0	2.0	0.0000000000	False
possibility is to replace	0.0	0.0	0.0	2.0	0.0000000000	False
chosen it to replace	0.0	0.0	0.0	2.0	0.0000000000	False
two and i proceed	0.0	0.0	0.0	2.0	0.0000000000	False
proceed in this fashion	0.0	0.0	0.0	2.0	0.0000000000	False
generated by this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
talk of a language	0.0	0.0	0.0	2.0	0.0000000000	False
language as being generated	0.0	0.0	0.0	2.0	0.0000000000	False
generated from a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar as a set	0.0	0.0	0.0	2.0	0.0000000000	False
generated from the start	0.0	0.0	0.0	4.0	0.0000000000	False
symbol s important warnings	0.0	0.0	0.0	2.0	0.0000000000	False
terminals and the set	0.0	0.0	0.0	2.0	0.0000000000	False
set of terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
disjoint and a production	0.0	0.0	0.0	2.0	0.0000000000	False
terminal by a string	0.0	0.0	0.0	2.0	0.0000000000	False
string consisting of terminals	0.0	0.0	0.0	2.0	0.0000000000	False
terminals or non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
terminals yeah and sentence	0.0	0.0	0.0	2.0	0.0000000000	False
string of terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
v.srinivasa rajkumar educational technology	0.0	0.0	0.0	2.0	0.0000000000	False
rajkumar educational technology i.i.t.delhi	0.0	0.0	0.0	2.0	0.0000000000	False
educational technology i.i.t.delhi presents	0.0	0.0	0.0	2.0	0.0000000000	False
i.i.t.delhi presents a video	0.0	0.0	0.0	2.0	0.0000000000	False
video course on programming	0.0	0.0	0.0	2.0	0.0000000000	False
languages by dr.s.arun kumar	0.0	0.0	0.0	2.0	0.0000000000	False
today we will continue	0.0	0.0	0.0	2.0	0.0000000000	False
grammar on last lecture	0.0	0.0	0.0	2.0	0.0000000000	False
symbols or grammatical categories	0.0	0.0	0.0	2.0	0.0000000000	False
set of terminal symbols	0.0	0.0	2.99261311173	16.0	0.3793103448	False
symbols which usually constitutes	0.0	0.0	0.0	2.0	0.0000000000	False
vocabulary of programming language	0.0	0.0	0.0	2.0	0.0000000000	False
finite collection of formation	0.0	0.0	0.0	2.0	0.0000000000	False
collection of formation rules	0.0	0.0	0.0	2.0	0.0000000000	False
formation rules or productions	0.0	0.0	0.0	2.0	0.0000000000	False
replacement and a start	0.0	0.0	0.0	2.0	0.0000000000	False
symbol which really signifies	0.0	0.0	0.0	2.0	0.0000000000	False
signifies the grammatical category	0.0	0.0	0.0	2.0	0.0000000000	False
category called a sentence	0.0	0.0	0.0	2.0	0.0000000000	False
sentence of the language	0.0	0.0	0.0	4.0	0.0000000000	False
language of a language	0.0	0.0	0.0	2.0	0.0000000000	False
generation of boolean expression	0.0	0.0	0.0	2.0	0.0000000000	False
set of non terminals	0.0	0.0	2.9972299169	6.0	0.0000000000	False
essentially the and expressions	0.0	0.0	0.0	2.0	0.0000000000	False
expressions the v stands	0.0	0.0	0.0	2.0	0.0000000000	False
stands for or expressions	0.0	0.0	0.0	2.0	0.0000000000	False
expressions the c stand	0.0	0.0	0.0	2.0	0.0000000000	False
stand for conditional exp	0.0	0.0	0.0	2.0	0.0000000000	False
conditional exp um complement	0.0	0.0	0.0	2.0	0.0000000000	False
exp um complement expressions	0.0	0.0	0.0	2.0	0.0000000000	False
symbol the terminal set	0.0	0.0	0.0	2.0	0.0000000000	False
open and close parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesis and the connectives	0.0	0.0	0.0	2.0	0.0000000000	False
black for terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
categories are a level	0.0	0.0	0.0	2.0	0.0000000000	False
green um light green	0.0	0.0	0.0	2.0	0.0000000000	False
green for some things	0.0	0.0	0.0	2.0	0.0000000000	False
string of terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
symbols can be generated	0.0	0.0	0.0	2.0	0.0000000000	False
generated from this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
cases i have circled	0.0	0.0	0.0	2.0	0.0000000000	False
number of other sentences	0.0	0.0	0.0	4.0	0.0000000000	False
sentences you will generate	0.0	0.0	0.0	2.0	0.0000000000	False
generate a large number	0.0	0.0	0.0	2.0	0.0000000000	False
case of this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
generate an infinite set	0.0	0.0	0.0	2.0	0.0000000000	False
infinite set of sentences	0.0	0.0	0.0	2.0	0.0000000000	False
set a large part	0.0	0.0	0.0	2.0	0.0000000000	False
large part of computer	0.0	0.0	0.0	2.0	0.0000000000	False
part of computer science	0.0	0.0	0.0	2.0	0.0000000000	False
science mathematics and logic	0.0	0.0	0.0	2.0	0.0000000000	False
terminals and the set	0.0	0.0	0.0	2.0	0.0000000000	False
symbols should be disjoint	0.0	0.0	0.0	2.0	0.0000000000	False
binary relation from non	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbols to strings	0.0	0.0	0.0	2.0	0.0000000000	False
right so the replacement	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbol and replace	0.0	0.0	0.0	2.0	0.0000000000	False
replace that this set	0.0	0.0	0.0	2.0	0.0000000000	False
generated from this set	0.0	0.0	0.0	2.0	0.0000000000	False
general for any set	0.0	0.0	0.0	2.0	0.0000000000	False
set a a star	0.0	0.0	0.0	2.0	0.0000000000	False
star is the set	0.0	0.0	0.0	2.0	0.0000000000	False
strings of finite length	0.0	0.0	0.0	4.0	0.0000000000	False
letter epsilon to denote	0.0	0.0	0.0	2.0	0.0000000000	False
set of all non	0.0	0.0	5.9972299169	6.0	0.0000000000	False
non empty strings generated	0.0	0.0	0.0	2.0	0.0000000000	False
string of zero length	0.0	0.0	0.0	2.0	0.0000000000	False
equal to a star	0.0	0.0	0.0	2.0	0.0000000000	False
star with epsilon removed	0.0	0.0	0.0	2.0	0.0000000000	False
side of the arrow	0.0	0.0	0.0	2.0	0.0000000000	False
single non terminal symbol	0.0	0.0	0.0	2.0	0.0000000000	False
terminals and non terminals	0.0	0.0	5.99630655586	8.0	0.4313725490	False
grammar right a context	0.0	0.0	0.0	2.0	0.0000000000	False
sensitive grammar has production	0.0	0.0	0.0	2.0	0.0000000000	False
grammar has production rules	0.0	0.0	0.0	2.0	0.0000000000	False
symbols you are allowed	0.0	0.0	0.0	2.0	0.0000000000	False
string of non terminals	0.0	0.0	4.9972299169	6.0	0.0000000000	False
assume we are choosing	0.0	0.0	0.0	2.0	0.0000000000	False
choosing this arbitrary string	0.0	0.0	0.0	2.0	0.0000000000	False
appears in a context	0.0	0.0	1.99630655586	8.0	0.4313725490	False
context and the rest	0.0	0.0	0.0	2.0	0.0000000000	False
appears in this context	0.0	0.0	0.0	2.0	0.0000000000	False
calling this grammar context	0.0	0.0	0.0	2.0	0.0000000000	False
non terminals and terminals	0.0	0.0	4.9972299169	6.0	0.0000000000	False
context that s appears	0.0	0.0	0.0	2.0	0.0000000000	False
uniform rule the production	0.0	0.0	0.0	2.0	0.0000000000	False
rule the production rule	0.0	0.0	0.0	2.0	0.0000000000	False
rule says that uniform	0.0	0.0	0.0	2.0	0.0000000000	False
uniform in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
replacement ok as suppose	0.0	0.0	0.0	2.0	0.0000000000	False
terminal can be replaced	0.0	0.0	0.0	2.0	0.0000000000	False
replaced by a string	0.0	0.0	0.0	2.0	0.0000000000	False
general than a context	0.0	0.0	0.0	2.0	0.0000000000	False
context free grammar production	0.0	0.0	0.0	2.0	0.0000000000	False
consists of the empty	0.0	0.0	0.0	4.0	0.0000000000	False
worry about some simpler	0.0	0.0	0.0	2.0	0.0000000000	False
rules in its context	0.0	0.0	0.0	2.0	0.0000000000	False
languages the language generated	0.0	0.0	0.0	2.0	0.0000000000	False
generated by any grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar is a set	0.0	0.0	0.0	2.0	0.0000000000	False
generated from the start	0.0	0.0	0.0	4.0	0.0000000000	False
located in some context	0.0	0.0	0.0	2.0	0.0000000000	False
general we would call	0.0	0.0	0.0	2.0	0.0000000000	False
call it a language	0.0	0.0	0.0	2.0	0.0000000000	False
language on the set	0.0	0.0	0.0	2.0	0.0000000000	False
symbols and a language	0.0	0.0	0.0	2.0	0.0000000000	False
infinite set of strings	0.0	0.0	0.0	2.0	0.0000000000	False
subset of t star	0.0	0.0	0.0	4.0	0.0000000000	False
star is a language	0.0	0.0	0.0	2.0	0.0000000000	False
define on any set	0.0	0.0	0.0	2.0	0.0000000000	False
empty set for strings	0.0	0.0	0.0	2.0	0.0000000000	False
single element the empty	0.0	0.0	0.0	2.0	0.0000000000	False
element the empty string	0.0	0.0	0.0	2.0	0.0000000000	False
lot of other languages	0.0	0.0	0.0	2.0	0.0000000000	False
star as two extreme	0.0	0.0	0.0	2.0	0.0000000000	False
extreme as one extreme	0.0	0.0	0.0	2.0	0.0000000000	False
language and the language	0.0	0.0	0.0	2.0	0.0000000000	False
language containing the empty	0.0	0.0	0.0	2.0	0.0000000000	False
set of possible programs	0.0	0.0	0.0	2.0	0.0000000000	False
programs and the problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem is of defining	0.0	0.0	0.0	2.0	0.0000000000	False
defining exactly what grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammars called regular grammars	0.0	0.0	0.0	2.0	0.0000000000	False
regular grammar every production	0.0	0.0	0.0	2.0	0.0000000000	False
form where this capital	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbol this capital	0.0	0.0	0.0	2.0	0.0000000000	False
denotes a terminal symbol	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbol in fact	0.0	0.0	0.0	2.0	0.0000000000	False
form then we call	0.0	0.0	0.0	2.0	0.0000000000	False
call this a right	0.0	0.0	0.0	2.0	0.0000000000	False
right linear regular grammar	0.0	0.0	4.9944598338	12.0	0.3666666667	True
first thing to realize	0.0	0.0	0.0	2.0	0.0000000000	False
difference ok a context	0.0	0.0	0.0	2.0	0.0000000000	False
free grammar allows productions	0.0	0.0	0.0	2.0	0.0000000000	False
linear regular grammar means	0.0	0.0	0.0	2.0	0.0000000000	False
appearing in this order	0.0	0.0	0.0	2.0	0.0000000000	False
order the terminal symbol	0.0	0.0	0.0	2.0	0.0000000000	False
symbol in a context	0.0	0.0	0.0	2.0	0.0000000000	False
terminal to be generated	0.0	0.0	0.0	2.0	0.0000000000	False
strings of the language	0.0	0.0	0.0	2.0	0.0000000000	False
form the terminal set	0.0	0.0	0.0	2.0	0.0000000000	False
non terminal symbols appearing	0.0	0.0	0.0	2.0	0.0000000000	False
appearing on the right	0.0	0.0	0.0	4.0	0.0000000000	False
generate a full sentence	0.0	0.0	0.0	2.0	0.0000000000	False
language so a right	0.0	0.0	0.0	2.0	0.0000000000	False
similarly you might define	0.0	0.0	0.0	2.0	0.0000000000	False
define a left linear	0.0	0.0	0.0	4.0	0.0000000000	False
left linear regular grammar	0.0	0.0	3.9972299169	6.0	0.0000000000	True
defined for example designed	0.0	0.0	0.0	2.0	0.0000000000	False
designed some hard ware	0.0	0.0	0.0	2.0	0.0000000000	False
hard ware using finite	0.0	0.0	0.0	2.0	0.0000000000	False
ware using finite state	0.0	0.0	0.0	2.0	0.0000000000	False
state machines it turns	0.0	0.0	0.0	2.0	0.0000000000	False
linear grammars actually represents	0.0	0.0	0.0	2.0	0.0000000000	False
represents finite state machines	0.0	0.0	0.0	2.0	0.0000000000	False
machines you know machines	0.0	0.0	0.0	2.0	0.0000000000	False
output i am talking	0.0	0.0	0.0	2.0	0.0000000000	False
talking of those kinds	0.0	0.0	0.0	2.0	0.0000000000	False
right so in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact you can represent	0.0	0.0	0.0	2.0	0.0000000000	False
diagram of the machine	0.0	0.0	0.0	2.0	0.0000000000	False
state as a non	0.0	0.0	0.0	2.0	0.0000000000	False
input symbol the input	0.0	0.0	0.0	2.0	0.0000000000	False
input into that state	0.0	0.0	0.0	2.0	0.0000000000	False
state machine automatically defines	0.0	0.0	0.0	2.0	0.0000000000	False
automatically defines a right	0.0	0.0	0.0	2.0	0.0000000000	False
defines a right linear	0.0	0.0	0.0	2.0	0.0000000000	False
machines have a start	0.0	0.0	0.0	2.0	0.0000000000	False
start symbol the start	0.0	0.0	0.0	2.0	0.0000000000	False
symbol the start symbol	0.0	0.0	0.0	2.0	0.0000000000	False
symbol is the start	0.0	0.0	0.0	2.0	0.0000000000	False
suma let us summarize	0.0	0.0	0.0	2.0	0.0000000000	False
general properties of grammars	0.0	0.0	0.0	2.0	0.0000000000	True
firstly every regular grammar	0.0	0.0	0.0	2.0	0.0000000000	False
regular grammar whether right	0.0	0.0	0.0	2.0	0.0000000000	False
grammar whether right linear	0.0	0.0	0.0	2.0	0.0000000000	False
right linear or left	0.0	0.0	0.0	4.0	0.0000000000	False
left linear every regular	0.0	0.0	0.0	2.0	0.0000000000	False
linear every regular grammar	0.0	0.0	0.0	2.0	0.0000000000	False
context free every context	0.0	0.0	0.0	4.0	0.0000000000	False
productions of the context	0.0	0.0	0.0	2.0	0.0000000000	False
grammar can be considered	0.0	0.0	0.0	2.0	0.0000000000	False
considered in the context	0.0	0.0	0.0	2.0	0.0000000000	False
context of empty strings	0.0	0.0	0.0	2.0	0.0000000000	False
strings on both sides	0.0	0.0	0.0	2.0	0.0000000000	False
sides of the non	0.0	0.0	0.0	2.0	0.0000000000	False
symbol then an empty	0.0	0.0	0.0	2.0	0.0000000000	False
bracket then an empty	0.0	0.0	0.0	2.0	0.0000000000	False
production as being padded	0.0	0.0	0.0	2.0	0.0000000000	False
context which contains empty	0.0	0.0	0.0	2.0	0.0000000000	False
string and the empty	0.0	0.0	0.0	2.0	0.0000000000	False
empty string implicitly appears	0.0	0.0	0.0	2.0	0.0000000000	False
ultimately in generating languages	0.0	0.0	0.0	2.0	0.0000000000	False
language supposing that language	0.0	0.0	0.0	2.0	0.0000000000	False
language can be generated	0.0	0.0	0.0	4.0	0.0000000000	False
generated by a right	0.0	0.0	3.99630655586	8.0	0.3548387097	False
grammar which will generate	0.0	0.0	0.0	2.0	0.0000000000	False
generated by a left	0.0	0.0	3.9972299169	6.0	0.0000000000	False
left linear grammar left	0.0	0.0	0.0	2.0	0.0000000000	False
general kind of production	0.0	0.0	0.0	2.0	0.0000000000	False
star for any set	0.0	0.0	0.0	2.0	0.0000000000	False
set of all strings	0.0	0.0	0.0	4.0	0.0000000000	False
terminal symbol ok obtained	0.0	0.0	0.0	2.0	0.0000000000	False
symbol ok obtained form	0.0	0.0	0.0	2.0	0.0000000000	False
string ok the set	0.0	0.0	0.0	2.0	0.0000000000	False
cross t the set	0.0	0.0	0.0	2.0	0.0000000000	False
right so t star	0.0	0.0	0.0	2.0	0.0000000000	False
set which is obtained	0.0	0.0	0.0	2.0	0.0000000000	False
obtained as the union	0.0	0.0	0.0	2.0	0.0000000000	False
union of cartesian products	0.0	0.0	0.0	2.0	0.0000000000	False
equal to zero right	0.0	0.0	0.0	2.0	0.0000000000	False
define a binary operation	0.0	0.0	0.0	2.0	0.0000000000	False
binary operation called catenation	0.0	0.0	0.0	2.0	0.0000000000	False
catenation ok the effect	0.0	0.0	0.0	2.0	0.0000000000	False
two strings and put	0.0	0.0	0.0	2.0	0.0000000000	False
simplicity let us assume	0.0	0.0	0.0	2.0	0.0000000000	False
assume that the set	0.0	0.0	0.0	2.0	0.0000000000	False
call those two symbols	0.0	0.0	0.0	2.0	0.0000000000	False
string in t star	0.0	0.0	0.0	4.0	0.0000000000	False
string in the set	0.0	0.0	0.0	2.0	0.0000000000	False
dot for the moment	0.0	0.0	0.0	2.0	0.0000000000	False
produce the string ababbbab	0.0	0.0	0.0	2.0	0.0000000000	False
juxtapose the two strings	0.0	0.0	0.0	2.0	0.0000000000	False
strings the two strings	0.0	0.0	0.0	2.0	0.0000000000	False
binary operation on strings	0.0	0.0	0.0	2.0	0.0000000000	False
strings it just puts	0.0	0.0	0.0	2.0	0.0000000000	False
puts the two strings	0.0	0.0	0.0	2.0	0.0000000000	False
string is of length	0.0	0.0	0.0	4.0	0.0000000000	False
belongs to the set	0.0	0.0	2.9972299169	6.0	0.0000000000	False
length three this belongs	0.0	0.0	0.0	2.0	0.0000000000	False
cube and this string	0.0	0.0	0.0	2.0	0.0000000000	False
star and so catenation	0.0	0.0	0.0	2.0	0.0000000000	False
operation from t star	0.0	0.0	0.0	2.0	0.0000000000	False
star cross t star	0.0	0.0	0.0	2.0	0.0000000000	False
star to t star	0.0	0.0	0.0	2.0	0.0000000000	False
finite length and juxtapose	0.0	0.0	0.0	2.0	0.0000000000	False
juxtapose an empty string	0.0	0.0	0.0	2.0	0.0000000000	False
back the same string	0.0	0.0	0.0	2.0	0.0000000000	False
juxtapose some other string	0.0	0.0	0.0	2.0	0.0000000000	False
back the other string	0.0	0.0	0.0	2.0	0.0000000000	False
string so the empty	0.0	0.0	0.0	2.0	0.0000000000	False
string satisfies these conditions	0.0	0.0	0.0	2.0	0.0000000000	False
belonging to t star	0.0	0.0	0.0	2.0	0.0000000000	False
concatenated with the empty	0.0	0.0	0.0	2.0	0.0000000000	False
catenation is juxtaposition operation	0.0	0.0	0.0	2.0	0.0000000000	False
rid of the dot	0.0	0.0	0.0	2.0	0.0000000000	False
dot in between right	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon is in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact the identity element	0.0	0.0	0.0	2.0	0.0000000000	False
identity element for catenation	0.0	0.0	0.0	2.0	0.0000000000	False
addition right secondly catenation	0.0	0.0	0.0	2.0	0.0000000000	False
associative in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
catenation and under catenation	0.0	0.0	0.0	2.0	0.0000000000	False
monoid because this operation	0.0	0.0	0.0	2.0	0.0000000000	False
arbitrary context sensitive grammar	0.0	0.0	0.0	2.0	0.0000000000	False
production of a context	0.0	0.0	0.0	2.0	0.0000000000	False
grammar what it specifies	0.0	0.0	0.0	2.0	0.0000000000	False
appears in some context	0.0	0.0	0.0	2.0	0.0000000000	False
string so which means	0.0	0.0	0.0	2.0	0.0000000000	False
means that the context	0.0	0.0	0.0	2.0	0.0000000000	False
rewriting i can replace	0.0	0.0	0.0	2.0	0.0000000000	False
drew what this production	0.0	0.0	0.0	2.0	0.0000000000	False
large in the generation	0.0	0.0	0.0	2.0	0.0000000000	False
arbitrary string and turns	0.0	0.0	0.0	2.0	0.0000000000	False
minimal a minimal shell	0.0	0.0	0.0	2.0	0.0000000000	False
case of our context	0.0	0.0	2.9972299169	6.0	0.0000000000	False
epsilon on both sides	0.0	0.0	0.0	2.0	0.0000000000	False
appears in an empty	0.0	0.0	0.0	2.0	0.0000000000	False
empty context which means	0.0	0.0	0.0	2.0	0.0000000000	False
means that you don	0.0	0.0	0.0	2.0	0.0000000000	False
appears on either side	0.0	0.0	0.0	2.0	0.0000000000	False
specifies the smallest kernel	0.0	0.0	0.0	2.0	0.0000000000	False
string in the generation	0.0	0.0	0.0	2.0	0.0000000000	False
occurs in the string	0.0	0.0	0.0	2.0	0.0000000000	False
replacing by this rule	0.0	0.0	0.0	2.0	0.0000000000	False
sense that this padding	0.0	0.0	0.0	2.0	0.0000000000	False
inclusive meaning of context	0.0	0.0	0.0	2.0	0.0000000000	False
meaning of context sensitiveness	0.0	0.0	0.0	2.0	0.0000000000	False
padding around that non	0.0	0.0	0.0	2.0	0.0000000000	False
symbol which will enable	0.0	0.0	0.0	2.0	0.0000000000	False
case of a context	0.0	0.0	0.0	2.0	0.0000000000	False
grammar the minimal padding	0.0	0.0	0.0	2.0	0.0000000000	False
rules can be thought	0.0	0.0	0.0	2.0	0.0000000000	False
thought of as rules	0.0	0.0	0.0	2.0	0.0000000000	False
rules in the context	0.0	0.0	0.0	2.0	0.0000000000	False
sides of the padding	0.0	0.0	0.0	2.0	0.0000000000	False
padding the minimal padding	0.0	0.0	0.0	2.0	0.0000000000	False
padding that you require	0.0	0.0	0.0	2.0	0.0000000000	False
require is the empty	0.0	0.0	0.0	2.0	0.0000000000	False
empty string which means	0.0	0.0	0.0	2.0	0.0000000000	False
care what the rest	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the string	0.0	0.0	0.0	4.0	0.0000000000	False
out that the context	0.0	0.0	0.0	2.0	0.0000000000	False
context sensitivity into account	0.0	0.0	0.0	2.0	0.0000000000	False
deal with the programming	0.0	0.0	0.0	2.0	0.0000000000	False
language as a context	0.0	0.0	0.0	2.0	0.0000000000	False
free grammar as generated	0.0	0.0	0.0	2.0	0.0000000000	False
generated by a context	0.0	0.0	0.0	2.0	0.0000000000	False
free grammar and deal	0.0	0.0	0.0	2.0	0.0000000000	False
deal with a context	0.0	0.0	0.0	2.0	0.0000000000	False
typical context sensitive feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature even in languages	0.0	0.0	0.0	2.0	0.0000000000	False
check on these context	0.0	0.0	0.0	2.0	0.0000000000	False
issues ok so undeclared	0.0	0.0	0.0	2.0	0.0000000000	False
efficient algorithms to recognize	0.0	0.0	0.0	4.0	0.0000000000	False
pause context sensitive languages	0.0	0.0	0.0	2.0	0.0000000000	False
context sensitive languages represented	0.0	0.0	0.0	2.0	0.0000000000	False
pause context free grammars	0.0	0.0	0.0	2.0	0.0000000000	False
linear algorithms available linear	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms available for phrasing	0.0	0.0	0.0	2.0	0.0000000000	False
phrasing context sensitive grammars	0.0	0.0	0.0	2.0	0.0000000000	False
part of the semantics	0.0	0.0	0.0	2.0	0.0000000000	False
sensitive aspects many people	0.0	0.0	0.0	2.0	0.0000000000	False
synonymous with the semantics	0.0	0.0	0.0	2.0	0.0000000000	False
semantics of the language	0.0	0.0	0.0	4.0	0.0000000000	False
context sensitive every language	0.0	0.0	0.0	2.0	0.0000000000	False
sensitive every language generated	0.0	0.0	0.0	2.0	0.0000000000	False
regular one every language	0.0	0.0	0.0	2.0	0.0000000000	False
rules may be left	0.0	0.0	0.0	2.0	0.0000000000	False
left linear such grammars	0.0	0.0	0.0	2.0	0.0000000000	False
conversion is what helps	0.0	0.0	0.0	2.0	0.0000000000	False
helps us to design	0.0	0.0	0.0	2.0	0.0000000000	False
design machines for recognizing	0.0	0.0	0.0	2.0	0.0000000000	False
machines for recognizing languages	0.0	0.0	0.0	2.0	0.0000000000	False
languages of this grammars	0.0	0.0	0.0	2.0	0.0000000000	False
subject of the theory	0.0	0.0	0.0	2.0	0.0000000000	False
regular if there exists	0.0	0.0	0.0	2.0	0.0000000000	False
exists a regular grammar	0.0	0.0	0.0	2.0	0.0000000000	False
regular grammar which generates	0.0	0.0	0.0	2.0	0.0000000000	False
free it there exists	0.0	0.0	0.0	2.0	0.0000000000	False
free grammar that generates	0.0	0.0	0.0	2.0	0.0000000000	False
sensitive if there exists	0.0	0.0	0.0	2.0	0.0000000000	False
sensitive grammar that generates	0.0	0.0	0.0	2.0	0.0000000000	False
language you have generated	0.0	0.0	0.0	2.0	0.0000000000	False
free but the language	0.0	0.0	0.0	2.0	0.0000000000	False
generate you have written	0.0	0.0	0.0	2.0	0.0000000000	False
written context sensitive grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar for a language	0.0	0.0	0.0	2.0	0.0000000000	False
free in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
out with a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
purely which is context	0.0	0.0	0.0	2.0	0.0000000000	False
generated by the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
long before any grammar	0.0	0.0	0.0	2.0	0.0000000000	False
large number of numerals	0.0	0.0	0.0	2.0	0.0000000000	False
notion of the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar in natural language	0.0	0.0	0.0	2.0	0.0000000000	False
language and um sanskrit	0.0	0.0	0.0	2.0	0.0000000000	False
rigorous um art form	0.0	0.0	0.0	2.0	0.0000000000	False
neat we have evolved	0.0	0.0	0.0	2.0	0.0000000000	False
evolved such neat notation	0.0	0.0	0.0	2.0	0.0000000000	False
neat notation for numbers	0.0	0.0	0.0	2.0	0.0000000000	False
numerals the terminal set	0.0	0.0	0.0	2.0	0.0000000000	False
set is the set	0.0	0.0	0.0	2.0	0.0000000000	False
require just one non	0.0	0.0	0.0	2.0	0.0000000000	False
replaced on a digit	0.0	0.0	0.0	2.0	0.0000000000	False
replaced by a digit	0.0	0.0	0.0	2.0	0.0000000000	False
nice and simple grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar which is equivalent	0.0	0.0	0.0	2.0	0.0000000000	False
sense that the romans	0.0	0.0	0.0	2.0	0.0000000000	False
romans had a pattern	0.0	0.0	0.0	2.0	0.0000000000	False
pattern in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
fifty hundred five hundred	0.0	0.0	0.0	2.0	0.0000000000	False
hundred five hundred thousand	0.0	0.0	0.0	2.0	0.0000000000	False
thousand they had symbols	0.0	0.0	0.0	2.0	0.0000000000	False
symbols also for ten	0.0	0.0	0.0	2.0	0.0000000000	False
ten thousand fifty thousand	0.0	0.0	0.0	2.0	0.0000000000	False
fifty thousand um hundred	0.0	0.0	0.0	2.0	0.0000000000	False
thousand um hundred thousand	0.0	0.0	0.0	2.0	0.0000000000	False
thousand five hundred thousand	0.0	0.0	0.0	2.0	0.0000000000	False
require if you continue	0.0	0.0	0.0	2.0	0.0000000000	False
require an infinite set	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbols ok supposing	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbols which means	0.0	0.0	0.0	2.0	0.0000000000	False
grammar already is violated	0.0	0.0	0.0	2.0	0.0000000000	False
ten can not precede	0.0	0.0	0.0	2.0	0.0000000000	False
sense the roman numerals	0.0	0.0	0.0	2.0	0.0000000000	False
equivalent after all remember	0.0	0.0	0.0	2.0	0.0000000000	False
aim is to represent	0.0	0.0	0.0	2.0	0.0000000000	False
set but the language	0.0	0.0	0.0	2.0	0.0000000000	False
right so the language	0.0	0.0	0.0	2.0	0.0000000000	False
previewed for the context	0.0	0.0	0.0	2.0	0.0000000000	False
equivalent context free grammar	0.0	0.0	0.0	2.0	0.0000000000	False
introduce a new non	0.0	0.0	0.0	2.0	0.0000000000	False
rid of the non	0.0	0.0	0.0	2.0	0.0000000000	False
quickly see that grammar	0.0	0.0	0.0	2.0	0.0000000000	False
common occurrence of left	0.0	0.0	0.0	2.0	0.0000000000	False
occurrence of left parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
equivalent way of writing	0.0	0.0	0.0	2.0	0.0000000000	False
make a grammar smaller	0.0	0.0	0.0	2.0	0.0000000000	False
grammar smaller to reduce	0.0	0.0	0.0	2.0	0.0000000000	False
number of non terminals	0.0	0.0	5.9972299169	6.0	0.0000000000	False
important thing to reduce	0.0	0.0	0.0	2.0	0.0000000000	False
parsing of the language	0.0	0.0	0.0	2.0	0.0000000000	False
depends how many non	0.0	0.0	0.0	2.0	0.0000000000	False
idea so which means	0.0	0.0	0.0	2.0	0.0000000000	False
matter of decision making	0.0	0.0	0.0	2.0	0.0000000000	False
decision making to choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose the right kind	0.0	0.0	0.0	2.0	0.0000000000	False
right kind of grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar right correct kind	0.0	0.0	0.0	2.0	0.0000000000	False
correct kind of grammar	0.0	0.0	0.0	2.0	0.0000000000	False
facilitate an easy explanation	0.0	0.0	0.0	2.0	0.0000000000	False
explanation of the semantics	0.0	0.0	0.0	2.0	0.0000000000	False
fact the arabic numerals	0.0	0.0	0.0	2.0	0.0000000000	False
numerals um the left	0.0	0.0	0.0	2.0	0.0000000000	False
linear and the right	0.0	0.0	0.0	2.0	0.0000000000	False
difference they both equivalent	0.0	0.0	0.0	2.0	0.0000000000	False
terms of actual generation	0.0	0.0	0.0	2.0	0.0000000000	False
generation but the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact of the matter	0.0	0.0	0.0	2.0	0.0000000000	False
semantics for the left	0.0	0.0	0.0	2.0	0.0000000000	False
thatn the right linear	0.0	0.0	0.0	2.0	0.0000000000	False
semantics for the right	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms actually will choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose the right linear	0.0	0.0	0.0	2.0	0.0000000000	False
linear over the left	0.0	0.0	0.0	2.0	0.0000000000	False
constraint in those parsing	0.0	0.0	0.0	2.0	0.0000000000	False
calls in this case	0.0	0.0	0.0	2.0	0.0000000000	False
case will could lead	0.0	0.0	0.0	2.0	0.0000000000	False
case they would lead	0.0	0.0	0.0	2.0	0.0000000000	False
v.srinivasa rajkumar educational technology	0.0	0.0	0.0	2.0	0.0000000000	False
rajkumar educational technology i.i.t	0.0	0.0	0.0	2.0	0.0000000000	False
educational technology i.i.t delhi	0.0	0.0	0.0	2.0	0.0000000000	False
technology i.i.t delhi presents	0.0	0.0	0.0	2.0	0.0000000000	False
delhi presents a video	0.0	0.0	0.0	2.0	0.0000000000	False
video course on programming	0.0	0.0	0.0	2.0	0.0000000000	False
languages by dr.s.arun kumar	0.0	0.0	0.0	2.0	0.0000000000	False
kumar deptt of comp.sc	0.0	0.0	0.0	2.0	0.0000000000	False
today we will talk	0.0	0.0	0.0	2.0	0.0000000000	False
grammar our favorite context	0.0	0.0	0.0	2.0	0.0000000000	False
favorite context free grammar	0.0	0.0	0.0	2.0	0.0000000000	False
sentence that we generated	0.0	0.0	0.0	2.0	0.0000000000	False
generated using this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
aim is to generate	0.0	0.0	0.0	2.0	0.0000000000	False
sentence of this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
out of three possibilities	0.0	0.0	0.0	2.0	0.0000000000	False
chose one in fact	0.0	0.0	0.0	2.0	0.0000000000	False
possibility will not give	0.0	0.0	0.0	2.0	0.0000000000	False
give you this sentence	0.0	0.0	0.0	4.0	0.0000000000	False
sentence ok so out	0.0	0.0	0.0	2.0	0.0000000000	False
possibility a this possibility	0.0	0.0	0.0	2.0	0.0000000000	False
generate this ultimate sentence	0.0	0.0	0.0	2.0	0.0000000000	False
essential in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
chosen this um chosen	0.0	0.0	0.0	2.0	0.0000000000	False
open bracket open bracket	0.0	0.0	0.0	2.0	0.0000000000	False
possibility of either firing	0.0	0.0	0.0	2.0	0.0000000000	False
supposing you had chosen	0.0	0.0	0.0	2.0	0.0000000000	False
derivation of a sentence	0.0	0.0	0.0	4.0	0.0000000000	False
sentence this particular order	0.0	0.0	0.0	2.0	0.0000000000	False
derivations after same sentence	0.0	0.0	0.0	2.0	0.0000000000	False
depending upon the order	0.0	0.0	0.0	2.0	0.0000000000	False
chose to apply productions	0.0	0.0	0.0	2.0	0.0000000000	False
productions in a sentence	0.0	0.0	0.0	2.0	0.0000000000	False
sentence in other words	0.0	0.0	0.0	2.0	0.0000000000	False
give you various orders	0.0	0.0	0.0	2.0	0.0000000000	False
chosen any particular order	0.0	0.0	0.0	2.0	0.0000000000	False
leftmost non terminal symbol	0.0	0.0	0.0	2.0	0.0000000000	False
derivations of this sentence	0.0	0.0	0.0	2.0	0.0000000000	False
talking about a context	0.0	0.0	0.0	2.0	0.0000000000	False
grammar and the replacement	0.0	0.0	0.0	2.0	0.0000000000	False
replacement of non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
terminals by their right	0.0	0.0	0.0	2.0	0.0000000000	False
sides in the production	0.0	0.0	0.0	2.0	0.0000000000	False
non terminal is chosen	0.0	0.0	0.0	2.0	0.0000000000	False
chosen first for replacement	0.0	0.0	0.0	2.0	0.0000000000	False
first for replacement provided	0.0	0.0	0.0	2.0	0.0000000000	False
steps in this derivation	0.0	0.0	0.0	2.0	0.0000000000	False
production you have applied	0.0	0.0	0.0	2.0	0.0000000000	False
justification which just tells	0.0	0.0	0.0	2.0	0.0000000000	False
tells you which production	0.0	0.0	0.0	2.0	0.0000000000	False
production number you applied	0.0	0.0	0.0	2.0	0.0000000000	False
essentially you could permute	0.0	0.0	0.0	2.0	0.0000000000	False
order of the productions	0.0	0.0	0.0	2.0	0.0000000000	False
productions of these applications	0.0	0.0	0.0	2.0	0.0000000000	False
choices either an application	0.0	0.0	0.0	2.0	0.0000000000	False
productions but your intermediate	0.0	0.0	0.0	2.0	0.0000000000	False
intermediate sentences your intermediate	0.0	0.0	0.0	2.0	0.0000000000	False
sentences your intermediate sentences	0.0	0.0	0.0	2.0	0.0000000000	False
application of these productions	0.0	0.0	0.0	2.0	0.0000000000	False
derived the same sentence	0.0	0.0	0.0	2.0	0.0000000000	False
productions for the derivation	0.0	0.0	0.0	2.0	0.0000000000	False
sentences in a context	0.0	0.0	0.0	2.0	0.0000000000	False
freeness of the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
derivation by just applying	0.0	0.0	0.0	2.0	0.0000000000	False
applying the same productions	0.0	0.0	0.0	2.0	0.0000000000	False
apply is still place	0.0	0.0	0.0	2.0	0.0000000000	False
alternative but to replace	0.0	0.0	0.0	2.0	0.0000000000	False
ordering on the application	0.0	0.0	0.0	2.0	0.0000000000	False
application of the productions	0.0	0.0	0.0	4.0	0.0000000000	False
order but other productions	0.0	0.0	0.0	2.0	0.0000000000	False
collapse them to give	0.0	0.0	0.0	2.0	0.0000000000	False
independence look for independence	0.0	0.0	0.0	2.0	0.0000000000	False
draw draw a tree	0.0	0.0	0.0	2.0	0.0000000000	False
tree of exact dependences	0.0	0.0	0.0	2.0	0.0000000000	False
dependencies in the applications	0.0	0.0	0.0	2.0	0.0000000000	False
applications of various productions	0.0	0.0	0.0	2.0	0.0000000000	False
productions so the root	0.0	0.0	0.0	2.0	0.0000000000	False
root of the parse	0.0	0.0	0.0	2.0	0.0000000000	False
generation of this sentence	0.0	0.0	0.0	2.0	0.0000000000	False
sentence the first production	0.0	0.0	0.0	2.0	0.0000000000	False
production that was applied	0.0	0.0	0.0	2.0	0.0000000000	False
symbol ok open parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
single symbol um remember	0.0	0.0	0.0	2.0	0.0000000000	False
convention that black denotes	0.0	0.0	0.0	2.0	0.0000000000	False
black denotes terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
symbols the eventual strings	0.0	0.0	0.0	2.0	0.0000000000	False
strings that you generate	0.0	0.0	0.0	2.0	0.0000000000	False
strings in black strings	0.0	0.0	0.0	2.0	0.0000000000	False
strings of the language	0.0	0.0	0.0	2.0	0.0000000000	False
language the colors denote	0.0	0.0	0.0	2.0	0.0000000000	False
colors denote um denote	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesis s close parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
generated then the right	0.0	0.0	0.0	2.0	0.0000000000	False
hand s be expanded	0.0	0.0	0.0	2.0	0.0000000000	False
expanded into open parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
matter in which order	0.0	0.0	0.0	4.0	0.0000000000	False
perform these two productions	0.0	0.0	0.0	2.0	0.0000000000	False
first s should produce	0.0	0.0	0.0	2.0	0.0000000000	False
color for the productions	0.0	0.0	0.0	2.0	0.0000000000	False
call the parse tree	0.0	0.0	0.0	2.0	0.0000000000	False
tree and the branches	0.0	0.0	0.0	2.0	0.0000000000	False
leaves of this tree	0.0	0.0	0.0	2.0	0.0000000000	False
read if you read	0.0	0.0	0.0	2.0	0.0000000000	False
tree if you read	0.0	0.0	0.0	2.0	0.0000000000	False
sentence that you generated	0.0	0.0	0.0	2.0	0.0000000000	False
open parenthesis open parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
close parenthesis close parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
parse tree for generating	0.0	0.0	0.0	2.0	0.0000000000	False
defined for every sentence	0.0	0.0	0.0	2.0	0.0000000000	False
important from the point	0.0	0.0	0.0	2.0	0.0000000000	False
view of compiling language	0.0	0.0	0.0	2.0	0.0000000000	False
implementation from the point	0.0	0.0	0.0	2.0	0.0000000000	False
view of specifying semantics	0.0	0.0	0.0	2.0	0.0000000000	False
sacrocite and the fact	0.0	0.0	0.0	2.0	0.0000000000	False
traversing this parse parse	0.0	0.0	0.0	2.0	0.0000000000	False
parse tree as presenting	0.0	0.0	0.0	2.0	0.0000000000	False
presenting the partial order	0.0	0.0	0.0	2.0	0.0000000000	False
order in the firing	0.0	0.0	0.0	2.0	0.0000000000	False
traversals of the parse	0.0	0.0	0.0	2.0	0.0000000000	False
tree of the sentence	0.0	0.0	0.0	2.0	0.0000000000	False
necessarily a unique derivation	0.0	0.0	0.0	2.0	0.0000000000	False
traversal of the tree	0.0	0.0	0.0	2.0	0.0000000000	False
topological sort just takes	0.0	0.0	0.0	2.0	0.0000000000	False
takes a partial order	0.0	0.0	0.0	2.0	0.0000000000	False
partial order and linearizes	0.0	0.0	0.0	2.0	0.0000000000	False
linearizes it you sort	0.0	0.0	0.0	2.0	0.0000000000	False
provide a linear order	0.0	0.0	0.0	2.0	0.0000000000	False
linear order a total	0.0	0.0	0.0	2.0	0.0000000000	False
order a total order	0.0	0.0	0.0	2.0	0.0000000000	False
partial order are maintained	0.0	0.0	0.0	2.0	0.0000000000	False
maintained but their dependencies	0.0	0.0	0.0	2.0	0.0000000000	False
dependencies do not exists	0.0	0.0	0.0	2.0	0.0000000000	False
exists you might place	0.0	0.0	0.0	2.0	0.0000000000	False
linearization of a tree	0.0	0.0	0.0	2.0	0.0000000000	False
fact for partial orders	0.0	0.0	0.0	2.0	0.0000000000	False
order is completely defined	0.0	0.0	0.0	2.0	0.0000000000	False
defined by the set	0.0	0.0	0.0	4.0	0.0000000000	False
essentially a parsed tree	0.0	0.0	0.0	2.0	0.0000000000	False
order by the set	0.0	0.0	0.0	2.0	0.0000000000	False
traverses you can make	0.0	0.0	0.0	2.0	0.0000000000	False
property in the theory	0.0	0.0	0.0	2.0	0.0000000000	False
theory of partial orders	0.0	0.0	0.0	2.0	0.0000000000	False
orders of that set	0.0	0.0	0.0	2.0	0.0000000000	False
syntactical in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
compiling or language def	0.0	0.0	0.0	2.0	0.0000000000	False
def um or language	0.0	0.0	0.0	2.0	0.0000000000	False
language implementation it doesn	0.0	0.0	0.0	2.0	0.0000000000	False
symbols that are coming	0.0	0.0	0.0	2.0	0.0000000000	False
symbols but any kind	0.0	0.0	0.0	2.0	0.0000000000	False
type of terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
make a clear distinction	0.0	0.0	0.0	2.0	0.0000000000	False
identifier and a operator	0.0	0.0	0.0	2.0	0.0000000000	False
call a concrete parse	0.0	0.0	0.0	2.0	0.0000000000	False
tree where we make	0.0	0.0	0.0	2.0	0.0000000000	False
symbols they are leaves	0.0	0.0	0.0	2.0	0.0000000000	False
leaves of the parse	0.0	0.0	0.0	2.0	0.0000000000	False
sentence you actually make	0.0	0.0	0.0	2.0	0.0000000000	False
clear that our intention	0.0	0.0	0.0	2.0	0.0000000000	False
intention was to define	0.0	0.0	0.0	2.0	0.0000000000	False
language of boolean expressions	0.0	0.0	3.9974533107	6.0	0.0000000000	False
expressions where the operations	0.0	0.0	0.0	2.0	0.0000000000	False
distinction between the operands	0.0	0.0	0.0	2.0	0.0000000000	False
operators is what leads	0.0	0.0	0.0	2.0	0.0000000000	False
call abstract syntax tree	0.0	0.0	0.0	2.0	0.0000000000	False
syntax tree actually elevates	0.0	0.0	0.0	2.0	0.0000000000	False
tree actually elevates replaces	0.0	0.0	0.0	2.0	0.0000000000	False
elevates replaces non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
easily elevate the operators	0.0	0.0	0.0	2.0	0.0000000000	False
nodes of this tree	0.0	0.0	0.0	2.0	0.0000000000	False
tree we could talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk we can talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk of a root	0.0	0.0	0.0	2.0	0.0000000000	False
syntax tree the operators	0.0	0.0	0.0	2.0	0.0000000000	False
identifiers of the operands	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
language the ultimate programming	0.0	0.0	0.0	2.0	0.0000000000	False
interested in the non	0.0	0.0	0.0	2.0	0.0000000000	False
symbols the concrete parts	0.0	0.0	0.0	2.0	0.0000000000	False
parts of the language	0.0	0.0	0.0	2.0	0.0000000000	False
real down to earth	0.0	0.0	0.0	2.0	0.0000000000	False
symbols in any programming	0.0	0.0	0.0	2.0	0.0000000000	False
language have some meaning	0.0	0.0	0.0	2.0	0.0000000000	False
bring about this distinction	0.0	0.0	0.0	2.0	0.0000000000	False
interested in what order	0.0	0.0	0.0	2.0	0.0000000000	False
operators on the operands	0.0	0.0	0.0	2.0	0.0000000000	False
reason why we included	0.0	0.0	0.0	2.0	0.0000000000	False
essential to have parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
uniform post fix notion	0.0	0.0	0.0	2.0	0.0000000000	False
notion or a uniform	0.0	0.0	0.0	2.0	0.0000000000	False
fact every language construct	0.0	0.0	0.0	2.0	0.0000000000	False
construct can be regarded	0.0	0.0	0.0	2.0	0.0000000000	False
regarded as a operator	0.0	0.0	0.0	2.0	0.0000000000	False
interested in giving meanings	0.0	0.0	0.0	2.0	0.0000000000	False
giving meanings to languages	0.0	0.0	0.0	2.0	0.0000000000	False
interested in the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
arithmetic calculations in school	0.0	0.0	0.0	2.0	0.0000000000	False
expression you can choose	0.0	0.0	0.0	2.0	0.0000000000	False
order so the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
syntax to an abstract	0.0	0.0	0.0	2.0	0.0000000000	False
tree of the expression	0.0	0.0	0.0	2.0	0.0000000000	False
oftenly using these abstract	0.0	0.0	0.0	2.0	0.0000000000	False
mind so now keeping	0.0	0.0	0.0	2.0	0.0000000000	False
mind let us define	0.0	0.0	0.0	2.0	0.0000000000	False
define a small programming	0.0	0.0	0.0	2.0	0.0000000000	False
boolean variables and expressions	0.0	0.0	0.0	2.0	0.0000000000	False
syntax of this programming	0.0	0.0	0.0	2.0	0.0000000000	False
programs in the language	0.0	0.0	0.0	2.0	0.0000000000	False
language i can generate	0.0	0.0	0.0	2.0	0.0000000000	False
generate from the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
programs of the language	0.0	0.0	0.0	2.0	0.0000000000	False
programs we are talking	0.0	0.0	0.0	2.0	0.0000000000	False
formalizing it and giving	0.0	0.0	0.0	2.0	0.0000000000	False
giving rules production rules	0.0	0.0	0.0	2.0	0.0000000000	False
rules for the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
compiler or a translator	0.0	0.0	0.0	2.0	0.0000000000	False
translator for this language	0.0	0.0	0.0	2.0	0.0000000000	False
require to specify things	0.0	0.0	0.0	2.0	0.0000000000	False
coding of various constructs	0.0	0.0	0.0	2.0	0.0000000000	False
part of the context	0.0	0.0	0.0	2.0	0.0000000000	False
context free grammar notation	0.0	0.0	0.0	2.0	0.0000000000	False
bark of a tree	0.0	0.0	0.0	2.0	0.0000000000	False
branches of parse trees	0.0	0.0	0.0	2.0	0.0000000000	False
coded in this color	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of entities commands	0.0	0.0	0.0	2.0	0.0000000000	False
commands and boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
brown for boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
atomic commands the assignment	0.0	0.0	0.0	2.0	0.0000000000	False
green all other compound	0.0	0.0	0.0	2.0	0.0000000000	False
program or the sentence	0.0	0.0	0.0	2.0	0.0000000000	False
sentence of the language	0.0	0.0	2.9974533107	6.0	0.0000000000	False
change my color coding	0.0	0.0	0.0	2.0	0.0000000000	False
coding i will inform	0.0	0.0	0.0	2.0	0.0000000000	False
language and the sentences	0.0	0.0	0.0	2.0	0.0000000000	False
sentences of this language	0.0	0.0	0.0	2.0	0.0000000000	False
language are all programs	0.0	0.0	0.0	2.0	0.0000000000	False
unlike say a language	0.0	0.0	0.0	2.0	0.0000000000	False
program in this language	0.0	0.0	0.0	2.0	0.0000000000	False
language so this rule	0.0	0.0	0.0	2.0	0.0000000000	False
specifies that a program	0.0	0.0	0.0	2.0	0.0000000000	False
program is any command	0.0	0.0	0.0	2.0	0.0000000000	False
program of this language	0.0	0.0	0.0	2.0	0.0000000000	False
command of this language	0.0	0.0	0.0	2.0	0.0000000000	False
language well a command	0.0	0.0	0.0	2.0	0.0000000000	False
sequence of two commands	0.0	0.0	0.0	2.0	0.0000000000	False
four productions c arrow	0.0	0.0	0.0	2.0	0.0000000000	False
arrow a c arrow	0.0	0.0	0.0	2.0	0.0000000000	False
semicolon c c arrow	0.0	0.0	0.0	2.0	0.0000000000	False
right so this bar	0.0	0.0	0.0	2.0	0.0000000000	False
specifies the various alternatives	0.0	0.0	0.0	2.0	0.0000000000	False
languages are called statement	0.0	0.0	0.0	2.0	0.0000000000	False
level this the thing	0.0	0.0	0.0	2.0	0.0000000000	False
definitions of the programming	0.0	0.0	0.0	2.0	0.0000000000	False
programming language in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of several levels	0.0	0.0	0.0	2.0	0.0000000000	False
level ok this command	0.0	0.0	0.0	2.0	0.0000000000	False
command level essentially tells	0.0	0.0	0.0	2.0	0.0000000000	False
commands from simpler commands	0.0	0.0	0.0	2.0	0.0000000000	False
command is a command	0.0	0.0	0.0	4.0	0.0000000000	False
command and the sequence	0.0	0.0	0.0	2.0	0.0000000000	False
sequence of two sequencing	0.0	0.0	0.0	2.0	0.0000000000	False
simple or compound commands	0.0	0.0	0.0	2.0	0.0000000000	False
blue this semi colon	0.0	0.0	0.0	2.0	0.0000000000	False
colon is the reserved	0.0	0.0	0.0	2.0	0.0000000000	False
word of the language	0.0	0.0	0.0	2.0	0.0000000000	False
black so the sequence	0.0	0.0	0.0	2.0	0.0000000000	False
command this conditional compound	0.0	0.0	0.0	2.0	0.0000000000	False
possibly a compound command	0.0	0.0	0.0	2.0	0.0000000000	False
level of grammar specification	0.0	0.0	0.0	2.0	0.0000000000	False
form let us assume	0.0	0.0	0.0	2.0	0.0000000000	False
expression then v assigned	0.0	0.0	0.0	2.0	0.0000000000	False
boolean so the language	0.0	0.0	0.0	2.0	0.0000000000	False
expressions i am defining	0.0	0.0	0.0	2.0	0.0000000000	False
defining a different grammar	0.0	0.0	0.0	2.0	0.0000000000	False
grammar just for variation	0.0	0.0	0.0	2.0	0.0000000000	False
defined but that grammar	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a hatch	0.0	0.0	0.0	2.0	0.0000000000	False
thing in that grammar	0.0	0.0	0.0	2.0	0.0000000000	False
constant true and false	0.0	0.0	0.0	2.0	0.0000000000	False
expression the terminal false	0.0	0.0	0.0	2.0	0.0000000000	False
symbols of the language	0.0	0.0	0.0	2.0	0.0000000000	False
make compound boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
grammar in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
designed a fully parenthesized	0.0	0.0	0.0	2.0	0.0000000000	False
atomic i have enclosed	0.0	0.0	0.0	2.0	0.0000000000	False
enclosed a new parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
supposing instead we defined	0.0	0.0	0.0	2.0	0.0000000000	False
define such a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
generated by this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
expand this and replace	0.0	0.0	4.99575551783	10.0	0.1726618705	False
make a different derivation	0.0	0.0	0.0	2.0	0.0000000000	False
expand this and give	0.0	0.0	0.0	2.0	0.0000000000	False
two then i chose	0.0	0.0	0.0	2.0	0.0000000000	False
brackets in the language	0.0	0.0	0.0	2.0	0.0000000000	False
give you the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
derivations with different syntax	0.0	0.0	0.0	2.0	0.0000000000	False
two different syntax trees	0.0	0.0	0.0	2.0	0.0000000000	False
syntax trees actually affect	0.0	0.0	0.0	2.0	0.0000000000	False
meaning of this language	0.0	0.0	0.0	2.0	0.0000000000	False
two had the values	0.0	0.0	0.0	2.0	0.0000000000	False
false then the evaluation	0.0	0.0	0.0	2.0	0.0000000000	False
evaluation of this syntax	0.0	0.0	0.0	4.0	0.0000000000	False
syntax tree would give	0.0	0.0	0.0	4.0	0.0000000000	False
true and the evaluation	0.0	0.0	0.0	2.0	0.0000000000	False
give you a value	0.0	0.0	0.0	2.0	0.0000000000	False
grammar for example falls	0.0	0.0	0.0	2.0	0.0000000000	False
language with unique meanings	0.0	0.0	0.0	2.0	0.0000000000	False
call such a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguous but the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
handle for the specification	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguous if there exists	0.0	0.0	0.0	2.0	0.0000000000	False
sentence in the language	0.0	0.0	0.0	4.0	0.0000000000	False
right so so ambiguity	0.0	0.0	0.0	2.0	0.0000000000	False
constraint in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
two different parse tree	0.0	0.0	0.0	2.0	0.0000000000	False
parse tree those parse	0.0	0.0	0.0	2.0	0.0000000000	False
tree those parse trees	0.0	0.0	0.0	2.0	0.0000000000	False
translating which means running	0.0	0.0	0.0	2.0	0.0000000000	False
problem of executing programs	0.0	0.0	0.0	2.0	0.0000000000	False
executing programs in order	0.0	0.0	0.0	2.0	0.0000000000	False
order to get meanings	0.0	0.0	0.0	2.0	0.0000000000	False
program to be interpreted	0.0	0.0	0.0	2.0	0.0000000000	False
compiler for that program	0.0	0.0	0.0	2.0	0.0000000000	False
program and the user	0.0	0.0	0.0	2.0	0.0000000000	False
user of that programming	0.0	0.0	0.0	2.0	0.0000000000	False
interpreted right so ambiguity	0.0	0.0	0.0	2.0	0.0000000000	False
means the execution behavior	0.0	0.0	0.0	2.0	0.0000000000	False
execution behavior of programs	0.0	0.0	0.0	2.0	0.0000000000	False
lot of our programming	0.0	0.0	0.0	4.0	0.0000000000	False
unambiguous yeah this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
tree for every sentence	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguity in most languages	0.0	0.0	0.0	2.0	0.0000000000	False
implicit order of evaluation	0.0	0.0	0.0	2.0	0.0000000000	False
expression without any parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
assumed that the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
tree for this expression	0.0	0.0	0.0	2.0	0.0000000000	False
expression if you remove	0.0	0.0	0.0	2.0	0.0000000000	False
tree of this form	0.0	0.0	0.0	2.0	0.0000000000	False
construct a syntax tree	0.0	0.0	0.0	2.0	0.0000000000	False
tree supposing i removed	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguity then the order	0.0	0.0	0.0	2.0	0.0000000000	False
multiplication should precede addition	0.0	0.0	0.0	2.0	0.0000000000	False
precede addition other wise	0.0	0.0	0.0	2.0	0.0000000000	False
mathematical notation most programming	0.0	0.0	0.0	2.0	0.0000000000	False
notation most programming languages	0.0	0.0	0.0	2.0	0.0000000000	False
construct has a perfect	0.0	0.0	0.0	2.0	0.0000000000	False
case of one construct	0.0	0.0	0.0	2.0	0.0000000000	False
v.srinivasa rajkumar educational technology	0.0	0.0	0.0	2.0	0.0000000000	False
rajkumar educational technology i.i.t	0.0	0.0	0.0	2.0	0.0000000000	False
educational technology i.i.t delhi	0.0	0.0	0.0	2.0	0.0000000000	False
technology i.i.t delhi presents	0.0	0.0	0.0	2.0	0.0000000000	False
delhi presents a video	0.0	0.0	0.0	2.0	0.0000000000	False
video course on programming	0.0	0.0	0.0	2.0	0.0000000000	False
languages by dr.s.arun kumar	0.0	0.0	0.0	2.0	0.0000000000	False
kumar deptt of comp.sc	0.0	0.0	0.0	2.0	0.0000000000	False
syntax welcome to lecture	0.0	0.0	0.0	2.0	0.0000000000	False
lecture five so today	0.0	0.0	0.0	2.0	0.0000000000	False
slightly more complicated programming	0.0	0.0	0.0	2.0	0.0000000000	False
brown i will change	0.0	0.0	0.0	2.0	0.0000000000	False
change um so programs	0.0	0.0	0.0	2.0	0.0000000000	False
commands and atomic commands	0.0	0.0	0.0	2.0	0.0000000000	False
commands and a context	0.0	0.0	0.0	2.0	0.0000000000	False
context free grammar notation	0.0	0.0	0.0	4.0	0.0000000000	False
grammar notation will remain	0.0	0.0	0.0	2.0	0.0000000000	False
grammar firstly the grammar	0.0	0.0	0.0	2.0	0.0000000000	False
black are reserve words	0.0	0.0	0.0	2.0	0.0000000000	False
reserve words so including	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguous if there exists	0.0	0.0	0.0	2.0	0.0000000000	False
sentence in the language	0.0	0.0	0.0	2.0	0.0000000000	False
tree the same sentence	0.0	0.0	0.0	2.0	0.0000000000	False
tree because the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
tree is just obtained	0.0	0.0	0.0	2.0	0.0000000000	False
obtained from the parse	0.0	0.0	0.0	2.0	0.0000000000	False
parse tree by elevating	0.0	0.0	0.0	2.0	0.0000000000	False
operators to the root	0.0	0.0	0.0	2.0	0.0000000000	False
root nodes and replacing	0.0	0.0	0.0	2.0	0.0000000000	False
restricted class of parse	0.0	0.0	0.0	2.0	0.0000000000	False
class of parse trees	0.0	0.0	0.0	2.0	0.0000000000	False
conditional and the loop	0.0	0.0	0.0	2.0	0.0000000000	False
loop i have eliminated	0.0	0.0	0.0	2.0	0.0000000000	False
introducing two reserved words	0.0	0.0	0.0	2.0	0.0000000000	False
words the closing bracket	0.0	0.0	0.0	2.0	0.0000000000	False
composition or the sequencing	0.0	0.0	0.0	2.0	0.0000000000	False
binary operator on commands	0.0	0.0	0.0	2.0	0.0000000000	False
assume c one semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
semicolon c two semicolon	0.0	0.0	0.0	4.0	0.0000000000	False
semicolon c three semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
atomic or compound commands	0.0	0.0	0.0	2.0	0.0000000000	False
compound commands i don	0.0	0.0	0.0	2.0	0.0000000000	False
two different parse trees	0.0	0.0	0.0	2.0	0.0000000000	False
triangles here to denote	0.0	0.0	0.0	2.0	0.0000000000	False
denote that these seats	0.0	0.0	0.0	2.0	0.0000000000	False
commands themselves can expand	0.0	0.0	0.0	2.0	0.0000000000	False
first this first semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
semicolon is the root	0.0	0.0	0.0	4.0	0.0000000000	False
semicolon is a right	0.0	0.0	0.0	2.0	0.0000000000	False
circle is a root	0.0	0.0	0.0	2.0	0.0000000000	False
root of the right	0.0	0.0	0.0	2.0	0.0000000000	False
case in which case	0.0	0.0	0.0	2.0	0.0000000000	False
root of the left	0.0	0.0	0.0	2.0	0.0000000000	False
root of the tree	0.0	0.0	0.0	2.0	0.0000000000	False
tree so strictly speaking	0.0	0.0	0.0	2.0	0.0000000000	False
speaking there is ambiguity	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguity in this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
operation in any programming	0.0	0.0	0.0	2.0	0.0000000000	False
two trees really correspond	0.0	0.0	0.0	2.0	0.0000000000	False
correspond to different bracketing	0.0	0.0	0.0	2.0	0.0000000000	False
tree for example corresponds	0.0	0.0	0.0	2.0	0.0000000000	False
corresponds to the bracketing	0.0	0.0	0.0	2.0	0.0000000000	False
corresponds to the case	0.0	0.0	0.0	2.0	0.0000000000	False
right so this corresponds	0.0	0.0	0.0	2.0	0.0000000000	False
two the whole thing	0.0	0.0	0.0	2.0	0.0000000000	False
semicolon c three right	0.0	0.0	0.0	2.0	0.0000000000	False
general um sequencing operation	0.0	0.0	0.0	2.0	0.0000000000	False
operation and the function	0.0	0.0	0.0	2.0	0.0000000000	False
implementation of the language	0.0	0.0	0.0	2.0	0.0000000000	False
regard to the semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
operation so the fact	0.0	0.0	0.0	2.0	0.0000000000	False
ambiguous does not matter	0.0	0.0	0.0	2.0	0.0000000000	False
behavior or the meanings	0.0	0.0	0.0	2.0	0.0000000000	False
meanings of the programs	0.0	0.0	0.0	2.0	0.0000000000	False
case things can change	0.0	0.0	0.0	2.0	0.0000000000	False
expression of boolean expressions	0.0	0.0	0.0	2.0	0.0000000000	False
value of boolean expression	0.0	0.0	0.0	2.0	0.0000000000	False
parse the boolean expression	0.0	0.0	0.0	2.0	0.0000000000	False
languages since algol sixty	0.0	0.0	0.0	2.0	0.0000000000	False
sixty use a notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation called the backus	0.0	0.0	0.0	2.0	0.0000000000	False
created by john backus	0.0	0.0	0.0	2.0	0.0000000000	False
john backus and peter	0.0	0.0	0.0	2.0	0.0000000000	False
backus and peter naur	0.0	0.0	0.0	2.0	0.0000000000	False
naur in the definition	0.0	0.0	0.0	2.0	0.0000000000	False
algol sixty the algol	0.0	0.0	0.0	2.0	0.0000000000	False
sixty the algol sixty	0.0	0.0	0.0	2.0	0.0000000000	False
rigorous syntactic form based	0.0	0.0	0.0	2.0	0.0000000000	False
free grammars to define	0.0	0.0	0.0	2.0	0.0000000000	False
define the language abs	0.0	0.0	0.0	2.0	0.0000000000	False
involved in the creation	0.0	0.0	0.0	2.0	0.0000000000	False
fortan language every fortan	0.0	0.0	0.0	2.0	0.0000000000	False
language every fortan compiler	0.0	0.0	0.0	2.0	0.0000000000	False
written by various people	0.0	0.0	0.0	2.0	0.0000000000	False
people gave different interpretations	0.0	0.0	0.0	2.0	0.0000000000	False
interpretations to the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
fortan comp fortan programs	0.0	0.0	0.0	2.0	0.0000000000	False
treated um the fortan	0.0	0.0	0.0	2.0	0.0000000000	False
compiler and moving programs	0.0	0.0	0.0	2.0	0.0000000000	False
machine or one compiler	0.0	0.0	0.0	2.0	0.0000000000	False
compiler to another compiler	0.0	0.0	0.0	2.0	0.0000000000	False
problem in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that you required	0.0	0.0	0.0	2.0	0.0000000000	False
required a whole team	0.0	0.0	0.0	2.0	0.0000000000	False
suit the new compiler	0.0	0.0	0.0	2.0	0.0000000000	False
architecture of the machine	0.0	0.0	0.0	2.0	0.0000000000	False
machine or it required	0.0	0.0	0.0	2.0	0.0000000000	False
patching up of programs	0.0	0.0	0.0	2.0	0.0000000000	False
form of theoretical study	0.0	0.0	0.0	2.0	0.0000000000	False
theoretical study and backus	0.0	0.0	0.0	2.0	0.0000000000	False
backus and naur define	0.0	0.0	0.0	2.0	0.0000000000	False
naur define the algol	0.0	0.0	0.0	2.0	0.0000000000	False
define the algol sixty	0.0	0.0	0.0	2.0	0.0000000000	False
language using this notation	0.0	0.0	0.0	2.0	0.0000000000	False
single symbol um single	0.0	0.0	0.0	2.0	0.0000000000	False
symbol um single character	0.0	0.0	0.0	2.0	0.0000000000	False
character non terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
words so they wrote	0.0	0.0	0.0	2.0	0.0000000000	False
statements within angle brackets	0.0	0.0	0.0	2.0	0.0000000000	False
mark on the type	0.0	0.0	0.0	2.0	0.0000000000	False
wrote all the productions	0.0	0.0	0.0	2.0	0.0000000000	False
productions in this form	0.0	0.0	0.0	2.0	0.0000000000	False
full the non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
non terminals being enclosed	0.0	0.0	0.0	2.0	0.0000000000	False
enclosed in angle brackets	0.0	0.0	0.0	2.0	0.0000000000	False
brackets and the arrow	0.0	0.0	0.0	2.0	0.0000000000	False
double colon and equals	0.0	0.0	0.0	2.0	0.0000000000	False
extended backus naur form	0.0	0.0	3.99786019971	6.0	0.0000000000	False
convince we should remember	0.0	0.0	0.0	2.0	0.0000000000	False
means is the introduction	0.0	0.0	0.0	2.0	0.0000000000	False
introduction of new non	0.0	0.0	0.0	2.0	0.0000000000	False
terminal symbols to aloow	0.0	0.0	0.0	2.0	0.0000000000	False
aloow for those kinds	0.0	0.0	0.0	2.0	0.0000000000	False
essentially uses the backus	0.0	0.0	0.0	2.0	0.0000000000	False
backus naur form extended	0.0	0.0	0.0	2.0	0.0000000000	False
form extended to include	0.0	0.0	0.0	2.0	0.0000000000	False
extended to include iterations	0.0	0.0	0.0	2.0	0.0000000000	False
include iterations in choice	0.0	0.0	0.0	2.0	0.0000000000	False
backus naur form production	0.0	0.0	0.0	2.0	0.0000000000	False
production of this form	0.0	0.0	2.99786019971	6.0	0.0000000000	False
form ok where alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha beta and gamma	0.0	0.0	0.0	2.0	0.0000000000	False
terminals or non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
notation for the backus	0.0	0.0	0.0	2.0	0.0000000000	False
backus naur form notation	0.0	0.0	0.0	2.0	0.0000000000	False
set of non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon or to beta	0.0	0.0	0.0	2.0	0.0000000000	False
notation this extended backus	0.0	0.0	0.0	2.0	0.0000000000	False
extended backus form notation	0.0	0.0	0.0	2.0	0.0000000000	False
machine if you run	0.0	0.0	0.0	2.0	0.0000000000	False
run the man pages	0.0	0.0	0.0	2.0	0.0000000000	False
pages for some command	0.0	0.0	0.0	2.0	0.0000000000	False
command you will find	0.0	0.0	0.0	2.0	0.0000000000	False
brackets of one kind	0.0	0.0	0.0	2.0	0.0000000000	False
square brackets to represent	0.0	0.0	0.0	2.0	0.0000000000	False
represent the various options	0.0	0.0	0.0	2.0	0.0000000000	False
options the several options	0.0	0.0	0.0	2.0	0.0000000000	False
options are either separated	0.0	0.0	0.0	2.0	0.0000000000	False
equivalent to this set	0.0	0.0	0.0	2.0	0.0000000000	False
definition of a programming	0.0	0.0	0.0	2.0	0.0000000000	False
aide in writing out	0.0	0.0	0.0	2.0	0.0000000000	False
writing out a grammar	0.0	0.0	0.0	2.0	0.0000000000	False
logical significance you wouldn	0.0	0.0	0.0	2.0	0.0000000000	False
allowed both the statements	0.0	0.0	0.0	2.0	0.0000000000	False
clause is an option	0.0	0.0	0.0	2.0	0.0000000000	False
ideally could be sep	0.0	0.0	0.0	2.0	0.0000000000	False
entity so you wouldn	0.0	0.0	0.0	2.0	0.0000000000	False
put that else clause	0.0	0.0	0.0	2.0	0.0000000000	False
clause in the definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition of your language	0.0	0.0	0.0	2.0	0.0000000000	False
language if your language	0.0	0.0	0.0	2.0	0.0000000000	False
allowed an else clause	0.0	0.0	0.0	2.0	0.0000000000	False
put the else clause	0.0	0.0	0.0	2.0	0.0000000000	False
amount of the number	0.0	0.0	0.0	2.0	0.0000000000	False
non terminal symbols remember	0.0	0.0	0.0	2.0	0.0000000000	False
remember that a programming	0.0	0.0	0.0	2.0	0.0000000000	False
language a real world	0.0	0.0	0.0	2.0	0.0000000000	False
real world programming language	0.0	0.0	0.0	2.0	0.0000000000	False
large piece of syntax	0.0	0.0	0.0	2.0	0.0000000000	False
adding these extra non	0.0	0.0	0.0	2.0	0.0000000000	False
significance for the compiler	0.0	0.0	0.0	2.0	0.0000000000	False
compiler which have significance	0.0	0.0	0.0	2.0	0.0000000000	False
accurately specifying the language	0.0	0.0	0.0	2.0	0.0000000000	False
language but other wise	0.0	0.0	0.0	2.0	0.0000000000	False
respect to ambiguity parsing	0.0	0.0	0.0	2.0	0.0000000000	False
significance from the non	0.0	0.0	0.0	2.0	0.0000000000	False
repetitions of some option	0.0	0.0	0.0	2.0	0.0000000000	False
alpha within braces beta	0.0	0.0	0.0	2.0	0.0000000000	False
productions of the form	0.0	0.0	0.0	2.0	0.0000000000	False
repetitions of the string	0.0	0.0	0.0	2.0	0.0000000000	False
production of the string	0.0	0.0	0.0	2.0	0.0000000000	False
denotes a zero number	0.0	0.0	0.0	2.0	0.0000000000	False
occurrence of a beta	0.0	0.0	0.0	2.0	0.0000000000	False
pascal manual the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
manual the syntax diagrams	0.0	0.0	0.0	2.0	0.0000000000	False
follow the arrow marks	0.0	0.0	0.0	2.0	0.0000000000	False
marks in the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
give you the productions	0.0	0.0	0.0	2.0	0.0000000000	False
ordinary context free notation	0.0	0.0	0.0	2.0	0.0000000000	False
reading manuals for learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning a new language	0.0	0.0	0.0	2.0	0.0000000000	False
last time we looked	0.0	0.0	0.0	2.0	0.0000000000	False
looked at a toy	0.0	0.0	0.0	2.0	0.0000000000	False
toy language which didn	0.0	0.0	0.0	2.0	0.0000000000	False
designed for the purposes	0.0	0.0	0.0	2.0	0.0000000000	False
purposes of teaching programming	0.0	0.0	0.0	2.0	0.0000000000	False
teaching programming languages compilers	0.0	0.0	0.0	2.0	0.0000000000	False
first course by nicolas	0.0	0.0	0.0	2.0	0.0000000000	False
worth himself the designer	0.0	0.0	0.0	2.0	0.0000000000	False
type the main features	0.0	0.0	0.0	2.0	0.0000000000	False
assignment sequencing bracketing looping	0.0	0.0	0.0	2.0	0.0000000000	False
arm conditional that means	0.0	0.0	0.0	2.0	0.0000000000	False
two one arm conditions	0.0	0.0	0.0	2.0	0.0000000000	False
arm conditions in sequence	0.0	0.0	0.0	2.0	0.0000000000	False
wise refinement of programs	0.0	0.0	0.0	2.0	0.0000000000	False
programs to be written	0.0	0.0	0.0	2.0	0.0000000000	False
written in a structured	0.0	0.0	0.0	2.0	0.0000000000	False
true to be nested	0.0	0.0	0.0	2.0	0.0000000000	False
refinement in the development	0.0	0.0	0.0	2.0	0.0000000000	False
equals for a production	0.0	0.0	0.0	2.0	0.0000000000	False
fashion so a program	0.0	0.0	0.0	2.0	0.0000000000	False
program my start symbol	0.0	0.0	0.0	2.0	0.0000000000	False
symbols and the non	0.0	0.0	0.0	2.0	0.0000000000	False
explicitly specify the terminals	0.0	0.0	0.0	2.0	0.0000000000	False
terminals and the non	0.0	0.0	0.0	2.0	0.0000000000	False
black and the non	0.0	0.0	0.0	2.0	0.0000000000	False
terminates with a dot	0.0	0.0	0.0	2.0	0.0000000000	False
dot with a period	0.0	0.0	0.0	2.0	0.0000000000	False
case of pascal programs	0.0	0.0	0.0	2.0	0.0000000000	False
pascal programs you terminate	0.0	0.0	0.0	2.0	0.0000000000	False
program with a dot	0.0	0.0	0.0	2.0	0.0000000000	False
dot right a block	0.0	0.0	0.0	2.0	0.0000000000	False
right a block consists	0.0	0.0	0.0	2.0	0.0000000000	False
consists of a declaration	0.0	0.0	0.0	4.0	0.0000000000	False
declaration and a statement	0.0	0.0	0.0	2.0	0.0000000000	False
names for the non	0.0	0.0	0.0	2.0	0.0000000000	False
single um single letter	0.0	0.0	0.0	2.0	0.0000000000	False
letter non terminal symbols	0.0	0.0	0.0	2.0	0.0000000000	False
enclosed in light brown	0.0	0.0	0.0	2.0	0.0000000000	False
constant declaration a constant	0.0	0.0	0.0	2.0	0.0000000000	False
declaration a constant declaration	0.0	0.0	0.0	2.0	0.0000000000	False
constant declaration which means	0.0	0.0	0.0	2.0	0.0000000000	False
means this word const	0.0	0.0	0.0	2.0	0.0000000000	False
const is a reserved	0.0	0.0	0.0	2.0	0.0000000000	False
put these in dark	0.0	0.0	0.0	2.0	0.0000000000	False
reason i have put	0.0	0.0	0.0	2.0	0.0000000000	False
put them in dark	0.0	0.0	0.0	2.0	0.0000000000	False
infinite set of identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
infinite set of numbers	0.0	0.0	0.0	2.0	0.0000000000	False
syntax the actual limits	0.0	0.0	0.0	2.0	0.0000000000	False
limits on the numbers	0.0	0.0	0.0	2.0	0.0000000000	False
numbers and the lengths	0.0	0.0	0.0	2.0	0.0000000000	False
lengths of the identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
part of the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
definition of the language	0.0	0.0	0.0	2.0	0.0000000000	False
light um this dark	0.0	0.0	0.0	2.0	0.0000000000	False
phase value as identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of data type	0.0	0.0	0.0	4.0	0.0000000000	False
commas so the commas	0.0	0.0	0.0	2.0	0.0000000000	False
commas are reserved word	0.0	0.0	0.0	2.0	0.0000000000	False
word of this language	0.0	0.0	0.0	2.0	0.0000000000	False
moment the word const	0.0	0.0	0.0	2.0	0.0000000000	False
occurrences of the clause	0.0	0.0	0.0	2.0	0.0000000000	False
terminated by a semi	0.0	0.0	3.99714693295	8.0	0.2978723404	False
single declaration and terminate	0.0	0.0	0.0	2.0	0.0000000000	False
constants the um terminate	0.0	0.0	0.0	2.0	0.0000000000	False
terminate the entire declaration	0.0	0.0	0.0	2.0	0.0000000000	False
declaration by a semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
declarations but you don	0.0	0.0	0.0	2.0	0.0000000000	False
variables separated by commas	0.0	0.0	0.0	2.0	0.0000000000	False
commas but the moment	0.0	0.0	0.0	2.0	0.0000000000	False
moment and the moment	0.0	0.0	0.0	2.0	0.0000000000	False
reserved word var occurring	0.0	0.0	0.0	2.0	0.0000000000	False
procedures and a procedure	0.0	0.0	0.0	2.0	0.0000000000	False
procedure has a procedure	0.0	0.0	0.0	2.0	0.0000000000	False
procedure as a reserved	0.0	0.0	0.0	2.0	0.0000000000	False
entire the entire procedure	0.0	0.0	0.0	2.0	0.0000000000	False
procedures in your program	0.0	0.0	0.0	2.0	0.0000000000	False
clauses they should occur	0.0	0.0	0.0	2.0	0.0000000000	False
occur in this order	0.0	0.0	0.0	2.0	0.0000000000	False
declarations this so declarations	0.0	0.0	0.0	2.0	0.0000000000	False
implicitly allows the production	0.0	0.0	0.0	2.0	0.0000000000	False
means an empty string	0.0	0.0	0.0	2.0	0.0000000000	False
case but a statement	0.0	0.0	0.0	2.0	0.0000000000	False
assignment an assignment statement	0.0	0.0	0.0	2.0	0.0000000000	False
statement is a statement	0.0	0.0	0.0	2.0	0.0000000000	False
statement where an assignment	0.0	0.0	0.0	2.0	0.0000000000	False
identifier um colon equals	0.0	0.0	0.0	2.0	0.0000000000	False
colon equals an expression	0.0	0.0	0.0	2.0	0.0000000000	False
equals an expression note	0.0	0.0	0.0	2.0	0.0000000000	False
relation between what identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
declared in this declaration	0.0	0.0	0.0	2.0	0.0000000000	False
declaration and what identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
statement so the syntax	0.0	0.0	0.0	2.0	0.0000000000	False
free but the language	0.0	0.0	0.0	2.0	0.0000000000	False
syntax of the language	0.0	0.0	0.0	4.0	0.0000000000	False
explicit procedure called statement	0.0	0.0	0.0	2.0	0.0000000000	False
identifier and implicit meaning	0.0	0.0	0.0	2.0	0.0000000000	False
declared as a procedure	0.0	0.0	0.0	2.0	0.0000000000	False
call is a reserved	0.0	0.0	0.0	2.0	0.0000000000	False
condition then a statement	0.0	0.0	0.0	2.0	0.0000000000	False
statement this whole thing	0.0	0.0	0.0	2.0	0.0000000000	False
statements you can call	0.0	0.0	0.0	2.0	0.0000000000	False
call as a sequence	0.0	0.0	0.0	2.0	0.0000000000	False
pair of brackets begin	0.0	0.0	0.0	2.0	0.0000000000	False
separated by a semicolon	0.0	0.0	0.0	2.0	0.0000000000	False
define in the end	0.0	0.0	0.0	2.0	0.0000000000	False
defined at the expense	0.0	0.0	0.0	2.0	0.0000000000	False
introducing new non terminals	0.0	0.0	0.0	2.0	0.0000000000	False
condition well the language	0.0	0.0	0.0	2.0	0.0000000000	False
expression e um note	0.0	0.0	0.0	2.0	0.0000000000	False
type available is integers	0.0	0.0	0.0	2.0	0.0000000000	False
integers the only expressions	0.0	0.0	0.0	2.0	0.0000000000	False
unary data type applies	0.0	0.0	0.0	2.0	0.0000000000	False
applies over all expressions	0.0	0.0	0.0	2.0	0.0000000000	False
unary condition this unary	0.0	0.0	0.0	2.0	0.0000000000	False
condition this unary predicate	0.0	0.0	0.0	2.0	0.0000000000	False
applies over all condi	0.0	0.0	0.0	2.0	0.0000000000	False
condi over all expressions	0.0	0.0	0.0	2.0	0.0000000000	False
predicate some unary predicate	0.0	0.0	0.0	2.0	0.0000000000	False
reason for choosing odd	0.0	0.0	0.0	2.0	0.0000000000	False
jump on not equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals in most hardware	0.0	0.0	0.0	2.0	0.0000000000	False
programs a large number	0.0	0.0	0.0	2.0	0.0000000000	False
number of your programs	0.0	0.0	0.0	2.0	0.0000000000	False
cases you could check	0.0	0.0	0.0	2.0	0.0000000000	False
simplified the original language	0.0	0.0	0.0	2.0	0.0000000000	False
single letter relational symbols	0.0	0.0	0.0	2.0	0.0000000000	False
greater than or equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals this is equals	0.0	0.0	0.0	2.0	0.0000000000	False
compiler for example allowed	0.0	0.0	0.0	2.0	0.0000000000	False
original pl zero compiler	0.0	0.0	0.0	2.0	0.0000000000	False
case of the expression	0.0	0.0	0.0	2.0	0.0000000000	False
two diff two extremes	0.0	0.0	0.0	2.0	0.0000000000	False
addition subtraction multiplication division	0.0	0.0	0.0	4.0	0.0000000000	False
sum of two expressions	0.0	0.0	0.0	2.0	0.0000000000	False
two expressions a difference	0.0	0.0	0.0	2.0	0.0000000000	False
difference of two expressions	0.0	0.0	0.0	2.0	0.0000000000	False
two expressions a product	0.0	0.0	0.0	2.0	0.0000000000	False
product of two expressions	0.0	0.0	0.0	2.0	0.0000000000	False
expressions or a quotient	0.0	0.0	0.0	2.0	0.0000000000	False
quotient of two expressions	0.0	0.0	0.0	2.0	0.0000000000	False
sequences which you understand	0.0	0.0	0.0	2.0	0.0000000000	False
priority order of evaluation	0.0	0.0	0.0	2.0	0.0000000000	False
define the expression language	0.0	0.0	0.0	4.0	0.0000000000	False
variable is an expression	0.0	0.0	0.0	2.0	0.0000000000	False
constant an integer constant	0.0	0.0	0.0	2.0	0.0000000000	False
statements of this grammar	0.0	0.0	0.0	2.0	0.0000000000	False
fully bracket every expression	0.0	0.0	0.0	2.0	0.0000000000	False
binary operator you put	0.0	0.0	0.0	2.0	0.0000000000	False
operator you put bracket	0.0	0.0	0.0	2.0	0.0000000000	False
bracket around the pair	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesis and e divided	0.0	0.0	0.0	2.0	0.0000000000	False
divided by e enclosed	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesis over every thing	0.0	0.0	0.0	2.0	0.0000000000	False
key key in parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
language in our abstract	0.0	0.0	0.0	2.0	0.0000000000	False
abstract syntax in string	0.0	0.0	0.0	2.0	0.0000000000	False
syntax in string form	0.0	0.0	0.0	2.0	0.0000000000	False
syntax as in tree	0.0	0.0	0.0	2.0	0.0000000000	False
notation can be translated	0.0	0.0	0.0	2.0	0.0000000000	False
abstract tree which preserves	0.0	0.0	0.0	2.0	0.0000000000	False
evaluation of the expressions	0.0	0.0	0.0	2.0	0.0000000000	False
expressions and vice versa	0.0	0.0	0.0	2.0	0.0000000000	False
versa given any abstract	0.0	0.0	0.0	2.0	0.0000000000	False
tree you can transform	0.0	0.0	0.0	2.0	0.0000000000	False
bracketed string of symbols	0.0	0.0	0.0	2.0	0.0000000000	False
view of the compiler	0.0	0.0	0.0	2.0	0.0000000000	False
tedious for every programmer	0.0	0.0	0.0	2.0	0.0000000000	False
write fully parenthesized versions	0.0	0.0	0.0	2.0	0.0000000000	False
fully parenthesized versions makes	0.0	0.0	0.0	2.0	0.0000000000	False
versions makes it makes	0.0	0.0	0.0	2.0	0.0000000000	False
strike a reasonable compromise	0.0	0.0	0.0	2.0	0.0000000000	False
conventions of mathematical notation	0.0	0.0	0.0	2.0	0.0000000000	False
parsing let me mention	0.0	0.0	0.0	2.0	0.0000000000	False
minus are also overloaded	0.0	0.0	0.0	2.0	0.0000000000	False
minus of a non	0.0	0.0	0.0	2.0	0.0000000000	False
minus are binary operators	0.0	0.0	0.0	2.0	0.0000000000	False
programming too um addition	0.0	0.0	0.0	2.0	0.0000000000	False
division multilic and multiplication	0.0	0.0	0.0	2.0	0.0000000000	False
types and over integer	0.0	0.0	0.0	2.0	0.0000000000	False
unary operators usually bind	0.0	0.0	0.0	2.0	0.0000000000	False
tightest ok that means	0.0	0.0	0.0	2.0	0.0000000000	False
means a unary operator	0.0	0.0	0.0	2.0	0.0000000000	False
expression enclosed in parenthesis	0.0	0.0	0.0	2.0	0.0000000000	False
symbol after the expression	0.0	0.0	0.0	2.0	0.0000000000	False
multiplication and division bind	0.0	0.0	0.0	2.0	0.0000000000	False
operators plus and minus	0.0	0.0	0.0	4.0	0.0000000000	False
minus ok however multiplication	0.0	0.0	0.0	2.0	0.0000000000	False
multiplication and division looses	0.0	0.0	0.0	2.0	0.0000000000	False
expression of this form	0.0	0.0	0.0	2.0	0.0000000000	False
form minus five star	0.0	0.0	0.0	2.0	0.0000000000	False
minus five star minus	0.0	0.0	0.0	2.0	0.0000000000	False
entire expression this minus	0.0	0.0	0.0	2.0	0.0000000000	False
expression this minus refers	0.0	0.0	0.0	2.0	0.0000000000	False
bracketing is this right	0.0	0.0	0.0	2.0	0.0000000000	False
account for the purpose	0.0	0.0	0.0	2.0	0.0000000000	False
giving your friendly user	0.0	0.0	0.0	2.0	0.0000000000	False
expression language is concerned	0.0	0.0	0.0	2.0	0.0000000000	False
concerned so that people	0.0	0.0	0.0	2.0	0.0000000000	False
normal knowledge of mathematics	0.0	0.0	0.0	2.0	0.0000000000	False
mathematic notation mathematical conventions	0.0	0.0	0.0	2.0	0.0000000000	False
mathematical conventions can write	0.0	0.0	0.0	2.0	0.0000000000	False
conventions can write programs	0.0	0.0	0.0	2.0	0.0000000000	False
write programs can write	0.0	0.0	0.0	2.0	0.0000000000	False
programs can write expressions	0.0	0.0	0.0	2.0	0.0000000000	False
provision of this convenience	0.0	0.0	0.0	2.0	0.0000000000	False
means that you require	0.0	0.0	0.0	2.0	0.0000000000	False
large number of non	0.0	0.0	0.0	2.0	0.0000000000	False
number of non terminals	0.0	0.0	0.0	4.0	0.0000000000	False
account all the conventions	0.0	0.0	0.0	2.0	0.0000000000	False
right so an expression	0.0	0.0	0.0	2.0	0.0000000000	False
expression is a term	0.0	0.0	0.0	2.0	0.0000000000	False
optionally an addition operator	0.0	0.0	0.0	2.0	0.0000000000	False
operator and a term	0.0	0.0	0.0	4.0	0.0000000000	False
right the addition operators	0.0	0.0	0.0	2.0	0.0000000000	False
binary plus and minus	0.0	0.0	0.0	2.0	0.0000000000	False
expression can be regarded	0.0	0.0	0.0	2.0	0.0000000000	False
signed or unsigned term	0.0	0.0	0.0	4.0	0.0000000000	False
term with an addition	0.0	0.0	0.0	2.0	0.0000000000	False
term um a term	0.0	0.0	0.0	2.0	0.0000000000	False
product of two factors	0.0	0.0	0.0	2.0	0.0000000000	False
factors or the quotient	0.0	0.0	0.0	2.0	0.0000000000	False
quotient of two factors	0.0	0.0	0.0	4.0	0.0000000000	False
multiplicative operator so star	0.0	0.0	0.0	2.0	0.0000000000	False
division are multiplicative operators	0.0	0.0	0.0	2.0	0.0000000000	False
factors right a factor	0.0	0.0	0.0	2.0	0.0000000000	False
factor is anything regarded	0.0	0.0	0.0	2.0	0.0000000000	False
number specified as part	0.0	0.0	0.0	2.0	0.0000000000	False
part of the expression	0.0	0.0	0.0	2.0	0.0000000000	False
expression in itself enclosed	0.0	0.0	0.0	2.0	0.0000000000	False
mutually and circularly non	0.0	0.0	0.0	2.0	0.0000000000	False
sum of two things	0.0	0.0	0.0	2.0	0.0000000000	False
out into a term	0.0	0.0	0.0	2.0	0.0000000000	False
large expression whose root	0.0	0.0	0.0	2.0	0.0000000000	False
expression whose root operation	0.0	0.0	0.0	2.0	0.0000000000	False
operation is an addition	0.0	0.0	0.0	2.0	0.0000000000	False
addition operation that means	0.0	0.0	0.0	2.0	0.0000000000	False
grammar really takes precedence	0.0	0.0	0.0	2.0	0.0000000000	False
takes precedence of operators	0.0	0.0	0.0	2.0	0.0000000000	False
absolutely essential for writing	0.0	0.0	0.0	2.0	0.0000000000	False
kind of syntactic definition	0.0	0.0	0.0	2.0	0.0000000000	False
expression as a signed	0.0	0.0	0.0	2.0	0.0000000000	False
signed or unsigned expression	0.0	0.0	0.0	2.0	0.0000000000	False
operator just in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of abstract trees	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesized expressions or abstract	0.0	0.0	0.0	2.0	0.0000000000	False
expressions or abstract syntax	0.0	0.0	0.0	2.0	0.0000000000	False
definition of the number	0.0	0.0	0.0	2.0	0.0000000000	False
number so a number	0.0	0.0	0.0	2.0	0.0000000000	False
signed or unsigned integer	0.0	0.0	0.0	2.0	0.0000000000	False
clause and the definition	0.0	0.0	0.0	2.0	0.0000000000	False
digits ok and digit	0.0	0.0	0.0	2.0	0.0000000000	False
defined as a character	0.0	0.0	0.0	2.0	0.0000000000	False
follow normal pascal rules	0.0	0.0	0.0	2.0	0.0000000000	False
pascal rules an identifier	0.0	0.0	0.0	2.0	0.0000000000	False
start with a letter	0.0	0.0	0.0	2.0	0.0000000000	False
letter in the case	0.0	0.0	0.0	2.0	0.0000000000	False
compiler all the alphabet	0.0	0.0	0.0	2.0	0.0000000000	False
modify it to include	0.0	0.0	0.0	2.0	0.0000000000	False
include lower case letters	0.0	0.0	0.0	2.0	0.0000000000	False
number from an identifier	0.0	0.0	0.0	2.0	0.0000000000	False
identifier is the occurrence	0.0	0.0	0.0	2.0	0.0000000000	False
occurrence of a letter	0.0	0.0	0.0	2.0	0.0000000000	False
symbol or a digit	0.0	0.0	0.0	2.0	0.0000000000	False
occurrence of the letter	0.0	0.0	0.0	2.0	0.0000000000	False
begin with a letter	0.0	0.0	0.0	2.0	0.0000000000	False
right and the reason	0.0	0.0	0.0	2.0	0.0000000000	False
rules also for recognizing	0.0	0.0	0.0	2.0	0.0000000000	False
recognizing that the word	0.0	0.0	0.0	2.0	0.0000000000	False
single word the word	0.0	0.0	0.0	2.0	0.0000000000	False
word the word begin	0.0	0.0	0.0	2.0	0.0000000000	False
begin has been recognized	0.0	0.0	0.0	2.0	0.0000000000	False
part of the process	0.0	0.0	0.0	2.0	0.0000000000	False
scanning or lexical analysis	0.0	0.0	0.0	2.0	0.0000000000	False
written in this language	0.0	0.0	0.0	2.0	0.0000000000	False
language it just consists	0.0	0.0	0.0	2.0	0.0000000000	False
divide up the program	0.0	0.0	0.0	2.0	0.0000000000	False
entity in the program	0.0	0.0	0.0	2.0	0.0000000000	False
recognize all those reserved	0.0	0.0	0.0	2.0	0.0000000000	False
words you should scan	0.0	0.0	0.0	2.0	0.0000000000	False
scan all the words	0.0	0.0	0.0	2.0	0.0000000000	False
treat them as identifiers	0.0	0.0	0.0	2.0	0.0000000000	False
out the entire constant	0.0	0.0	0.0	2.0	0.0000000000	False
constant in this case	0.0	0.0	0.0	2.0	0.0000000000	False
string of digits representing	0.0	0.0	0.0	4.0	0.0000000000	False
digits representing an integer	0.0	0.0	0.0	4.0	0.0000000000	False
unit so a scanner	0.0	0.0	0.0	2.0	0.0000000000	False
typically takes a file	0.0	0.0	0.0	2.0	0.0000000000	False
lexemes yeah the word	0.0	0.0	0.0	2.0	0.0000000000	False
desk file it means	0.0	0.0	0.0	2.0	0.0000000000	False
means any unbounded sequence	0.0	0.0	0.0	2.0	0.0000000000	False
unbounded sequence ordered sequence	0.0	0.0	0.0	2.0	0.0000000000	False
ordered sequence of object	0.0	0.0	0.0	2.0	0.0000000000	False
object so the process	0.0	0.0	0.0	2.0	0.0000000000	False
process of scanning converts	0.0	0.0	0.0	2.0	0.0000000000	False
scanning converts the file	0.0	0.0	0.0	2.0	0.0000000000	False
takes over the handling	0.0	0.0	0.0	2.0	0.0000000000	False
scanning would have created	0.0	0.0	0.0	2.0	0.0000000000	False
created a single lexeme	0.0	0.0	0.0	2.0	0.0000000000	False
scanning they would lose	0.0	0.0	0.0	2.0	0.0000000000	False
unit in the form	0.0	0.0	0.0	2.0	0.0000000000	False
form of some structured	0.0	0.0	0.0	2.0	0.0000000000	False
structured um an element	0.0	0.0	0.0	2.0	0.0000000000	False
element of some structured	0.0	0.0	0.0	2.0	0.0000000000	False
constant it is striped	0.0	0.0	0.0	2.0	0.0000000000	False
create a huge table	0.0	0.0	0.0	2.0	0.0000000000	False
table of the amount	0.0	0.0	0.0	2.0	0.0000000000	False
extract from the program	0.0	0.0	0.0	2.0	0.0000000000	False
checking for example type	0.0	0.0	0.0	2.0	0.0000000000	False
type checking runtime type	0.0	0.0	0.0	2.0	0.0000000000	False
checking runtime type checks	0.0	0.0	0.0	2.0	0.0000000000	False
runtime type checks compile	0.0	0.0	0.0	2.0	0.0000000000	False
checks compile time type	0.0	0.0	0.0	2.0	0.0000000000	False
compile time type checks	0.0	0.0	0.0	2.0	0.0000000000	False
type checks to detect	0.0	0.0	0.0	2.0	0.0000000000	False
detect un declared variables	0.0	0.0	0.0	2.0	0.0000000000	False
good way of detecting	0.0	0.0	0.0	2.0	0.0000000000	False
resident always in memory	0.0	0.0	0.0	2.0	0.0000000000	False
reference during the process	0.0	0.0	0.0	2.0	0.0000000000	False
context sensitive um issues	0.0	0.0	0.0	2.0	0.0000000000	False
assigned the right type	0.0	0.0	0.0	2.0	0.0000000000	False
expression in the right	0.0	0.0	0.0	2.0	0.0000000000	False
type so you require	0.0	0.0	0.0	2.0	0.0000000000	False
word or each lexeme	0.0	0.0	0.0	2.0	0.0000000000	False
correctly in the program	0.0	0.0	0.0	2.0	0.0000000000	False
semantics of the language	0.0	0.0	0.0	2.0	0.0000000000	True
start the next lecture	0.0	0.0	0.0	2.0	0.0000000000	False
basic notions of semantics	0.0	0.0	0.0	2.0	0.0000000000	False
toy language new features	0.0	0.0	0.0	2.0	0.0000000000	False
defined the syntactic definitions	0.0	0.0	0.0	2.0	0.0000000000	False
long as you parenthesizes	0.0	0.0	0.0	2.0	0.0000000000	False
parenthesizes the new features	0.0	0.0	0.0	2.0	0.0000000000	False
long as you define	0.0	0.0	0.0	2.0	0.0000000000	False
important how the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
give to the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
graders so i guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess sometime next week	0.0	0.0	0.0	2.0	0.0000000000	False
out the first homework	0.0	0.0	0.0	2.0	0.0000000000	False
assignment for this class	0.0	0.0	0.0	2.0	0.0000000000	False
class is this loud	0.0	0.0	0.0	2.0	0.0000000000	False
people in the back	0.0	0.0	0.0	2.0	0.0000000000	False
mic a bit louder	0.0	0.0	0.0	2.0	0.0000000000	False
out the first problem	0.0	0.0	0.0	2.0	0.0000000000	False
ll be two weeks	0.0	0.0	0.0	2.0	0.0000000000	False
problems in this class	0.0	0.0	0.0	2.0	0.0000000000	False
graders are usually members	0.0	0.0	0.0	2.0	0.0000000000	False
ll email the class	0.0	0.0	0.0	2.0	0.0000000000	False
class to solicit applications	0.0	0.0	0.0	2.0	0.0000000000	False
interested in becoming graders	0.0	0.0	0.0	2.0	0.0000000000	False
graders for this class	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a fun	0.0	0.0	0.0	2.0	0.0000000000	False
four times this quarter	0.0	0.0	0.0	2.0	0.0000000000	False
spend one evening staying	0.0	0.0	0.0	2.0	0.0000000000	False
grading all the homework	0.0	0.0	0.0	2.0	0.0000000000	False
half of the teaching	0.0	0.0	0.0	2.0	0.0000000000	False
good solution and amazing	0.0	0.0	0.0	2.0	0.0000000000	False
solution and amazing solution	0.0	0.0	0.0	2.0	0.0000000000	False
solution and to give	0.0	0.0	0.0	2.0	0.0000000000	False
solutions becoming a grader	0.0	0.0	0.0	2.0	0.0000000000	False
graders are paid positions	0.0	0.0	0.0	2.0	0.0000000000	False
sort of hang out	0.0	0.0	0.0	2.0	0.0000000000	False
out for an evening	0.0	0.0	0.0	2.0	0.0000000000	False
grade all the assignments	0.0	0.0	0.0	2.0	0.0000000000	False
grader i ll send	0.0	0.0	0.0	0.0	0.0000000000	False
details and to solicit	0.0	0.0	0.0	2.0	0.0000000000	False
talk about linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
notes have been posted	0.0	0.0	0.0	2.0	0.0000000000	False
written out and work	0.0	0.0	0.0	2.0	0.0000000000	False
work through the details	0.0	0.0	0.0	2.0	0.0000000000	False
download detailed lecture notes	0.0	0.0	0.0	2.0	0.0000000000	False
amount  some amount	0.0	0.0	0.0	2.0	0.0000000000	False
amount of linear algebra	0.0	0.0	0.0	2.0	0.0000000000	False
refresher on linear algebra	0.0	0.0	0.0	4.0	0.0000000000	False
week s discussion section	0.0	0.0	0.0	0.0	0.0000000000	False
section will be taught	0.0	0.0	0.0	2.0	0.0000000000	False
taught by the tas	0.0	0.0	0.0	2.0	0.0000000000	False
linear algebra i talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about today sort	0.0	0.0	0.0	2.0	0.0000000000	False
things i m claiming	0.0	0.0	0.0	0.0	0.0000000000	False
written out in detail	0.0	0.0	0.0	2.0	0.0000000000	False
showing you a fun	0.0	0.0	0.0	2.0	0.0000000000	False
talked about supervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
supervised learning and supervised	0.0	0.0	0.0	2.0	0.0000000000	False
learning and supervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm what the close	0.0	0.0	0.0	2.0	0.0000000000	False
lecture was the problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem of predicting housing	0.0	0.0	0.0	2.0	0.0000000000	False
right " housing price	0.0	0.0	0.0	2.0	0.0000000000	False
house in the training	0.0	0.0	0.0	4.0	0.0000000000	False
houses and the prices	0.0	0.0	0.0	4.0	0.0000000000	False
show you a video	0.0	0.0	0.0	4.0	0.0000000000	False
load the big screen	0.0	0.0	0.0	2.0	0.0000000000	False
pomerleau at some work	0.0	0.0	0.0	2.0	0.0000000000	False
mellon on applied supervised	0.0	0.0	0.0	2.0	0.0000000000	False
work on a vehicle	0.0	0.0	0.0	2.0	0.0000000000	False
vehicle known as alvin	0.0	0.0	0.0	2.0	0.0000000000	False
supervised or any algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
dean pomerleau s voice	0.0	0.0	0.0	0.0	0.0000000000	False
pomerleau s voice mention	0.0	0.0	0.0	0.0	0.0000000000	False
voice mention and algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm called neural network	0.0	0.0	0.0	2.0	0.0000000000	False
lecture let s watch	0.0	0.0	0.0	0.0	0.0000000000	False
shows that we re	0.0	0.0	0.0	0.0	0.0000000000	False
re on this segment	0.0	0.0	0.0	2.0	0.0000000000	False
segment of the road	0.0	0.0	0.0	4.0	0.0000000000	False
steer at this angle	0.0	0.0	0.0	4.0	0.0000000000	False
human provides the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of " correct	0.0	0.0	0.0	2.0	0.0000000000	False
correct " steering directions	0.0	0.0	0.0	4.0	0.0000000000	False
directions to the car	0.0	0.0	0.0	2.0	0.0000000000	False
job of the car	0.0	0.0	0.0	2.0	0.0000000000	False
car on the road	0.0	0.0	0.0	2.0	0.0000000000	False
road on the monitor	0.0	0.0	0.0	2.0	0.0000000000	False
left where the mouse	0.0	0.0	0.0	2.0	0.0000000000	False
mouse pointer is moving	0.0	0.0	0.0	2.0	0.0000000000	False
horizontal line actually shows	0.0	0.0	0.0	2.0	0.0000000000	False
shows the human steering	0.0	0.0	0.0	2.0	0.0000000000	False
area right here shows	0.0	0.0	0.0	2.0	0.0000000000	False
shows the steering direction	0.0	0.0	0.0	2.0	0.0000000000	False
moving the steering wheel	0.0	0.0	0.0	2.0	0.0000000000	False
steering a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit to the left	0.0	0.0	0.0	2.0	0.0000000000	False
region this second line	0.0	0.0	0.0	2.0	0.0000000000	False
line here where mamos	0.0	0.0	0.0	2.0	0.0000000000	False
output of the learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm currently thinks	0.0	0.0	0.0	2.0	0.0000000000	False
thinks is the right	0.0	0.0	0.0	2.0	0.0000000000	False
entire range of steering	0.0	0.0	0.0	2.0	0.0000000000	False
range of steering directions	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm collects more examples	0.0	0.0	0.0	2.0	0.0000000000	False
confidently choose a steering	0.0	0.0	0.0	2.0	0.0000000000	False
choose a steering direction	0.0	0.0	0.0	2.0	0.0000000000	False
back to the chalkboard	0.0	0.0	2.99840764331	6.0	0.0000000000	False
ago and autonomous driving	0.0	0.0	0.0	2.0	0.0000000000	False
heard of the darpa	0.0	0.0	0.0	2.0	0.0000000000	False
car across a desert	0.0	0.0	0.0	2.0	0.0000000000	False
call the regression problem	0.0	0.0	0.0	4.0	0.0000000000	False
predict a continuous value	0.0	0.0	0.0	2.0	0.0000000000	False
continuous value steering directions	0.0	0.0	0.0	2.0	0.0000000000	False
first supervised learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
re going to return	0.0	0.0	0.0	2.0	0.0000000000	False
housing prices in portland	0.0	0.0	0.0	2.0	0.0000000000	False
dataset of a number	0.0	0.0	0.0	2.0	0.0000000000	False
houses of different sizes	0.0	0.0	0.0	2.0	0.0000000000	False
make your other dataset	0.0	0.0	0.0	2.0	0.0000000000	False
call a training set	0.0	0.0	0.0	2.0	0.0000000000	False
relationship between the size	0.0	0.0	0.0	2.0	0.0000000000	False
size of the house	0.0	0.0	3.99787685775	8.0	0.0000000000	False
price of the house	0.0	0.0	0.0	4.0	0.0000000000	False
task a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
first piece of notation	0.0	0.0	0.0	2.0	0.0000000000	False
case alphabet m denote	0.0	0.0	0.0	2.0	0.0000000000	False
number of training examples	0.0	0.0	3.99734607219	10.0	0.0000000000	False
alphabet m to denote	0.0	0.0	0.0	2.0	0.0000000000	False
alphabet x to denote	0.0	0.0	0.0	2.0	0.0000000000	False
denote the input variables	0.0	0.0	0.0	2.0	0.0000000000	False
ll often also call	0.0	0.0	0.0	2.0	0.0000000000	False
denote the " output	0.0	0.0	0.0	2.0	0.0000000000	False
row on the table	0.0	0.0	0.0	2.0	0.0000000000	False
words the ith row	0.0	0.0	0.0	2.0	0.0000000000	False
row in that table	0.0	0.0	0.0	2.0	0.0000000000	False
superscript i in parentheses	0.0	0.0	0.0	2.0	0.0000000000	False
parentheses is just sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of an index	0.0	0.0	0.0	2.0	0.0000000000	False
row of my list	0.0	0.0	0.0	2.0	0.0000000000	False
list of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples so in supervised	0.0	0.0	0.0	2.0	0.0000000000	False
re given a training	0.0	0.0	0.0	2.0	0.0000000000	False
re going to feed	0.0	0.0	0.0	2.0	0.0000000000	False
feed our training set	0.0	0.0	0.0	2.0	0.0000000000	False
comprising our m training	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm then has output	0.0	0.0	0.0	2.0	0.0000000000	False
denoted lower case alphabet	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis do nt worry	0.0	0.0	0.0	0.0	0.0000000000	False
hypothesis has a deep	0.0	0.0	0.0	2.0	0.0000000000	False
reasons and the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
area in square feet	0.0	0.0	0.0	2.0	0.0000000000	False
feet saying and output	0.0	0.0	0.0	2.0	0.0000000000	False
output estimates the price	0.0	0.0	0.0	2.0	0.0000000000	False
price of this house	0.0	0.0	0.0	2.0	0.0000000000	False
house so the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
inputs x to outputs	0.0	0.0	0.0	2.0	0.0000000000	False
design a learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
purposes of this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
representation for the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
generally for many regression	0.0	0.0	0.0	2.0	0.0000000000	False
bedrooms in these houses	0.0	0.0	0.0	2.0	0.0000000000	False
bedrooms in the house	0.0	0.0	0.0	2.0	0.0000000000	False
size and square feet	0.0	0.0	0.0	2.0	0.0000000000	False
two denote the number	0.0	0.0	0.0	2.0	0.0000000000	False
theta rho plus theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta 1x1 plus theta	0.0	0.0	0.0	2.0	0.0000000000	False
dependent on the theta	0.0	0.0	0.0	2.0	0.0000000000	False
price that my hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
hypothesis predicts a house	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis predicts this house	0.0	0.0	0.0	2.0	0.0000000000	False
cost one final piece	0.0	0.0	0.0	2.0	0.0000000000	False
final piece of notation	0.0	0.0	0.0	4.0	0.0000000000	False
write this a bit	0.0	0.0	0.0	2.0	0.0000000000	False
sum from i equals	0.0	0.0	5.99681528662	12.0	0.3973509934	False
features in my learning	0.0	0.0	0.0	2.0	0.0000000000	False
fair amount of notation	0.0	0.0	0.0	4.0	0.0000000000	False
proceed through the rest	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
symbol and you re	0.0	0.0	0.0	0.0	0.0000000000	False
standardize notation and make	0.0	0.0	0.0	2.0	0.0000000000	False
lot of our descriptions	0.0	0.0	0.0	2.0	0.0000000000	False
descriptions of learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms a lot	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms a lot easier	0.0	0.0	0.0	2.0	0.0000000000	False
remember what it means	0.0	0.0	0.0	2.0	0.0000000000	False
class who ve forgotten	0.0	0.0	0.0	0.0	0.0000000000	False
wondering what some symbol	0.0	0.0	0.0	2.0	0.0000000000	False
symbol means any questions	0.0	0.0	0.0	2.0	0.0000000000	False
theta or the theta	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of our learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm and theta	0.0	0.0	0.0	2.0	0.0000000000	False
job of the learning	0.0	0.0	0.0	2.0	0.0000000000	False
training set to choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose or to learn	0.0	0.0	0.0	2.0	0.0000000000	False
learn appropriate parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta and theta transpose	0.0	0.0	0.0	2.0	0.0000000000	False
theta all great questions	0.0	0.0	0.0	2.0	0.0000000000	False
great questions the answer	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis or can theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta be a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of other variables	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm we ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
class a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
talk about higher order	0.0	0.0	0.0	2.0	0.0000000000	False
chose the parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis h will make	0.0	0.0	0.0	2.0	0.0000000000	False
predictions of the housing	0.0	0.0	0.0	2.0	0.0000000000	False
set so one thing	0.0	0.0	0.0	2.0	0.0000000000	False
predictions of a learning	0.0	0.0	0.0	2.0	0.0000000000	False
accurate on a training	0.0	0.0	0.0	2.0	0.0000000000	False
make that theta square	0.0	0.0	0.0	2.0	0.0000000000	False
difference between the prediction	0.0	0.0	0.0	2.0	0.0000000000	False
prediction of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
minimize over the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
area between the predicted	0.0	0.0	0.0	2.0	0.0000000000	False
examples so the sum	0.0	0.0	0.0	2.0	0.0000000000	False
house in my training	0.0	0.0	0.0	2.0	0.0000000000	False
actual target variable mine	0.0	0.0	0.0	2.0	0.0000000000	False
mine is actual price	0.0	0.0	0.0	2.0	0.0000000000	False
sum of the squared	0.0	0.0	0.0	4.0	0.0000000000	False
define j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
predicted by my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
minus the actual value	0.0	0.0	0.0	2.0	0.0000000000	False
minimize as a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
quantity j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
sort of linear algebra	0.0	0.0	0.0	2.0	0.0000000000	False
show that this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
broader class of algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
ll get there eventually	0.0	0.0	0.0	2.0	0.0000000000	False
talk about a couple	0.0	0.0	0.0	2.0	0.0000000000	False
couple of different algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
theta the first algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
start with some value	0.0	0.0	0.0	2.0	0.0000000000	False
value of my parameter	0.0	0.0	0.0	2.0	0.0000000000	False
vector theta maybe initialize	0.0	0.0	0.0	2.0	0.0000000000	False
initialize my parameter vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector of all zeros	0.0	0.0	5.99787685775	8.0	0.0000000000	False
correct that i sort	0.0	0.0	0.0	2.0	0.0000000000	False
changing my parameter vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector theta to reduce	0.0	0.0	0.0	2.0	0.0000000000	False
reduce j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
lower the big screen	0.0	0.0	0.0	2.0	0.0000000000	False
show you an animation	0.0	0.0	0.0	2.0	0.0000000000	False
first algorithm for minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
minimizing j of theta	0.0	0.0	3.99734607219	10.0	0.4395604396	False
plot and the axes	0.0	0.0	0.0	2.0	0.0000000000	False
horizontal axes are theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta zero and theta	0.0	0.0	3.99787685775	8.0	0.5263157895	False
represented by the height	0.0	0.0	0.0	2.0	0.0000000000	False
height of this plot	0.0	0.0	0.0	2.0	0.0000000000	False
plot so the surface	0.0	0.0	0.0	2.0	0.0000000000	False
surface represents the function	0.0	0.0	0.0	2.0	0.0000000000	False
function j of theta	0.0	0.0	7.99734607219	10.0	0.0000000000	False
theta and the axes	0.0	0.0	0.0	2.0	0.0000000000	False
axes of this function	0.0	0.0	0.0	2.0	0.0000000000	False
inputs of this function	0.0	0.0	0.0	2.0	0.0000000000	False
function are the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
choose some initial point	0.0	0.0	0.0	2.0	0.0000000000	False
start from that point	0.0	0.0	0.0	2.0	0.0000000000	False
denoted by the star	0.0	0.0	0.0	2.0	0.0000000000	False
imagine that this display	0.0	0.0	0.0	2.0	0.0000000000	False
shows a 3d landscape	0.0	0.0	0.0	2.0	0.0000000000	False
landscape imagine you re	0.0	0.0	0.0	0.0	0.0000000000	False
hill in some park	0.0	0.0	0.0	2.0	0.0000000000	False
physically at the position	0.0	0.0	0.0	2.0	0.0000000000	False
position of that star	0.0	0.0	0.0	2.0	0.0000000000	False
imagine you can stand	0.0	0.0	0.0	2.0	0.0000000000	False
stand on that hill	0.0	0.0	0.0	2.0	0.0000000000	False
hill and you re	0.0	0.0	0.0	0.0	0.0000000000	False
direction of steepest descent	0.0	0.0	5.9957537155	16.0	0.0000000000	False
step in this direction	0.0	0.0	0.0	2.0	0.0000000000	False
direction that the gradient	0.0	0.0	0.0	2.0	0.0000000000	False
step and you end	0.0	0.0	0.0	2.0	0.0000000000	False
point on this hill	0.0	0.0	0.0	2.0	0.0000000000	False
minimum of this function	0.0	0.0	0.0	4.0	0.0000000000	False
property of gradient descent	0.0	0.0	2.99840764331	6.0	0.0000000000	False
lower left hand corner	0.0	0.0	0.0	2.0	0.0000000000	False
corner of this plot	0.0	0.0	0.0	2.0	0.0000000000	False
slightly different initial starting	0.0	0.0	0.0	4.0	0.0000000000	False
right so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out if you run	0.0	0.0	0.0	2.0	0.0000000000	False
descent from that point	0.0	0.0	0.0	2.0	0.0000000000	False
completely different local optimum	0.0	0.0	0.0	2.0	0.0000000000	False
aware that gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
descent can sometimes depend	0.0	0.0	0.0	2.0	0.0000000000	False
work out the math	0.0	0.0	0.0	2.0	0.0000000000	False
math of the gradient	0.0	0.0	0.0	2.0	0.0000000000	False
issue of local optimum	0.0	0.0	0.0	2.0	0.0000000000	False
descent algorithm we re	0.0	0.0	0.0	0.0	0.0000000000	False
repeatedly take a step	0.0	0.0	0.0	2.0	0.0000000000	False
step in the direction	0.0	0.0	0.0	2.0	0.0000000000	False
re going to update	0.0	0.0	2.99840764331	6.0	0.0000000000	False
update the parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
parameters theta as theta	0.0	0.0	0.0	2.0	0.0000000000	False
partial derivative with respect	0.0	0.0	5.99734607219	10.0	0.3252032520	False
update the i parameter	0.0	0.0	0.0	2.0	0.0000000000	False
iteration of gradient descent	0.0	0.0	4.99840764331	6.0	0.0000000000	False
descent just a point	0.0	0.0	0.0	2.0	0.0000000000	False
equals notation to denote	0.0	0.0	0.0	2.0	0.0000000000	False
notation to denote setting	0.0	0.0	0.0	2.0	0.0000000000	False
variable on the left	0.0	0.0	0.0	2.0	0.0000000000	False
variable on the right	0.0	0.0	0.0	2.0	0.0000000000	False
hand side all right	0.0	0.0	0.0	2.0	0.0000000000	False
write a colon equals	0.0	0.0	0.0	2.0	0.0000000000	False
part of a computer	0.0	0.0	0.0	2.0	0.0000000000	False
part of an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
value on the right	0.0	0.0	0.0	2.0	0.0000000000	False
value on the left	0.0	0.0	0.0	2.0	0.0000000000	False
hand side in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
truth i m claiming	0.0	0.0	0.0	0.0	0.0000000000	False
claiming that the value	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the value	0.0	0.0	0.0	2.0	0.0000000000	False
operation where we overwrite	0.0	0.0	0.0	2.0	0.0000000000	False
asserting that the values	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm sort of makes	0.0	0.0	0.0	2.0	0.0000000000	False
sort of makes sense	0.0	0.0	0.0	2.0	0.0000000000	False
problem and to work	0.0	0.0	0.0	2.0	0.0000000000	False
work out gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
first somewhat mathematical lecture	0.0	0.0	0.0	2.0	0.0000000000	False
quarter we ll work	0.0	0.0	0.0	0.0	0.0000000000	False
work through the steps	0.0	0.0	0.0	4.0	0.0000000000	False
out what this gradient	0.0	0.0	0.0	2.0	0.0000000000	False
respect to the parameter	0.0	0.0	0.0	4.0	0.0000000000	False
one-half of script theta	0.0	0.0	0.0	2.0	0.0000000000	False
two times one-half times	0.0	0.0	0.0	2.0	0.0000000000	False
times one-half times theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta of x minus	0.0	0.0	0.0	2.0	0.0000000000	False
inside the square right	0.0	0.0	0.0	2.0	0.0000000000	False
cancel so this leaves	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of this sum	0.0	0.0	0.0	2.0	0.0000000000	False
terms in the sum	0.0	0.0	0.0	2.0	0.0000000000	False
term here of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta i gets updated	0.0	0.0	3.99840764331	6.0	0.0000000000	False
theta i minus alpha	0.0	0.0	0.0	4.0	0.0000000000	False
parameter of the algorithm	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm called the learning	0.0	0.0	0.0	2.0	0.0000000000	False
standing on the hill	0.0	0.0	0.0	4.0	0.0000000000	False
hand if you choose	0.0	0.0	0.0	2.0	0.0000000000	False
small than your steepest	0.0	0.0	0.0	2.0	0.0000000000	False
long time to converge	0.0	0.0	0.0	2.0	0.0000000000	False
large then the steepest	0.0	0.0	0.0	2.0	0.0000000000	False
descent may actually end	0.0	0.0	0.0	2.0	0.0000000000	False
make lots of errors	0.0	0.0	0.0	2.0	0.0000000000	False
property into an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
generally for m training	0.0	0.0	0.0	2.0	0.0000000000	False
re going to repeat	0.0	0.0	0.0	2.0	0.0000000000	False
convergence the following step	0.0	0.0	0.0	2.0	0.0000000000	False
out the appropriate equation	0.0	0.0	0.0	2.0	0.0000000000	False
equation for m examples	0.0	0.0	0.0	2.0	0.0000000000	False
updated theta i minus	0.0	0.0	0.0	2.0	0.0000000000	False
alpha times the sum	0.0	0.0	0.0	2.0	0.0000000000	False
nt bother to show	0.0	0.0	0.0	0.0	0.0000000000	False
back to the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
specific problem of linear	0.0	0.0	0.0	2.0	0.0000000000	False
problem of linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
out for ordinary release	0.0	0.0	0.0	2.0	0.0000000000	False
contours of the function	0.0	0.0	0.0	2.0	0.0000000000	False
contours of a bow	0.0	0.0	0.0	2.0	0.0000000000	False
descent on this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
randomly at the position	0.0	0.0	0.0	2.0	0.0000000000	False
position of that cross	0.0	0.0	0.0	2.0	0.0000000000	False
result of one step	0.0	0.0	0.0	2.0	0.0000000000	False
step of gradient descent	0.0	0.0	1.99840764331	6.0	0.0000000000	False
hypothesis cost the function	0.0	0.0	0.0	2.0	0.0000000000	False
fake value of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
smaller and smaller steps	0.0	0.0	0.0	4.0	0.0000000000	False
converge and the reason	0.0	0.0	0.0	2.0	0.0000000000	False
update theta by subtracting	0.0	0.0	0.0	2.0	0.0000000000	False
subtracting from alpha times	0.0	0.0	0.0	2.0	0.0000000000	False
alpha times the gradient	0.0	0.0	0.0	4.0	0.0000000000	False
local minimum the gradient	0.0	0.0	0.0	2.0	0.0000000000	False
steps as you approach	0.0	0.0	0.0	2.0	0.0000000000	False
approach the local minimum	0.0	0.0	9.99734607219	10.0	0.3448275862	False
local minimum make sense	0.0	0.0	0.0	2.0	0.0000000000	False
plot of the housing	0.0	0.0	0.0	2.0	0.0000000000	False
parameters to the vector	0.0	0.0	0.0	2.0	0.0000000000	False
line at the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
bottom shows the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis with the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
initialization so initially theta	0.0	0.0	0.0	2.0	0.0000000000	False
predicts that all prices	0.0	0.0	0.0	2.0	0.0000000000	False
found the least square	0.0	0.0	0.0	2.0	0.0000000000	False
fit for the data	0.0	0.0	0.0	2.0	0.0000000000	False
chalkboard are there questions	0.0	0.0	0.0	2.0	0.0000000000	False
cases the new values	0.0	0.0	0.0	2.0	0.0000000000	False
right and converged means	0.0	0.0	0.0	2.0	0.0000000000	False
means that the value	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a question	0.0	0.0	0.0	2.0	0.0000000000	False
theta has nt changed	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to minimize	0.0	0.0	0.0	2.0	0.0000000000	False
minimize is not changing	0.0	0.0	0.0	2.0	0.0000000000	False
sort of standard heuristics	0.0	0.0	0.0	2.0	0.0000000000	False
standard rules of thumb	0.0	0.0	0.0	2.0	0.0000000000	False
decide if gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent has converged	0.0	0.0	0.0	2.0	0.0000000000	False
descent so one feature	0.0	0.0	0.0	2.0	0.0000000000	False
answer the second part	0.0	0.0	0.0	2.0	0.0000000000	False
gradient of the function	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of the function	0.0	0.0	0.0	2.0	0.0000000000	False
direction of steepest ascent	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of a function	0.0	0.0	0.0	4.0	0.0000000000	False
function sort of turns	0.0	0.0	0.0	2.0	0.0000000000	False
sort of turns out	0.0	0.0	0.0	2.0	0.0000000000	False
out to just give	0.0	0.0	0.0	2.0	0.0000000000	False
give you the direction	0.0	0.0	0.0	2.0	0.0000000000	False
derivative and that turns	0.0	0.0	0.0	2.0	0.0000000000	False
nt a great term	0.0	0.0	0.0	0.0	0.0000000000	False
refers to the fact	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent you re	0.0	0.0	0.0	0.0	0.0000000000	False
training set you re	0.0	0.0	0.0	0.0	0.0000000000	False
re going to perform	0.0	0.0	0.0	2.0	0.0000000000	False
oregon in our training	0.0	0.0	0.0	2.0	0.0000000000	False
u.s census size databases	0.0	0.0	0.0	2.0	0.0000000000	False
millions of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
re running batch rate	0.0	0.0	0.0	2.0	0.0000000000	False
batch rate and descent	0.0	0.0	0.0	2.0	0.0000000000	False
means that to perform	0.0	0.0	0.0	2.0	0.0000000000	False
sum from j equals	0.0	0.0	0.0	4.0	0.0000000000	False
million that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
sort of a lot	0.0	0.0	0.0	2.0	0.0000000000	False
lot of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples where your computer	0.0	0.0	0.0	2.0	0.0000000000	False
downhill on the function	0.0	0.0	0.0	2.0	0.0000000000	False
theta so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
write down an alternative	0.0	0.0	0.0	2.0	0.0000000000	False
call it incremental gradient	0.0	0.0	0.0	2.0	0.0000000000	False
convergence and will iterate	0.0	0.0	0.0	2.0	0.0000000000	False
iterate for j equals	0.0	0.0	0.0	2.0	0.0000000000	False
sort of gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
update all the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
data runs you perform	0.0	0.0	0.0	2.0	0.0000000000	False
update for all values	0.0	0.0	0.0	2.0	0.0000000000	False
indexes and the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
simultaneously and the advantage	0.0	0.0	0.0	2.0	0.0000000000	False
advantage of this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
order to start learning	0.0	0.0	0.0	2.0	0.0000000000	False
order to start modifying	0.0	0.0	0.0	2.0	0.0000000000	False
start modifying the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
training example and perform	0.0	0.0	0.0	4.0	0.0000000000	False
derivative of the error	0.0	0.0	0.0	2.0	0.0000000000	False
update and you sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of keep adapting	0.0	0.0	0.0	2.0	0.0000000000	False
scan over your entire	0.0	0.0	0.0	2.0	0.0000000000	False
entire u.s census database	0.0	0.0	0.0	2.0	0.0000000000	False
contours of your function	0.0	0.0	0.0	2.0	0.0000000000	False
run the constant gradient	0.0	0.0	0.0	2.0	0.0000000000	False
end up going uphill	0.0	0.0	0.0	2.0	0.0000000000	False
wander to the region	0.0	0.0	0.0	2.0	0.0000000000	False
sort of keep wandering	0.0	0.0	0.0	2.0	0.0000000000	False
bit near the region	0.0	0.0	0.0	2.0	0.0000000000	False
bit the global minimum	0.0	0.0	0.0	2.0	0.0000000000	False
minimum and in practice	0.0	0.0	0.0	2.0	0.0000000000	False
ll ask what questions	0.0	0.0	0.0	2.0	0.0000000000	False
training example and update	0.0	0.0	0.0	2.0	0.0000000000	False
update all the theta	0.0	0.0	0.0	2.0	0.0000000000	False
perform the second gradient	0.0	0.0	0.0	2.0	0.0000000000	False
training example one training	0.0	0.0	0.0	2.0	0.0000000000	False
iterative algorithm for performing	0.0	0.0	0.0	2.0	0.0000000000	False
theta and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
problem of least squares	0.0	0.0	0.0	2.0	0.0000000000	False
solve for the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
theta in close form	0.0	0.0	2.99840764331	6.0	0.0000000000	False
run an iterative algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
typically done requires projections	0.0	0.0	0.0	2.0	0.0000000000	False
taking lots of derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
derivatives and writing lots	0.0	0.0	0.0	2.0	0.0000000000	False
writing lots of algebra	0.0	0.0	0.0	2.0	0.0000000000	False
derive the closed form	0.0	0.0	0.0	2.0	0.0000000000	False
form solution of theta	0.0	0.0	0.0	2.0	0.0000000000	False
ll need to introduce	0.0	0.0	0.0	2.0	0.0000000000	False
introduce a new notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation for matrix derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
personal work has turned	0.0	0.0	0.0	2.0	0.0000000000	False
algebra rather than writing	0.0	0.0	0.0	2.0	0.0000000000	False
out pages and pages	0.0	0.0	0.0	2.0	0.0000000000	False
re going to define	0.0	0.0	0.0	2.0	0.0000000000	False
define this new notation	0.0	0.0	0.0	2.0	0.0000000000	False
work out the minimization	0.0	0.0	0.0	2.0	0.0000000000	False
minimization given a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of a vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector of parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of the gradient	0.0	0.0	0.0	2.0	0.0000000000	False
dimensional vector with indices	0.0	0.0	0.0	2.0	0.0000000000	False
vector with indices ranging	0.0	0.0	0.0	2.0	0.0000000000	False
rewrite the gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent as updating	0.0	0.0	0.0	2.0	0.0000000000	False
updating the parameter vector	0.0	0.0	0.0	4.0	0.0000000000	False
vector theta  notice	0.0	0.0	0.0	2.0	0.0000000000	False
previous parameter minus alpha	0.0	0.0	0.0	2.0	0.0000000000	False
parameter minus alpha times	0.0	0.0	0.0	2.0	0.0000000000	False
boards out of order	0.0	0.0	0.0	2.0	0.0000000000	False
function f that maps	0.0	0.0	0.0	2.0	0.0000000000	False
maps from the space	0.0	0.0	0.0	2.0	0.0000000000	False
matrices to the space	0.0	0.0	0.0	2.0	0.0000000000	False
space of real numbers	0.0	0.0	0.0	2.0	0.0000000000	False
matrix so this function	0.0	0.0	0.0	2.0	0.0000000000	False
matrices to real numbers	0.0	0.0	0.0	2.0	0.0000000000	False
matrix let me define	0.0	0.0	0.0	2.0	0.0000000000	False
respect to its input	0.0	0.0	0.0	2.0	0.0000000000	False
respect to the elements	0.0	0.0	0.0	2.0	0.0000000000	False
number of rows equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals number of columns	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the sum	0.0	0.0	0.0	2.0	0.0000000000	False
nt seen this sort	0.0	0.0	0.0	0.0	0.0000000000	False
sort of operator notation	0.0	0.0	0.0	2.0	0.0000000000	False
applied to the square	0.0	0.0	0.0	2.0	0.0000000000	False
written without the parentheses	0.0	0.0	0.0	2.0	0.0000000000	False
sum of diagonal elements	0.0	0.0	0.0	4.0	0.0000000000	False
facts about the trace	0.0	0.0	0.0	2.0	0.0000000000	False
operator and about derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
write these without proof	0.0	0.0	0.0	2.0	0.0000000000	False
trace of the matrix	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the trace	0.0	0.0	5.99787685775	8.0	0.2941176471	False
trace of a product	0.0	0.0	0.0	2.0	0.0000000000	False
product of three matrices	0.0	0.0	0.0	2.0	0.0000000000	False
matrix at the end	0.0	0.0	0.0	2.0	0.0000000000	False
trace of a times	0.0	0.0	0.0	2.0	0.0000000000	False
matrix b and move	0.0	0.0	0.0	2.0	0.0000000000	False
defined as a trace	0.0	0.0	0.0	2.0	0.0000000000	False
number so the trace	0.0	0.0	0.0	2.0	0.0000000000	False
matrix a and output	0.0	0.0	0.0	2.0	0.0000000000	False
output a real number	0.0	0.0	0.0	2.0	0.0000000000	False
respect to the matrix	0.0	0.0	0.0	4.0	0.0000000000	False
referring to the definitions	0.0	0.0	0.0	2.0	0.0000000000	False
traces and matrix derivatives	0.0	0.0	0.0	4.0	0.0000000000	False
easy ones the trace	0.0	0.0	0.0	2.0	0.0000000000	False
trace of a transposed	0.0	0.0	0.0	2.0	0.0000000000	False
transposed because the trace	0.0	0.0	0.0	2.0	0.0000000000	False
matrix so the trace	0.0	0.0	0.0	2.0	0.0000000000	False
sort of just algebra	0.0	0.0	0.0	2.0	0.0000000000	False
armed with these things	0.0	0.0	0.0	2.0	0.0000000000	False
theta as a function	0.0	0.0	0.0	2.0	0.0000000000	False
iterative algorithm so work	0.0	0.0	0.0	2.0	0.0000000000	False
out let me define	0.0	0.0	0.0	2.0	0.0000000000	False
inputs from my training	0.0	0.0	0.0	2.0	0.0000000000	False
inputs to the vector	0.0	0.0	0.0	2.0	0.0000000000	False
row of this matrix	0.0	0.0	0.0	2.0	0.0000000000	False
set my second training	0.0	0.0	0.0	2.0	0.0000000000	False
defined as matrix capital	0.0	0.0	0.0	2.0	0.0000000000	False
vector theta this derivation	0.0	0.0	0.0	2.0	0.0000000000	False
two or three sets	0.0	0.0	0.0	2.0	0.0000000000	False
sets so x times	0.0	0.0	0.0	2.0	0.0000000000	False
times theta  remember	0.0	0.0	0.0	2.0	0.0000000000	False
remember how matrix vector	0.0	0.0	0.0	2.0	0.0000000000	False
rows of the matrix	0.0	0.0	0.0	2.0	0.0000000000	False
matrix so x times	0.0	0.0	0.0	2.0	0.0000000000	False
predictions of your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
defined the y vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector so x theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta minus y contained	0.0	0.0	0.0	2.0	0.0000000000	False
vector in m training	0.0	0.0	0.0	2.0	0.0000000000	False
vector than z transpose	0.0	0.0	0.0	2.0	0.0000000000	False
product of a vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector with a sum	0.0	0.0	0.0	2.0	0.0000000000	False
product of this vector	0.0	0.0	0.0	2.0	0.0000000000	False
squares of the elements	0.0	0.0	0.0	2.0	0.0000000000	False
elements of this vector	0.0	0.0	0.0	2.0	0.0000000000	False
notations at you today	0.0	0.0	0.0	2.0	0.0000000000	False
examples and the number	0.0	0.0	0.0	2.0	0.0000000000	False
feature vector that runs	0.0	0.0	0.0	2.0	0.0000000000	False
sort of theta transpose	0.0	0.0	0.0	2.0	0.0000000000	False
feature vectors that index	0.0	0.0	0.0	2.0	0.0000000000	False
cool so we re	0.0	0.0	0.0	0.0	0.0000000000	False
theta and we ve	0.0	0.0	0.0	0.0	0.0000000000	False
written j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
compactly using this matrix	0.0	0.0	0.0	2.0	0.0000000000	False
notation so in order	0.0	0.0	0.0	2.0	0.0000000000	False
fairly quickly without proof	0.0	0.0	0.0	2.0	0.0000000000	False
derivative and the one-half	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the answers	0.0	0.0	0.0	2.0	0.0000000000	False
lecture notes and make	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you understand	0.0	0.0	0.0	2.0	0.0000000000	False
work through every step	0.0	0.0	0.0	2.0	0.0000000000	False
referring to the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
taking a quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic function and expanding	0.0	0.0	0.0	2.0	0.0000000000	False
great so this quantity	0.0	0.0	0.0	2.0	0.0000000000	False
trace operator without changing	0.0	0.0	0.0	2.0	0.0000000000	False
one-half derivative with respect	0.0	0.0	0.0	2.0	0.0000000000	False
theta of the trace	0.0	0.0	7.99787685775	8.0	0.4166666667	False
permutation property of trace	0.0	0.0	0.0	2.0	0.0000000000	False
theta at the end	0.0	0.0	0.0	2.0	0.0000000000	False
trace of theta times	0.0	0.0	0.0	2.0	0.0000000000	False
theta times theta transposed	0.0	0.0	0.0	2.0	0.0000000000	False
minus derivative with respect	0.0	0.0	0.0	2.0	0.0000000000	False
number and the transpose	0.0	0.0	0.0	2.0	0.0000000000	False
real number without changing	0.0	0.0	0.0	2.0	0.0000000000	False
last term with respect	0.0	0.0	0.0	2.0	0.0000000000	False
rule that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
ll find in lecture	0.0	0.0	0.0	2.0	0.0000000000	False
find in lecture notes	0.0	0.0	0.0	2.0	0.0000000000	False
equal to x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
identity which we re	0.0	0.0	0.0	0.0	0.0000000000	False
re going to ignore	0.0	0.0	0.0	2.0	0.0000000000	False
transposed and the matrix	0.0	0.0	0.0	2.0	0.0000000000	False
trace of y transpose	0.0	0.0	0.0	2.0	0.0000000000	False
rules that i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
board s really bad	0.0	0.0	0.0	0.0	0.0000000000	False
back into our formula	0.0	0.0	0.0	2.0	0.0000000000	False
one-half x transpose theta	0.0	0.0	0.0	2.0	0.0000000000	False
transpose x theta minus	0.0	0.0	0.0	2.0	0.0000000000	False
inverse times x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
fit to the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters in closed form	0.0	0.0	0.0	2.0	0.0000000000	False
out reams of algebra	0.0	0.0	0.0	2.0	0.0000000000	False
excited ? any quick	0.0	0.0	0.0	2.0	0.0000000000	False
inverse ? pseudo inverse	0.0	0.0	0.0	4.0	0.0000000000	False
pseudo inverse pseudo inverse	0.0	0.0	0.0	2.0	0.0000000000	False
pseudo inverse ? pseudo	0.0	0.0	0.0	2.0	0.0000000000	False
out that in cases	0.0	0.0	0.0	2.0	0.0000000000	False
inverse minimized to solve	0.0	0.0	0.0	2.0	0.0000000000	False
turns out x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
invertible that usually means	0.0	0.0	0.0	2.0	0.0000000000	False
dependent it usually means	0.0	0.0	0.0	2.0	0.0000000000	False
repeat the same feature	0.0	0.0	0.0	2.0	0.0000000000	False
obtained by the pseudo	0.0	0.0	0.0	2.0	0.0000000000	False
inverses of the inverse	0.0	0.0	0.0	2.0	0.0000000000	False
nt be a problem	0.0	0.0	0.0	0.0	0.0000000000	False
lecture of this class	0.0	0.0	0.0	2.0	0.0000000000	False
topics i do today	0.0	0.0	0.0	2.0	0.0000000000	False
illogical flow of ideas	0.0	0.0	0.0	2.0	0.0000000000	False
talked about linear regression	0.0	0.0	2.99848561333	6.0	0.0000000000	False
linear regression and today	0.0	0.0	0.0	2.0	0.0000000000	False
sort of an adaptation	0.0	0.0	0.0	2.0	0.0000000000	False
mentors probably favorite machine	0.0	0.0	0.0	2.0	0.0000000000	False
favorite machine learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
interpretation of linear regression	0.0	0.0	0.0	4.0	0.0000000000	False
fitting logistic regression models	0.0	0.0	0.0	2.0	0.0000000000	False
recap where we re	0.0	0.0	0.0	0.0	0.0000000000	False
superscript i to denote	0.0	0.0	0.0	2.0	0.0000000000	False
denote the i training	0.0	0.0	0.0	2.0	0.0000000000	False
denote the predicted value	0.0	0.0	0.0	2.0	0.0000000000	False
franchised by the vector	0.0	0.0	0.0	2.0	0.0000000000	False
convention that x subscript	0.0	0.0	0.0	2.0	0.0000000000	False
accounts for the intercept	0.0	0.0	0.0	2.0	0.0000000000	False
regression model and lowercase	0.0	0.0	0.0	2.0	0.0000000000	False
features in my training	0.0	0.0	0.0	2.0	0.0000000000	False
size of the house	0.0	0.0	7.9964664311	14.0	0.3734061931	False
house and the number	0.0	0.0	0.0	2.0	0.0000000000	False
recapping the previous lecture	0.0	0.0	0.0	2.0	0.0000000000	False
defined this quadratic cos	0.0	0.0	0.0	2.0	0.0000000000	False
function j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta of xi minus	0.0	0.0	0.0	2.0	0.0000000000	False
examples and my training	0.0	0.0	0.0	2.0	0.0000000000	False
training set so lowercase	0.0	0.0	0.0	2.0	0.0000000000	False
number of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
size of my training	0.0	0.0	0.0	2.0	0.0000000000	False
minimizes this enclosed form	0.0	0.0	0.0	2.0	0.0000000000	False
move on in today	0.0	0.0	0.0	2.0	0.0000000000	False
fair amount of notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation to all remember	0.0	0.0	0.0	2.0	0.0000000000	False
partway through this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
re having trouble remembering	0.0	0.0	0.0	2.0	0.0000000000	False
trouble remembering what lowercase	0.0	0.0	0.0	2.0	0.0000000000	False
features was the size	0.0	0.0	0.0	2.0	0.0000000000	False
houses in square feet	0.0	0.0	0.0	2.0	0.0000000000	False
area of the house	0.0	0.0	0.0	2.0	0.0000000000	False
feature was the number	0.0	0.0	0.0	2.0	0.0000000000	False
bedrooms in the house	0.0	0.0	0.0	2.0	0.0000000000	False
apply a machine-learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to some problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem that you care	0.0	0.0	0.0	2.0	0.0000000000	False
care about the choice	0.0	0.0	0.0	2.0	0.0000000000	False
choice of the features	0.0	0.0	0.0	2.0	0.0000000000	False
give the learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
idea of the feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature of the number	0.0	0.0	0.0	2.0	0.0000000000	False
price of the house	0.0	0.0	0.0	4.0	0.0000000000	False
theta zero plus theta	0.0	0.0	0.0	4.0	0.0000000000	False
model if you choose	0.0	0.0	0.0	2.0	0.0000000000	False
copy the same data	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the size	0.0	0.0	0.0	2.0	0.0000000000	False
square of the size	0.0	0.0	0.0	2.0	0.0000000000	False
house in say square	0.0	0.0	0.0	2.0	0.0000000000	False
footage of the house	0.0	0.0	0.0	2.0	0.0000000000	False
fitting a quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
function for you theta	0.0	0.0	0.0	2.0	0.0000000000	False
fit to the data	0.0	0.0	4.99848561333	6.0	0.0000000000	False
fill a model theta	0.0	0.0	0.0	2.0	0.0000000000	False
fit a six model	0.0	0.0	0.0	2.0	0.0000000000	False
find that the curve	0.0	0.0	0.0	2.0	0.0000000000	False
model in a sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that it fits	0.0	0.0	0.0	2.0	0.0000000000	False
fits your training data	0.0	0.0	0.0	2.0	0.0000000000	False
model in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
good predictor of housing	0.0	0.0	0.0	2.0	0.0000000000	False
predictor of housing prices	0.0	0.0	0.0	2.0	0.0000000000	False
prices as a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of the size	0.0	0.0	0.0	4.0	0.0000000000	False
out of the models	0.0	0.0	0.0	2.0	0.0000000000	False
model fits the data	0.0	0.0	0.0	2.0	0.0000000000	False
component in this data	0.0	0.0	0.0	2.0	0.0000000000	False
data that the linear	0.0	0.0	0.0	2.0	0.0000000000	False
function is not capturing	0.0	0.0	0.0	2.0	0.0000000000	False
bit later and talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the problems	0.0	0.0	0.0	2.0	0.0000000000	False
problems associated with fitting	0.0	0.0	0.0	2.0	0.0000000000	False
two small a set	0.0	0.0	0.0	2.0	0.0000000000	False
features just to give	0.0	0.0	0.0	2.0	0.0000000000	False
call this the problem	0.0	0.0	0.0	2.0	0.0000000000	False
refers to a setting	0.0	0.0	0.0	2.0	0.0000000000	False
patterns in the data	0.0	0.0	0.0	2.0	0.0000000000	False
data that the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm is just failing	0.0	0.0	0.0	2.0	0.0000000000	False
fit and this problem	0.0	0.0	0.0	2.0	0.0000000000	False
fitting the idiosyncratic properties	0.0	0.0	0.0	2.0	0.0000000000	False
properties of this data	0.0	0.0	0.0	2.0	0.0000000000	False
trends of how housing	0.0	0.0	0.0	2.0	0.0000000000	False
vary as the function	0.0	0.0	0.0	2.0	0.0000000000	False
two very different problems	0.0	0.0	0.0	2.0	0.0000000000	False
problems we ll define	0.0	0.0	0.0	0.0	0.0000000000	False
issue of selecting features	0.0	0.0	0.0	2.0	0.0000000000	False
teach us the learning	0.0	0.0	0.0	2.0	0.0000000000	False
ll talk about feature	0.0	0.0	0.0	2.0	0.0000000000	False
talk about feature selection	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms later this quarter	0.0	0.0	0.0	2.0	0.0000000000	False
automatic algorithms for choosing	0.0	0.0	0.0	2.0	0.0000000000	False
talk about a class	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms called non-parametric learning	0.0	0.0	0.0	2.0	0.0000000000	False
choose features very carefully	0.0	0.0	0.0	4.0	0.0000000000	False
discussion of locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm parametric learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm parametric learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
defined as an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
fixed number of parameters	0.0	0.0	0.0	2.0	0.0000000000	False
fix set of parameters	0.0	0.0	0.0	2.0	0.0000000000	False
set of parameters theta	0.0	0.0	0.0	4.0	0.0000000000	False
first non-parametric learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm the formal definition	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm where the number	0.0	0.0	0.0	2.0	0.0000000000	False
size of the training	0.0	0.0	0.0	4.0	0.0000000000	False
defined as a number	0.0	0.0	0.0	2.0	0.0000000000	False
number of parameters grows	0.0	0.0	0.0	2.0	0.0000000000	False
linearly with the size	0.0	0.0	0.0	2.0	0.0000000000	False
slightly less formal definition	0.0	0.0	0.0	2.0	0.0000000000	False
stuff that your learning	0.0	0.0	0.0	2.0	0.0000000000	False
linearly with the training	0.0	0.0	0.0	2.0	0.0000000000	False
specific non-parametric learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm called locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
couple of other names	0.0	0.0	0.0	2.0	0.0000000000	False
loess for self-hysterical reasons	0.0	0.0	0.0	2.0	0.0000000000	False
loess is usually spelled	0.0	0.0	0.0	2.0	0.0000000000	False
call it locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
worry a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
fit to this data	0.0	0.0	0.0	2.0	0.0000000000	False
data you can sit	0.0	0.0	0.0	2.0	0.0000000000	False
sit around and stare	0.0	0.0	0.0	2.0	0.0000000000	False
decide whether the features	0.0	0.0	0.0	2.0	0.0000000000	False
features are used right	0.0	0.0	0.0	2.0	0.0000000000	False
sit around and fiddle	0.0	0.0	0.0	2.0	0.0000000000	False
features that the model	0.0	0.0	0.0	2.0	0.0000000000	False
talk about an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
fit theta to minimize	0.0	0.0	0.0	4.0	0.0000000000	False
theta to minimize sum	0.0	0.0	0.0	4.0	0.0000000000	False
linear regression in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
locally weighted linear regression	0.0	0.0	5.99798081777	8.0	0.5256410256	True
linear regression you re	0.0	0.0	0.0	0.0	0.0000000000	False
slightly different you re	0.0	0.0	0.0	0.0	0.0000000000	False
account only the data	0.0	0.0	0.0	2.0	0.0000000000	False
vicinity of this point	0.0	0.0	0.0	2.0	0.0000000000	False
linear regression to fit	0.0	0.0	0.0	2.0	0.0000000000	False
fit a straight line	0.0	0.0	3.99798081777	8.0	0.4162436548	False
sub-set of the data	0.0	0.0	0.0	2.0	0.0000000000	False
set and i fit	0.0	0.0	0.0	2.0	0.0000000000	False
evaluate this particular value	0.0	0.0	0.0	2.0	0.0000000000	False
value of straight line	0.0	0.0	0.0	2.0	0.0000000000	False
return for my algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
outputs in locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na fall	0.0	0.0	0.0	2.0	0.0000000000	False
re going to fit	0.0	0.0	0.0	2.0	0.0000000000	False
right ? so notice	0.0	0.0	0.0	2.0	0.0000000000	False
notice that  suppose	0.0	0.0	0.0	2.0	0.0000000000	False
conversely if xi minus	0.0	0.0	0.0	2.0	0.0000000000	False
close to zero right	0.0	0.0	0.0	2.0	0.0000000000	False
minus some large number	0.0	0.0	0.0	2.0	0.0000000000	False
give the points close	0.0	0.0	0.0	2.0	0.0000000000	False
large weight and give	0.0	0.0	0.0	2.0	0.0000000000	False
times this quadratic term	0.0	0.0	0.0	4.0	0.0000000000	False
quadratic term for points	0.0	0.0	0.0	2.0	0.0000000000	False
points plus zero times	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic term for faraway	0.0	0.0	0.0	2.0	0.0000000000	False
term for faraway points	0.0	0.0	0.0	2.0	0.0000000000	False
weighted linear regression fits	0.0	0.0	0.0	2.0	0.0000000000	False
regression fits a set	0.0	0.0	0.0	2.0	0.0000000000	False
paying much more attention	0.0	0.0	0.0	2.0	0.0000000000	False
fitting the points close	0.0	0.0	0.0	2.0	0.0000000000	False
points close by accurately	0.0	0.0	0.0	2.0	0.0000000000	False
contribution from faraway points	0.0	0.0	0.0	2.0	0.0000000000	False
choice on many problems	0.0	0.0	0.0	2.0	0.0000000000	False
plug in other functions	0.0	0.0	0.0	2.0	0.0000000000	False
formula i ve written	0.0	0.0	0.0	0.0	0.0000000000	False
cosmetically looks a bit	0.0	0.0	0.0	2.0	0.0000000000	False
associating with these points	0.0	0.0	0.0	2.0	0.0000000000	False
centered around the position	0.0	0.0	0.0	2.0	0.0000000000	False
ll give a weight	0.0	0.0	0.0	2.0	0.0000000000	False
proportional to the height	0.0	0.0	0.0	2.0	0.0000000000	False
evaluated at this point	0.0	0.0	0.0	2.0	0.0000000000	False
proportionate to that height	0.0	0.0	0.0	2.0	0.0000000000	False
parameter to this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
ll denote as tow	0.0	0.0	0.0	2.0	0.0000000000	False
suspiciously like the variants	0.0	0.0	0.0	2.0	0.0000000000	False
convenient form or function	0.0	0.0	0.0	2.0	0.0000000000	False
function this parameter tow	0.0	0.0	0.0	2.0	0.0000000000	False
fast the weights fall	0.0	0.0	0.0	2.0	0.0000000000	False
fall of with distance	0.0	0.0	0.0	2.0	0.0000000000	False
guess so if tow	0.0	0.0	0.0	2.0	0.0000000000	False
narrow gaussian  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
fairly narrow bell shape	0.0	0.0	0.0	2.0	0.0000000000	False
weights of the points	0.0	0.0	0.0	2.0	0.0000000000	False
rapidly whereas if tow	0.0	0.0	0.0	2.0	0.0000000000	False
choosing a weighting function	0.0	0.0	0.0	2.0	0.0000000000	False
weighting function that falls	0.0	0.0	0.0	2.0	0.0000000000	False
distance from your query	0.0	0.0	0.0	2.0	0.0000000000	False
regression to a data	0.0	0.0	0.0	2.0	0.0000000000	False
line making that prediction	0.0	0.0	0.0	2.0	0.0000000000	False
put a straight line	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to make	0.0	0.0	0.0	2.0	0.0000000000	False
run a new fitting	0.0	0.0	0.0	2.0	0.0000000000	False
procedure and then evaluate	0.0	0.0	0.0	2.0	0.0000000000	False
line that you fit	0.0	0.0	0.0	2.0	0.0000000000	False
position of the value	0.0	0.0	0.0	2.0	0.0000000000	False
position of the query	0.0	0.0	0.0	2.0	0.0000000000	False
query where you re	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to make	0.0	0.0	0.0	2.0	0.0000000000	False
point along the x-axis	0.0	0.0	0.0	2.0	0.0000000000	False
x-axis then you find	0.0	0.0	0.0	2.0	0.0000000000	False
find that locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
curve for a data	0.0	0.0	0.0	2.0	0.0000000000	False
problem set we re	0.0	0.0	0.0	0.0	0.0000000000	False
topic let me check	0.0	0.0	0.0	2.0	0.0000000000	False
weighted regression can run	0.0	0.0	0.0	2.0	0.0000000000	False
penancier for the problem	0.0	0.0	0.0	2.0	0.0000000000	False
problems with locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
things i ll leave	0.0	0.0	0.0	0.0	0.0000000000	False
leave you to discover	0.0	0.0	0.0	2.0	0.0000000000	False
mentioned yeah ? instructor	0.0	0.0	0.0	2.0	0.0000000000	False
right so the question	0.0	0.0	0.0	2.0	0.0000000000	False
write a code implementing	0.0	0.0	0.0	2.0	0.0000000000	False
code implementing locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
regression on the data	0.0	0.0	0.0	2.0	0.0000000000	False
band with parameter tow	0.0	0.0	0.0	2.0	0.0000000000	False
talk about model selection	0.0	0.0	0.0	2.0	0.0000000000	False
weights are not random	0.0	0.0	0.0	2.0	0.0000000000	False
purpose of this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
choose to define things	0.0	0.0	0.0	2.0	0.0000000000	False
lead anywhere in fact	0.0	0.0	0.0	2.0	0.0000000000	False
out that i happened	0.0	0.0	0.0	2.0	0.0000000000	False
bell-shaped function to define	0.0	0.0	0.0	2.0	0.0000000000	False
force in the definition	0.0	0.0	0.0	2.0	0.0000000000	False
huge set of houses	0.0	0.0	0.0	2.0	0.0000000000	False
linear for each house	0.0	0.0	0.0	2.0	0.0000000000	False
result for each input	0.0	0.0	0.0	2.0	0.0000000000	False
theta to your entire	0.0	0.0	0.0	2.0	0.0000000000	False
data set again turns	0.0	0.0	0.0	2.0	0.0000000000	False
out there are algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms that  turns	0.0	0.0	0.0	2.0	0.0000000000	False
out there are ways	0.0	0.0	0.0	2.0	0.0000000000	False
efficient for large data	0.0	0.0	0.0	2.0	0.0000000000	False
nt want to talk	0.0	0.0	0.0	0.0	0.0000000000	False
work of andrew moore	0.0	0.0	0.0	2.0	0.0000000000	False
andrew moore on kd-trees	0.0	0.0	0.0	2.0	0.0000000000	False
out ways to fit	0.0	0.0	0.0	2.0	0.0000000000	False
models much more efficiently	0.0	0.0	0.0	2.0	0.0000000000	False
locally weighted regression remember	0.0	0.0	0.0	2.0	0.0000000000	False
regression remember the outline	0.0	0.0	0.0	2.0	0.0000000000	False
beginning of this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
move on to talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
put aside locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
talk about ordinary unweighted	0.0	0.0	0.0	2.0	0.0000000000	False
ordinary unweighted linear regression	0.0	1.0	0.0	2.0	0.0000000000	False
things we could optimize	0.0	0.0	0.0	2.0	0.0000000000	False
square of the area	0.0	0.0	0.0	2.0	0.0000000000	False
area between the predictions	0.0	0.0	0.0	2.0	0.0000000000	False
predictions of the hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses and the values	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the absolute value	0.0	0.0	0.0	2.0	0.0000000000	False
value of the areas	0.0	0.0	0.0	2.0	0.0000000000	False
areas or the areas	0.0	0.0	0.0	2.0	0.0000000000	False
areas to the power	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions that will serve	0.0	0.0	0.0	2.0	0.0000000000	False
serve to  justify	0.0	0.0	0.0	2.0	0.0000000000	False
re minimizing the sum	0.0	0.0	0.0	2.0	0.0000000000	False
squares regression make sense	0.0	0.0	0.0	2.0	0.0000000000	False
describe do nt hold	0.0	0.0	0.0	0.0	0.0000000000	False
squares actually still makes	0.0	0.0	0.0	2.0	0.0000000000	False
sense in many circumstances	0.0	0.0	0.0	2.0	0.0000000000	False
circumstances but this sort	0.0	0.0	0.0	2.0	0.0000000000	False
endow the least squares	0.0	0.0	0.0	2.0	0.0000000000	False
model with probabilistic semantics	0.0	0.0	0.0	2.0	0.0000000000	False
house it s sold	0.0	0.0	0.0	0.0	0.0000000000	False
function of the features	0.0	0.0	0.0	2.0	0.0000000000	False
error term as capturing	0.0	0.0	0.0	2.0	0.0000000000	False
term as capturing unmodeled	0.0	0.0	0.0	2.0	0.0000000000	False
features of a house	0.0	0.0	0.0	2.0	0.0000000000	False
features that we jut	0.0	0.0	0.0	2.0	0.0000000000	False
jut fail to capture	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon as random noise	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon is our error	0.0	0.0	0.0	2.0	0.0000000000	False
error term that captures	0.0	0.0	0.0	2.0	0.0000000000	False
unmodeled effects just things	0.0	0.0	0.0	2.0	0.0000000000	False
model maybe the function	0.0	0.0	0.0	2.0	0.0000000000	False
assume that the errors	0.0	0.0	6.99747602221	10.0	0.4389721627	False
distribution i ll assume	0.0	0.0	0.0	0.0	0.0000000000	False
epsilon i are distributed	0.0	0.0	0.0	2.0	0.0000000000	False
right ? to denote	0.0	0.0	0.0	2.0	0.0000000000	False
denote a normal distribution	0.0	0.0	0.0	2.0	0.0000000000	False
quickly raise your hand	0.0	0.0	0.0	2.0	0.0000000000	False
hand if you ve	0.0	0.0	0.0	0.0	0.0000000000	False
density of our epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
implies that the probability	0.0	0.0	0.0	2.0	0.0000000000	False
distribution of a price	0.0	0.0	0.0	2.0	0.0000000000	False
price of a house	0.0	0.0	3.99798081777	8.0	0.2039800995	False
gaussian with that density	0.0	0.0	0.0	2.0	0.0000000000	False
house given the features	0.0	0.0	0.0	2.0	0.0000000000	False
features of the house	0.0	0.0	0.0	2.0	0.0000000000	False
house and my parameters	0.0	0.0	0.0	2.0	0.0000000000	False
variable that s distributed	0.0	0.0	0.0	0.0	0.0000000000	False
gaussian with mean theta	0.0	0.0	0.0	2.0	0.0000000000	False
transpose xi and variance	0.0	0.0	0.0	2.0	0.0000000000	False
variance sigma squared right	0.0	0.0	0.0	2.0	0.0000000000	False
housing prices are generated	0.0	0.0	0.0	2.0	0.0000000000	False
equal to theta transpose	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian noise with variance	0.0	0.0	0.0	2.0	0.0000000000	False
noise with variance sigma	0.0	0.0	0.0	2.0	0.0000000000	False
squared so the price	0.0	0.0	0.0	2.0	0.0000000000	False
make sense ? raise	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian ? two reasons	0.0	0.0	0.0	2.0	0.0000000000	False
limit theorem it turns	0.0	0.0	0.0	2.0	0.0000000000	False
vast majority of problems	0.0	0.0	0.0	2.0	0.0000000000	False
apply a linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
distribution of the errors	0.0	0.0	0.0	2.0	0.0000000000	False
find that the errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors really are gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
assumption for the error	0.0	0.0	0.0	2.0	0.0000000000	False
error in regression problems	0.0	0.0	0.0	2.0	0.0000000000	False
random variables will tend	0.0	0.0	0.0	2.0	0.0000000000	False
tend towards a gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
caused by many effects	0.0	0.0	0.0	2.0	0.0000000000	False
mood of the seller	0.0	0.0	0.0	2.0	0.0000000000	False
mood of the buyer	0.0	0.0	0.0	2.0	0.0000000000	False
features that we miss	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian if in practice	0.0	0.0	0.0	2.0	0.0000000000	False
unreasonable assumption i guess	0.0	0.0	0.0	2.0	0.0000000000	False
learning all the assumptions	0.0	0.0	0.0	2.0	0.0000000000	False
true in the absence	0.0	0.0	0.0	2.0	0.0000000000	False
housing prices are priced	0.0	0.0	0.0	2.0	0.0000000000	False
prices are not continued	0.0	0.0	0.0	2.0	0.0000000000	False
continued as value random	0.0	0.0	0.0	2.0	0.0000000000	False
cents in housing prices	0.0	0.0	0.0	2.0	0.0000000000	False
hurt our learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
ll say a bit	0.0	0.0	0.0	2.0	0.0000000000	False
generative and discriminative learning	0.0	0.0	0.0	2.0	0.0000000000	False
point out one bit	0.0	0.0	0.0	2.0	0.0000000000	False
theta as a random	0.0	0.0	0.0	2.0	0.0000000000	False
variable so in statistics	0.0	0.0	0.0	2.0	0.0000000000	False
true value of theta	0.0	0.0	5.99848561333	6.0	0.0000000000	False
theta that s out	0.0	0.0	0.0	0.0	0.0000000000	False
nt know what theta	0.0	0.0	0.0	0.0	0.0000000000	False
random value of theta	0.0	0.0	0.0	2.0	0.0000000000	False
value of theta out	0.0	0.0	0.0	4.0	0.0000000000	False
condition on random variables	0.0	0.0	0.0	2.0	0.0000000000	False
part of the class	0.0	0.0	0.0	2.0	0.0000000000	False
class where we re	0.0	0.0	0.0	0.0	0.0000000000	False
taking sort of frequentist	0.0	0.0	0.0	2.0	0.0000000000	False
class we re thinking	0.0	0.0	0.0	0.0	0.0000000000	False
re thinking of theta	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to estimate	0.0	0.0	0.0	2.0	0.0000000000	False
right so we re	0.0	0.0	0.0	0.0	0.0000000000	False
re gon na make	0.0	0.0	0.0	2.0	0.0000000000	False
make one more assumption	0.0	0.0	0.0	2.0	0.0000000000	False
assumption let s assume	0.0	0.0	0.0	0.0	0.0000000000	False
independently and identically distributed	0.0	0.0	0.0	2.0	0.0000000000	False
distributed part just means	0.0	0.0	0.0	2.0	0.0000000000	False
assuming that the epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
call this the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
theta as the probability	0.0	0.0	0.0	2.0	0.0000000000	False
product over my training	0.0	0.0	0.0	2.0	0.0000000000	False
densities that i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
probability of the data	0.0	0.0	7.99798081777	8.0	0.3445378151	False
probability are often confused	0.0	0.0	0.0	2.0	0.0000000000	False
confused so the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
thing as the probability	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood and for probability	0.0	0.0	0.0	2.0	0.0000000000	False
thing as a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of theta holding	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the parameters	0.0	0.0	3.99848561333	6.0	0.0000000000	False
parameters and the probability	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the data	0.0	0.0	0.0	2.0	0.0000000000	False
consistent in that terminology	0.0	0.0	0.0	2.0	0.0000000000	False
estimate the parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
choose for your model	0.0	0.0	0.0	2.0	0.0000000000	False
principle of maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
choose theta to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
words choose the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
massive likely your estimation	0.0	0.0	0.0	2.0	0.0000000000	False
case l of theta	0.0	0.0	0.0	2.0	0.0000000000	False
capital l of theta	0.0	0.0	0.0	2.0	0.0000000000	False
nt bother to write	0.0	0.0	0.0	0.0	0.0000000000	False
log and a product	0.0	0.0	0.0	2.0	0.0000000000	False
sum of over logs	0.0	0.0	0.0	2.0	0.0000000000	False
sum of the logs	0.0	0.0	0.0	2.0	0.0000000000	False
simplifies to m times	0.0	0.0	0.0	2.0	0.0000000000	False
times one over root	0.0	0.0	0.0	2.0	0.0000000000	False
root two pi sigma	0.0	0.0	0.0	2.0	0.0000000000	False
log of explanation cancel	0.0	0.0	0.0	2.0	0.0000000000	False
board okay so maximizing	0.0	0.0	0.0	2.0	0.0000000000	False
maximizing the log likelihood	0.0	0.0	0.0	4.0	0.0000000000	False
minus sign so maximizing	0.0	0.0	0.0	2.0	0.0000000000	False
ordinary least squares algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
assuming this probabilistic model	0.0	0.0	0.0	2.0	0.0000000000	False
assuming iid gaussian errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors on our data	0.0	0.0	0.0	2.0	0.0000000000	False
notice that the value	0.0	0.0	0.0	2.0	0.0000000000	False
value of sigma squared	0.0	0.0	2.99848561333	6.0	0.0000000000	False
squared does nt matter	0.0	0.0	0.0	0.0	0.0000000000	False
matter what the value	0.0	0.0	0.0	2.0	0.0000000000	False
variance of a gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
matter what sigma squared	0.0	0.0	0.0	4.0	0.0000000000	False
positive number the value	0.0	0.0	0.0	2.0	0.0000000000	False
clean up another couple	0.0	0.0	0.0	2.0	0.0000000000	False
ll see what questions	0.0	0.0	0.0	2.0	0.0000000000	False
re asking about overfitting	0.0	0.0	0.0	2.0	0.0000000000	False
thing s you re	0.0	0.0	0.0	0.0	0.0000000000	False
deeper questions about learning	0.0	0.0	0.0	2.0	0.0000000000	False
questions about learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
probabilistic interpretation in order	0.0	0.0	0.0	2.0	0.0000000000	False
derive our next learning	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to predict	0.0	0.0	0.0	4.0	0.0000000000	False
predict is continuous values	0.0	0.0	0.0	2.0	0.0000000000	False
value y you re	0.0	0.0	0.0	0.0	0.0000000000	False
predict will be discreet	0.0	0.0	0.0	2.0	0.0000000000	False
number of discrete values	0.0	0.0	0.0	2.0	0.0000000000	False
case i ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
ll talk about binding	0.0	0.0	0.0	2.0	0.0000000000	False
talk about binding classification	0.0	0.0	0.0	2.0	0.0000000000	False
classification where y takes	0.0	0.0	0.0	2.0	0.0000000000	False
problems if you re	0.0	0.0	0.0	0.0	0.0000000000	False
based on some features	0.0	0.0	0.0	2.0	0.0000000000	False
features that the patient	0.0	0.0	0.0	2.0	0.0000000000	False
patient has a disease	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to decide	0.0	0.0	0.0	2.0	0.0000000000	False
decide will this house	0.0	0.0	0.0	2.0	0.0000000000	False
ll either be sold	0.0	0.0	0.0	2.0	0.0000000000	False
nt be other standing	0.0	0.0	0.0	0.0	0.0000000000	False
build a spam filter	0.0	0.0	0.0	2.0	0.0000000000	False
sit in whether predicting	0.0	0.0	0.0	2.0	0.0000000000	False
predicting whether a computer	0.0	0.0	0.0	2.0	0.0000000000	False
computer system will crash	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to predict	0.0	0.0	0.0	2.0	0.0000000000	False
predict will this computing	0.0	0.0	0.0	2.0	0.0000000000	False
classification problem y takes	0.0	0.0	0.0	2.0	0.0000000000	False
takes on two values	0.0	0.0	0.0	2.0	0.0000000000	False
set you can fit	0.0	0.0	0.0	2.0	0.0000000000	False
data set i ve	0.0	0.0	0.0	0.0	0.0000000000	False
set i ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
amazingly easy classification problem	0.0	0.0	0.0	2.0	0.0000000000	False
right ? the relationship	0.0	0.0	0.0	2.0	0.0000000000	False
regressions to this data	0.0	0.0	0.0	4.0	0.0000000000	False
hypothesis to this straight	0.0	0.0	0.0	2.0	0.0000000000	False
straight line and threshold	0.0	0.0	0.0	2.0	0.0000000000	False
right answer you predict	0.0	0.0	0.0	2.0	0.0000000000	False
linear regression to classification	0.0	0.0	3.99848561333	6.0	0.0000000000	False
regression to classification problems	0.0	0.0	0.0	4.0	0.0000000000	False
bad idea to apply	0.0	0.0	0.0	4.0	0.0000000000	False
change my training set	0.0	0.0	0.0	2.0	0.0000000000	False
training set by giving	0.0	0.0	0.0	2.0	0.0000000000	False
obvious what the relationship	0.0	0.0	0.0	2.0	0.0000000000	False
convey much new information	0.0	0.0	0.0	2.0	0.0000000000	False
surprise that this corresponds	0.0	0.0	0.0	2.0	0.0000000000	False
corresponds to y equals	0.0	0.0	0.0	2.0	0.0000000000	False
data set you end	0.0	0.0	0.0	2.0	0.0000000000	False
predictions of your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
completely if your threshold	0.0	0.0	0.0	2.0	0.0000000000	False
threshold  your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
make it even worse	0.0	0.0	0.0	2.0	0.0000000000	False
regression to classification algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm sometimes it work	0.0	0.0	0.0	2.0	0.0000000000	False
value of y lies	0.0	0.0	0.0	2.0	0.0000000000	False
form of our hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
lies in the unit	0.0	0.0	0.0	2.0	0.0000000000	False
choosing a linear function	0.0	0.0	0.0	2.0	0.0000000000	False
function for my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
theta x of theta	0.0	0.0	0.0	2.0	0.0000000000	False
large and it crosses	0.0	0.0	0.0	2.0	0.0000000000	False
crosses the vertical axis	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down this function	0.0	0.0	0.0	2.0	0.0000000000	False
function it actually turns	0.0	0.0	0.0	2.0	0.0000000000	False
out naturally as part	0.0	0.0	0.0	2.0	0.0000000000	False
broader class of models	0.0	0.0	0.0	2.0	0.0000000000	False
models and another reason	0.0	0.0	0.0	2.0	0.0000000000	False
talk about next week	0.0	0.0	0.0	2.0	0.0000000000	False
output by my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
outputs and my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
assume that the probability	0.0	0.0	0.0	2.0	0.0000000000	False
equal to h subscript	0.0	0.0	0.0	4.0	0.0000000000	False
imagine that my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
outputting all these numbers	0.0	0.0	0.0	2.0	0.0000000000	False
probability of y equals	0.0	0.0	0.0	2.0	0.0000000000	False
two equations and write	0.0	0.0	0.0	2.0	0.0000000000	False
write them more compactly	0.0	0.0	0.0	2.0	0.0000000000	False
power of y times	0.0	0.0	0.0	2.0	0.0000000000	False
power of one minus	0.0	0.0	0.0	2.0	0.0000000000	False
power of one times	0.0	0.0	0.0	2.0	0.0000000000	False
thing to the power	0.0	0.0	0.0	2.0	0.0000000000	False
times this thing power	0.0	0.0	0.0	2.0	0.0000000000	False
compact way of writing	0.0	0.0	0.0	2.0	0.0000000000	False
hope our parameter fitting	0.0	0.0	0.0	2.0	0.0000000000	False
fit the parameters theta	0.0	0.0	0.0	4.0	0.0000000000	False
theta of my model	0.0	0.0	0.0	2.0	0.0000000000	False
pfyi given xi parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
dropped this theta subscript	0.0	0.0	0.0	2.0	0.0000000000	False
write a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
find a maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
find the  setting	0.0	0.0	0.0	2.0	0.0000000000	False
setting the parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
parameters theta that maximizes	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood l of theta	0.0	0.0	0.0	4.0	0.0000000000	False
work with the derivations	0.0	0.0	0.0	2.0	0.0000000000	False
log of the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood rather than maximize	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood so the log	0.0	0.0	0.0	2.0	0.0000000000	False
theta of our model	0.0	0.0	0.0	2.0	0.0000000000	False
model we ll find	0.0	0.0	0.0	0.0	0.0000000000	False
ll find the value	0.0	0.0	0.0	2.0	0.0000000000	False
maximizes this log likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
apply the same gradient	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that we learned	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
function and you remember	0.0	0.0	0.0	2.0	0.0000000000	False
talked about least squares	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the quadratic error	0.0	0.0	0.0	4.0	0.0000000000	False
likelihood and you remember	0.0	0.0	0.0	2.0	0.0000000000	False
repeatedly take the value	0.0	0.0	0.0	2.0	0.0000000000	False
theta and you replace	0.0	0.0	0.0	2.0	0.0000000000	False
previous value of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta plus a learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning rate alpha times	0.0	0.0	0.0	2.0	0.0000000000	False
alpha times the gradient	0.0	0.0	0.0	2.0	0.0000000000	False
gradient of the cos	0.0	0.0	0.0	2.0	0.0000000000	False
cos function the log	0.0	0.0	0.0	2.0	0.0000000000	False
function the log likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
log likelihood will respect	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic error term today	0.0	0.0	0.0	2.0	0.0000000000	False
term today we re	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
maximize rather than minimize	0.0	0.0	0.0	2.0	0.0000000000	False
call this gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm so to figure	0.0	0.0	0.0	2.0	0.0000000000	False
out what this gradient	0.0	0.0	0.0	2.0	0.0000000000	False
order to derive gradient	0.0	0.0	0.0	2.0	0.0000000000	False
compute the partial derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
derivatives of your objective	0.0	0.0	0.0	2.0	0.0000000000	False
objective function with respect	0.0	0.0	0.0	2.0	0.0000000000	False
right ? it turns	0.0	0.0	0.0	2.0	0.0000000000	False
compute this partial derivative	0.0	0.0	0.0	2.0	0.0000000000	False
lower case l theta	0.0	0.0	0.0	2.0	0.0000000000	False
log likelihood of theta	0.0	0.0	0.0	2.0	0.0000000000	False
partial derivative with respect	0.0	0.0	0.0	2.0	0.0000000000	False
theta i you find	0.0	0.0	0.0	2.0	0.0000000000	False
write down a couple	0.0	0.0	0.0	2.0	0.0000000000	False
blackboards full of math	0.0	0.0	0.0	2.0	0.0000000000	False
plug in the definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition for f subscript	0.0	0.0	0.0	2.0	0.0000000000	False
subscript theta as function	0.0	0.0	0.0	2.0	0.0000000000	False
work through the algebra	0.0	0.0	0.0	2.0	0.0000000000	False
out it ll simplify	0.0	0.0	0.0	0.0	0.0000000000	False
theta j gets updated	0.0	0.0	0.0	2.0	0.0000000000	False
theta j plus alpha	0.0	0.0	0.0	2.0	0.0000000000	False
remember seeing this formula	0.0	0.0	0.0	2.0	0.0000000000	False
last lecture ? right	0.0	0.0	0.0	2.0	0.0000000000	False
worked up bastrian descent	0.0	0.0	0.0	2.0	0.0000000000	False
descent for least squares	0.0	0.0	0.0	2.0	0.0000000000	False
making all that noise	0.0	0.0	0.0	2.0	0.0000000000	False
earlier about least squares	0.0	0.0	0.0	2.0	0.0000000000	False
bad idea for classification	0.0	0.0	0.0	2.0	0.0000000000	False
idea for classification problems	0.0	0.0	0.0	2.0	0.0000000000	False
math and i skipped	0.0	0.0	0.0	2.0	0.0000000000	False
claiming at the end	0.0	0.0	0.0	2.0	0.0000000000	False
right ? the definition	0.0	0.0	0.0	2.0	0.0000000000	False
logistic function of theta	0.0	0.0	0.0	2.0	0.0000000000	False
function of theta transpose	0.0	0.0	0.0	2.0	0.0000000000	False
similar on the surface	0.0	0.0	0.0	2.0	0.0000000000	False
descent rule i derived	0.0	0.0	0.0	2.0	0.0000000000	False
totally different learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
coincidence that you ended	0.0	0.0	0.0	2.0	0.0000000000	False
elegant generalized learning models	0.0	0.0	0.0	2.0	0.0000000000	False
cool one last comment	0.0	0.0	0.0	2.0	0.0000000000	False
last comment as part	0.0	0.0	0.0	2.0	0.0000000000	False
part of a sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of learning process	0.0	0.0	0.0	2.0	0.0000000000	False
derivatives and i ended	0.0	0.0	0.0	2.0	0.0000000000	False
nt want to make	0.0	0.0	0.0	0.0	0.0000000000	False
wrote out the entirety	0.0	0.0	0.0	2.0	0.0000000000	False
entirety of this derivation	0.0	0.0	0.0	2.0	0.0000000000	False
follow every single step	0.0	0.0	0.0	2.0	0.0000000000	False
derivatives of this log	0.0	0.0	0.0	2.0	0.0000000000	False
interested in seriously masking	0.0	0.0	0.0	2.0	0.0000000000	False
masking machine learning material	0.0	0.0	0.0	2.0	0.0000000000	False
lecture notes and read	0.0	0.0	0.0	2.0	0.0000000000	False
read through every line	0.0	0.0	0.0	2.0	0.0000000000	False
line and go yep	0.0	0.0	0.0	2.0	0.0000000000	False
material my concrete suggestion	0.0	0.0	0.0	2.0	0.0000000000	False
read through the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
cover up the derivation	0.0	0.0	0.0	4.0	0.0000000000	False
good advice for studying	0.0	0.0	0.0	2.0	0.0000000000	False
technical material like machine	0.0	0.0	0.0	2.0	0.0000000000	False
material like machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
rederive the entire thing	0.0	0.0	0.0	2.0	0.0000000000	False
pieces of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
theory and various proofs	0.0	0.0	0.0	2.0	0.0000000000	False
great way to study	0.0	0.0	0.0	2.0	0.0000000000	False
original derivation all right	0.0	0.0	0.0	2.0	0.0000000000	False
nt get to newton	0.0	0.0	0.0	0.0	0.0000000000	False
newton s method today	0.0	0.0	0.0	0.0	0.0000000000	False
quick digression to talk	0.0	0.0	0.0	2.0	0.0000000000	False
discussion sort of alluding	0.0	0.0	0.0	2.0	0.0000000000	False
lot about the perceptron	0.0	0.0	0.0	2.0	0.0000000000	False
quarter we ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
ll talk about learning	0.0	0.0	0.0	2.0	0.0000000000	False
talk about learning theory	0.0	0.0	0.0	4.0	0.0000000000	False
theta of x equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals g of theta	0.0	0.0	0.0	2.0	0.0000000000	False
step function it turns	0.0	0.0	0.0	2.0	0.0000000000	False
learning called the perceptron	0.0	0.0	0.0	2.0	0.0000000000	False
ascent for logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression and the learning	0.0	0.0	0.0	2.0	0.0000000000	False
classic gradient ascent rule	0.0	0.0	0.0	2.0	0.0000000000	False
rule for logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm than least squares	0.0	0.0	0.0	2.0	0.0000000000	False
regression and logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm with probabilistic semantics	0.0	0.0	0.0	2.0	0.0000000000	False
type of learning rule	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm than logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
simplicity of this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
talks about the perceptron	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm this particular video	0.0	0.0	0.0	2.0	0.0000000000	False
series titled the machine	0.0	0.0	0.0	2.0	0.0000000000	False
world and was produced	0.0	0.0	0.0	2.0	0.0000000000	False
wgbh television in cooperation	0.0	0.0	0.0	2.0	0.0000000000	False
cooperation with the bbc	0.0	0.0	0.0	2.0	0.0000000000	False
pbs a few years	0.0	0.0	0.0	2.0	0.0000000000	False
years ago this shows	0.0	0.0	0.0	2.0	0.0000000000	False
shows you what machine	0.0	0.0	0.0	2.0	0.0000000000	False
fun clip on perceptron	0.0	0.0	0.0	2.0	0.0000000000	False
clip on perceptron algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
built a few working	0.0	0.0	0.0	2.0	0.0000000000	False
explore the mysterious problem	0.0	0.0	0.0	2.0	0.0000000000	False
brain learns this perceptron	0.0	0.0	0.0	2.0	0.0000000000	False
perceptron is being trained	0.0	0.0	0.0	2.0	0.0000000000	False
out many complex rules	0.0	0.0	0.0	2.0	0.0000000000	False
complex rules about faces	0.0	0.0	0.0	2.0	0.0000000000	False
writing a computer program	0.0	0.0	0.0	2.0	0.0000000000	False
facial features and hair	0.0	0.0	0.0	2.0	0.0000000000	False
features and hair outline	0.0	0.0	0.0	2.0	0.0000000000	False
hair outline and takes	0.0	0.0	0.0	2.0	0.0000000000	False
outline and takes longer	0.0	0.0	0.0	2.0	0.0000000000	False
takes longer to learn	0.0	0.0	0.0	2.0	0.0000000000	False
told by dr taylor	0.0	0.0	0.0	2.0	0.0000000000	False
puts on his wig	0.0	0.0	0.0	2.0	0.0000000000	False
part searching after training	0.0	0.0	0.0	2.0	0.0000000000	False
distinguish male from female	0.0	0.0	0.0	2.0	0.0000000000	False
female it has learned	0.0	0.0	0.0	2.0	0.0000000000	False
ll see you guys	0.0	0.0	0.0	2.0	0.0000000000	False
announcements before we jump	0.0	0.0	0.0	2.0	0.0000000000	False
today s technical material	0.0	0.0	0.0	0.0	0.0000000000	False
handout with the sort	0.0	0.0	0.0	2.0	0.0000000000	False
choosing and proposing class	0.0	0.0	0.0	2.0	0.0000000000	False
class projects so project	0.0	0.0	0.0	2.0	0.0000000000	False
projects so project proposals	0.0	0.0	0.0	2.0	0.0000000000	False
project for this class	0.0	0.0	0.0	2.0	0.0000000000	False
class due on friday	0.0	0.0	0.0	2.0	0.0000000000	False
19th of this month	0.0	0.0	0.0	2.0	0.0000000000	False
two and a half	0.0	0.0	0.0	2.0	0.0000000000	False
nt yet formed teams	0.0	0.0	0.0	0.0	0.0000000000	False
formed teams or started	0.0	0.0	0.0	2.0	0.0000000000	False
teams or started thinking	0.0	0.0	0.0	2.0	0.0000000000	False
started thinking about project	0.0	0.0	0.0	2.0	0.0000000000	False
thinking about project ideas	0.0	0.0	0.0	2.0	0.0000000000	False
handout with the guidelines	0.0	0.0	0.0	2.0	0.0000000000	False
send me your proposals	0.0	0.0	0.0	2.0	0.0000000000	False
sort of just fishing	0.0	0.0	0.0	2.0	0.0000000000	False
fishing around for ideas	0.0	0.0	0.0	2.0	0.0000000000	False
office hours on friday	0.0	0.0	0.0	2.0	0.0000000000	False
hours on friday mornings	0.0	0.0	0.0	2.0	0.0000000000	False
list of project ideas	0.0	0.0	0.0	2.0	0.0000000000	False
ideas that i sort	0.0	0.0	0.0	2.0	0.0000000000	False
collected from my colleagues	0.0	0.0	0.0	2.0	0.0000000000	False
senior phd students working	0.0	0.0	0.0	2.0	0.0000000000	False
ideas and a variety	0.0	0.0	0.0	2.0	0.0000000000	False
re having trouble coming	0.0	0.0	0.0	2.0	0.0000000000	False
previous class i mentioned	0.0	0.0	0.0	2.0	0.0000000000	False
fun and educational thing	0.0	0.0	0.0	2.0	0.0000000000	False
registered in this class	0.0	0.0	0.0	2.0	0.0000000000	False
logistical details about applying	0.0	0.0	0.0	2.0	0.0000000000	False
encourage you to sort	0.0	0.0	0.0	2.0	0.0000000000	False
respond to that email	0.0	0.0	0.0	2.0	0.0000000000	False
ll get later today	0.0	0.0	0.0	2.0	0.0000000000	False
due in two weeks	0.0	0.0	0.0	2.0	0.0000000000	False
late days for problem	0.0	0.0	0.0	2.0	0.0000000000	False
days for problem set	0.0	0.0	0.0	2.0	0.0000000000	False
based on problem set	0.0	0.0	0.0	2.0	0.0000000000	False
problem set one solutions	0.0	0.0	0.0	2.0	0.0000000000	False
set one solutions questions	0.0	0.0	0.0	2.0	0.0000000000	False
talk about new test	0.0	0.0	0.0	2.0	0.0000000000	False
test methods for fitting	0.0	0.0	0.0	2.0	0.0000000000	False
methods for fitting models	0.0	0.0	0.0	2.0	0.0000000000	False
models like logistic regression	0.0	0.0	0.0	4.0	0.0000000000	False
talk about exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
family distributions and generalized	0.0	0.0	0.0	2.0	0.0000000000	False
nice class of ideas	0.0	0.0	0.0	2.0	0.0000000000	False
ideas that will tie	0.0	0.0	0.0	2.0	0.0000000000	False
ordinary v squares models	0.0	0.0	0.0	2.0	0.0000000000	False
lecture and this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
large amounts of material	0.0	0.0	0.0	2.0	0.0000000000	False
sort of the foundations	0.0	0.0	0.0	2.0	0.0000000000	False
prerequisites for this class	0.0	0.0	0.0	2.0	0.0000000000	False
terms of a background	0.0	0.0	0.0	2.0	0.0000000000	False
section taught this week	0.0	0.0	0.0	2.0	0.0000000000	False
briefly go over sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of octave notation	0.0	0.0	0.0	2.0	0.0000000000	False
review of the probability	0.0	0.0	0.0	2.0	0.0000000000	False
discussion section all right	0.0	0.0	0.0	2.0	0.0000000000	False
last lecture i talked	0.0	0.0	0.0	2.0	0.0000000000	False
theta under this model	0.0	0.0	0.0	2.0	0.0000000000	False
write down the log	0.0	0.0	0.0	4.0	0.0000000000	False
log like we heard	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a gradient	0.0	0.0	0.0	2.0	0.0000000000	False
ascent interval for finding	0.0	0.0	0.0	2.0	0.0000000000	False
finding the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of the parameter	0.0	0.0	0.0	4.0	0.0000000000	False
last time i wrote	0.0	0.0	0.0	4.0	0.0000000000	False
wrote down the learning	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
favor a logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
meaning find the value	0.0	0.0	0.0	2.0	0.0000000000	False
maximizes this log likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
gradient ascent or gradient	0.0	0.0	0.0	2.0	0.0000000000	False
ascent or gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for fitting models	0.0	0.0	0.0	2.0	0.0000000000	False
faster than gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
ascent and this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm is called newton	0.0	0.0	0.0	2.0	0.0000000000	False
describe newton s method	0.0	0.0	0.0	0.0	0.0000000000	False
function f of theta	0.0	0.0	0.0	2.0	0.0000000000	False
sort of slowly change	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for fitting mass	0.0	0.0	0.0	2.0	0.0000000000	False
fitting mass and likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
mass and likelihood models	0.0	0.0	0.0	2.0	0.0000000000	False
axis of of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta as some value	0.0	0.0	0.0	2.0	0.0000000000	False
value we ll call	0.0	0.0	0.0	0.0	0.0000000000	False
ll call theta superscript	0.0	0.0	0.0	2.0	0.0000000000	False
method does we re	0.0	0.0	0.0	0.0	0.0000000000	False
re going to evaluate	0.0	0.0	0.0	2.0	0.0000000000	False
ll use the linear	0.0	0.0	0.0	2.0	0.0000000000	False
consummation to the function	0.0	0.0	0.0	2.0	0.0000000000	False
tangents to my function	0.0	0.0	0.0	4.0	0.0000000000	False
hope that makes sense	0.0	0.0	0.0	2.0	0.0000000000	False
makes sense  starting	0.0	0.0	0.0	2.0	0.0000000000	False
starting the function work	0.0	0.0	0.0	2.0	0.0000000000	False
function work out nicely	0.0	0.0	0.0	2.0	0.0000000000	False
function at that point	0.0	0.0	0.0	2.0	0.0000000000	False
intercepts the horizontal axis	0.0	0.0	0.0	2.0	0.0000000000	False
thing with the dec	0.0	0.0	0.0	2.0	0.0000000000	False
point take the tangent	0.0	0.0	0.0	2.0	0.0000000000	False
iterations of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
theta zero to theta	0.0	0.0	0.0	2.0	0.0000000000	False
call that capital delta	0.0	0.0	0.0	2.0	0.0000000000	False
capital delta so capital	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of f evaluated	0.0	0.0	0.0	2.0	0.0000000000	False
horizontal length a gradient	0.0	0.0	0.0	2.0	0.0000000000	False
slope of this function	0.0	0.0	0.0	2.0	0.0000000000	False
defined as the ratio	0.0	0.0	0.0	2.0	0.0000000000	False
height and this width	0.0	0.0	0.0	2.0	0.0000000000	False
theta zero minus delta	0.0	0.0	0.0	2.0	0.0000000000	False
newton s method precedes	0.0	0.0	0.0	0.0	0.0000000000	False
equals theta t minus	0.0	0.0	0.0	4.0	0.0000000000	False
minus f of theta	0.0	0.0	0.0	2.0	0.0000000000	False
apply the same idea	0.0	0.0	0.0	4.0	0.0000000000	False
maximizing the log likelihood	0.0	0.0	0.0	4.0	0.0000000000	False
function l of theta	0.0	0.0	0.0	2.0	0.0000000000	False
function ? you set	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of the function	0.0	0.0	0.0	2.0	0.0000000000	False
theta one equals theta	0.0	0.0	0.0	2.0	0.0000000000	False
double prime of theta	0.0	0.0	0.0	2.0	0.0000000000	False
equal to l prime	0.0	0.0	0.0	2.0	0.0000000000	False
optimum does this make	0.0	0.0	0.0	2.0	0.0000000000	False
sense ? any questions	0.0	0.0	0.0	2.0	0.0000000000	False
complicated there are conditions	0.0	0.0	0.0	2.0	0.0000000000	False
models i ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
matter when i implement	0.0	0.0	0.0	2.0	0.0000000000	False
back to all zeros	0.0	0.0	0.0	2.0	0.0000000000	False
deal how you initialize	0.0	0.0	0.0	2.0	0.0000000000	False
ll sort of answer	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms will generally converge	0.0	0.0	0.0	2.0	0.0000000000	False
large a linear rate	0.0	0.0	0.0	2.0	0.0000000000	False
linear rate for gradient	0.0	0.0	0.0	2.0	0.0000000000	False
rate for gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
conversions of these algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
turns out that newton	0.0	0.0	0.0	2.0	0.0000000000	False
method is an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
enjoys extremely fast conversions	0.0	0.0	0.0	2.0	0.0000000000	False
conversions the technical term	0.0	0.0	0.0	2.0	0.0000000000	False
means that every iteration	0.0	0.0	0.0	2.0	0.0000000000	False
number of significant digits	0.0	0.0	0.0	2.0	0.0000000000	False
digits that your solution	0.0	0.0	0.0	2.0	0.0000000000	False
accurate to just lots	0.0	0.0	0.0	2.0	0.0000000000	False
lots of constant factors	0.0	0.0	0.0	2.0	0.0000000000	False
essentially get to square	0.0	0.0	0.0	2.0	0.0000000000	False
error on every iteration	0.0	0.0	0.0	2.0	0.0000000000	False
newton s method result	0.0	0.0	0.0	0.0	0.0000000000	False
method result that holds	0.0	0.0	0.0	2.0	0.0000000000	False
accurate but the fact	0.0	0.0	0.0	2.0	0.0000000000	False
implement newton s method	0.0	0.0	0.0	0.0	0.0000000000	False
method for logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
converges like a dozen	0.0	0.0	0.0	2.0	0.0000000000	False
size problems of tens	0.0	0.0	0.0	2.0	0.0000000000	False
features so one thing	0.0	0.0	0.0	2.0	0.0000000000	False
thing i should talk	0.0	0.0	0.0	2.0	0.0000000000	False
method for the case	0.0	0.0	0.0	2.0	0.0000000000	False
single-row number the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
method for when theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta is a vector	0.0	0.0	0.0	2.0	0.0000000000	False
gradient of your objective	0.0	0.0	0.0	2.0	0.0000000000	False
derivative where hij equals	0.0	0.0	0.0	2.0	0.0000000000	False
vector of first derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
first derivatives times sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of the inverse	0.0	0.0	0.0	2.0	0.0000000000	False
inverse of the matrix	0.0	0.0	0.0	2.0	0.0000000000	False
matrix of second derivatives	0.0	0.0	0.0	4.0	0.0000000000	False
thing of multiple dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
reasonable number of features	0.0	0.0	0.0	4.0	0.0000000000	False
features and training examples	0.0	0.0	0.0	2.0	0.0000000000	False
conversion anywhere from sort	0.0	0.0	0.0	2.0	0.0000000000	False
compare to gradient ascent	0.0	0.0	0.0	4.0	0.0000000000	False
means far fewer iterations	0.0	0.0	0.0	2.0	0.0000000000	False
fewer iterations to converge	0.0	0.0	0.0	2.0	0.0000000000	False
iterations to converge compared	0.0	0.0	0.0	2.0	0.0000000000	False
converge compared to gradient	0.0	0.0	0.0	2.0	0.0000000000	False
large number of features	0.0	0.0	0.0	2.0	0.0000000000	False
features in your learning	0.0	0.0	0.0	2.0	0.0000000000	False
slightly computationally expensive step	0.0	0.0	0.0	2.0	0.0000000000	False
minus thank you problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem also i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
parameters for logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
wanted to use newton	0.0	0.0	0.0	2.0	0.0000000000	False
change ? all right	0.0	0.0	0.0	2.0	0.0000000000	False
change i ll leave	0.0	0.0	0.0	0.0	0.0000000000	False
leave you to work	0.0	0.0	0.0	4.0	0.0000000000	False
right let s talk	0.0	0.0	0.0	0.0	0.0000000000	False
talk about generalized linear	0.0	0.0	0.0	4.0	0.0000000000	False
algorithms we ve talked	0.0	0.0	0.0	0.0	0.0000000000	False
algorithms for modeling pfy	0.0	0.0	0.0	2.0	0.0000000000	False
natural distribution of zeros	0.0	0.0	0.0	2.0	0.0000000000	False
distribution models random variables	0.0	0.0	0.0	2.0	0.0000000000	False
variables with two values	0.0	0.0	0.0	2.0	0.0000000000	False
functions i could ve	0.0	0.0	0.0	0.0	0.0000000000	False
default choice that lead	0.0	0.0	0.0	2.0	0.0000000000	False
longer piece of chalk	0.0	0.0	0.0	2.0	0.0000000000	False
chalk i should warn	0.0	0.0	0.0	2.0	0.0000000000	False
ideas in generalized linear	0.0	0.0	0.0	2.0	0.0000000000	False
point you  point	0.0	0.0	0.0	2.0	0.0000000000	False
out the key ideas	0.0	0.0	0.0	2.0	0.0000000000	False
key ideas and give	0.0	0.0	0.0	2.0	0.0000000000	False
give you a gist	0.0	0.0	0.0	2.0	0.0000000000	False
details in the map	0.0	0.0	0.0	2.0	0.0000000000	False
map and the derivations	0.0	0.0	0.0	2.0	0.0000000000	False
derivations i ll leave	0.0	0.0	0.0	0.0	0.0000000000	False
suppose we have data	0.0	0.0	0.0	2.0	0.0000000000	False
variable parameterized by phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi so the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution has the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability of y equals	0.0	0.0	0.0	4.0	0.0000000000	False
right so the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
phi in the specifies	0.0	0.0	0.0	2.0	0.0000000000	False
vary the parameter theta	0.0	0.0	0.0	2.0	0.0000000000	False
distributions as you vary	0.0	0.0	0.0	2.0	0.0000000000	False
ll say the cost	0.0	0.0	0.0	2.0	0.0000000000	False
ll say a bit	0.0	0.0	0.0	2.0	0.0000000000	False
parameter of the distribution	0.0	0.0	0.0	4.0	0.0000000000	False
choice of these functions	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na sort	0.0	0.0	0.0	2.0	0.0000000000	False
forms of the functions	0.0	0.0	0.0	2.0	0.0000000000	False
write down specific formulas	0.0	0.0	0.0	2.0	0.0000000000	False
gaussians are special cases	0.0	0.0	0.0	2.0	0.0000000000	False
cases of exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
formula of the distributions	0.0	0.0	0.0	2.0	0.0000000000	False
distributions with different means	0.0	0.0	0.0	4.0	0.0000000000	False
ll get gaussian distributions	0.0	0.0	0.0	2.0	0.0000000000	False
means for my fixed	0.0	0.0	0.0	2.0	0.0000000000	False
sufficient statistic and statistics	0.0	0.0	0.0	2.0	0.0000000000	False
formal sense of sufficient	0.0	0.0	0.0	2.0	0.0000000000	False
statistic for a probability	0.0	0.0	0.0	2.0	0.0000000000	False
worry about we sort	0.0	0.0	0.0	2.0	0.0000000000	False
nt need that property	0.0	0.0	0.0	0.0	0.0000000000	False
parameter of this distribution	0.0	0.0	0.0	2.0	0.0000000000	False
product of raw numbers	0.0	0.0	0.0	2.0	0.0000000000	False
examples of exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
distributions we ll start	0.0	0.0	0.0	0.0	0.0000000000	False
equals one by phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi so the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
parameter of phi specifies	0.0	0.0	0.0	2.0	0.0000000000	False
phi specifies the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability that y equals	0.0	0.0	5.99732477261	10.0	0.3255425710	False
identical to my formula	0.0	0.0	0.0	2.0	0.0000000000	False
formula for the distribution	0.0	0.0	0.0	4.0	0.0000000000	False
probability of y parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
notation where we talked	0.0	0.0	0.0	2.0	0.0000000000	False
talked about logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression the probability	0.0	0.0	0.0	2.0	0.0000000000	False
times one minus phi	0.0	0.0	0.0	2.0	0.0000000000	False
exponent of the log	0.0	0.0	0.0	2.0	0.0000000000	False
exponentiation in taking log	0.0	0.0	0.0	2.0	0.0000000000	False
cancel each other out	0.0	0.0	0.0	2.0	0.0000000000	False
make sure it makes	0.0	0.0	0.0	2.0	0.0000000000	False
sense i ll clean	0.0	0.0	0.0	0.0	0.0000000000	False
ll clean another board	0.0	0.0	0.0	2.0	0.0000000000	False
four equal to log	0.0	0.0	0.0	2.0	0.0000000000	False
equal to log phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi over one minus	0.0	0.0	0.0	2.0	0.0000000000	False
theta as a function	0.0	0.0	0.0	2.0	0.0000000000	False
logistic function magically falls	0.0	0.0	0.0	2.0	0.0000000000	False
definitions from the board	0.0	0.0	0.0	2.0	0.0000000000	False
log of one minus	0.0	0.0	0.0	2.0	0.0000000000	False
phi and are function	0.0	0.0	0.0	2.0	0.0000000000	False
plug in this definition	0.0	0.0	0.0	4.0	0.0000000000	False
recap what we ve	0.0	0.0	0.0	0.0	0.0000000000	False
function of the distribution	0.0	0.0	0.0	4.0	0.0000000000	False
expand this term out	0.0	0.0	0.0	2.0	0.0000000000	False
minus y times log	0.0	0.0	2.99839486356	6.0	0.0000000000	False
times log y minus	0.0	0.0	0.0	4.0	0.0000000000	False
log y minus phi	0.0	0.0	0.0	4.0	0.0000000000	False
log  one minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus of a log	0.0	0.0	0.0	2.0	0.0000000000	False
times log one minus	0.0	0.0	0.0	2.0	0.0000000000	False
log one minus phi	0.0	0.0	0.0	2.0	0.0000000000	False
minus phi becomes sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of y times	0.0	0.0	0.0	2.0	0.0000000000	False
phi does that make	0.0	0.0	0.0	2.0	0.0000000000	False
times a real number	0.0	0.0	0.0	2.0	0.0000000000	False
one-dimensional vector transposed times	0.0	0.0	0.0	2.0	0.0000000000	False
times a one-dimensional vector	0.0	0.0	0.0	2.0	0.0000000000	False
number times real number	0.0	0.0	0.0	2.0	0.0000000000	False
number towards the end	0.0	0.0	0.0	2.0	0.0000000000	False
out to be scalers	0.0	0.0	0.0	2.0	0.0000000000	False
imagine that we re	0.0	0.0	0.0	0.0	0.0000000000	False
re restricting the domain	0.0	0.0	0.0	2.0	0.0000000000	False
domain of the input	0.0	0.0	0.0	2.0	0.0000000000	False
input of the function	0.0	0.0	0.0	2.0	0.0000000000	False
function for y equals	0.0	0.0	0.0	2.0	0.0000000000	False
write down y equals	0.0	0.0	0.0	2.0	0.0000000000	False
cool so this takes	0.0	0.0	0.0	2.0	0.0000000000	False
invites in the form	0.0	0.0	0.0	2.0	0.0000000000	False
quickly for the gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
nt do the algebra	0.0	0.0	0.0	0.0	0.0000000000	False
algebra for the gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
basically just write out	0.0	0.0	0.0	2.0	0.0000000000	False
write out the answers	0.0	0.0	0.0	2.0	0.0000000000	False
normal distribution with sequence	0.0	0.0	0.0	2.0	0.0000000000	False
distribution with sequence squared	0.0	0.0	0.0	2.0	0.0000000000	False
dividing the maximum likelihood	0.0	0.0	0.0	4.0	0.0000000000	False
maximum likelihood  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of ordinary squares	0.0	0.0	0.0	2.0	0.0000000000	False
ordinary squares we showed	0.0	0.0	0.0	2.0	0.0000000000	False
showed that the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
squared did nt matter	0.0	0.0	0.0	0.0	0.0000000000	False
matter when we divide	0.0	0.0	0.0	2.0	0.0000000000	False
value of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
purposes of just writing	0.0	0.0	0.0	2.0	0.0000000000	False
worry about it lecture	0.0	0.0	0.0	2.0	0.0000000000	False
talks a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
class a bit easier	0.0	0.0	0.0	2.0	0.0000000000	False
bit easier and simpler	0.0	0.0	0.0	2.0	0.0000000000	False
easier and simpler today	0.0	0.0	0.0	2.0	0.0000000000	False
square equals one square	0.0	0.0	0.0	2.0	0.0000000000	False
essentially just a scaling	0.0	0.0	0.0	2.0	0.0000000000	False
minus one-half y squared	0.0	0.0	0.0	2.0	0.0000000000	False
one-half y squared times	0.0	0.0	0.0	2.0	0.0000000000	False
equal to minus one-half	0.0	0.0	0.0	2.0	0.0000000000	False
excuse me plus sign	0.0	0.0	0.0	2.0	0.0000000000	False
expresses the gaussian density	0.0	0.0	0.0	2.0	0.0000000000	False
density in the form	0.0	0.0	0.0	2.0	0.0000000000	False
exponential family distribution minus	0.0	0.0	0.0	2.0	0.0000000000	False
family distribution minus half	0.0	0.0	0.0	2.0	0.0000000000	False
right and so result	0.0	0.0	0.0	2.0	0.0000000000	False
written in the form	0.0	0.0	0.0	4.0	0.0000000000	False
normal distribution it turns	0.0	0.0	0.0	2.0	0.0000000000	False
generalization of gaussian random	0.0	0.0	0.0	2.0	0.0000000000	False
high dimension to vectors	0.0	0.0	0.0	2.0	0.0000000000	False
vectors the normal distribution	0.0	0.0	0.0	2.0	0.0000000000	False
exponential family it turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns out the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
right so the models	0.0	0.0	0.0	2.0	0.0000000000	False
ll be coin tosses	0.0	0.0	0.0	2.0	0.0000000000	False
tosses with two outcomes	0.0	0.0	0.0	2.0	0.0000000000	False
two outcomes the models	0.0	0.0	0.0	2.0	0.0000000000	False
outcomes the models outcomes	0.0	0.0	0.0	2.0	0.0000000000	False
heard of the parson	0.0	0.0	0.0	2.0	0.0000000000	False
things like the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of radioactive decays	0.0	0.0	0.0	2.0	0.0000000000	False
decays in a sample	0.0	0.0	0.0	2.0	0.0000000000	False
customers to your website	0.0	0.0	0.0	2.0	0.0000000000	False
numbers of visitors arriving	0.0	0.0	0.0	2.0	0.0000000000	False
arriving in a store	0.0	0.0	0.0	2.0	0.0000000000	False
store the parson distribution	0.0	0.0	0.0	2.0	0.0000000000	False
exponential distributions are distributions	0.0	0.0	0.0	2.0	0.0000000000	False
numbers so they re	0.0	0.0	0.0	0.0	0.0000000000	False
standing at the bus	0.0	0.0	0.0	2.0	0.0000000000	False
wait for my bus	0.0	0.0	0.0	2.0	0.0000000000	False
model that with sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of gamma distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution or exponential families	0.0	0.0	0.0	2.0	0.0000000000	False
family even more distributions	0.0	0.0	0.0	2.0	0.0000000000	False
probability distributions over probability	0.0	0.0	0.0	2.0	0.0000000000	False
distributions over probability distributions	0.0	0.0	0.0	2.0	0.0000000000	False
distributions and also things	0.0	0.0	0.0	2.0	0.0000000000	False
things like the wisha	0.0	0.0	0.0	2.0	0.0000000000	False
distribution over covariance matrices	0.0	0.0	0.0	2.0	0.0000000000	False
form of exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
set where he asks	0.0	0.0	0.0	2.0	0.0000000000	False
derive a generalized linear	0.0	0.0	0.0	4.0	0.0000000000	False
topic of having chosen	0.0	0.0	0.0	2.0	0.0000000000	False
chosen and exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
model ? so generalized	0.0	0.0	0.0	2.0	0.0000000000	False
models are often abbreviated	0.0	0.0	0.0	2.0	0.0000000000	False
choice of those functions	0.0	0.0	0.0	2.0	0.0000000000	False
exponential families with parameter	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to predict	0.0	0.0	0.0	4.0	0.0000000000	False
predict how many customers	0.0	0.0	0.0	2.0	0.0000000000	False
arrived at your website	0.0	0.0	0.0	4.0	0.0000000000	False
people  the number	0.0	0.0	0.0	2.0	0.0000000000	False
hits on your website	0.0	0.0	0.0	2.0	0.0000000000	False
website by parson distribution	0.0	0.0	0.0	2.0	0.0000000000	False
parson distribution since parson	0.0	0.0	0.0	2.0	0.0000000000	False
distribution since parson distribution	0.0	0.0	0.0	2.0	0.0000000000	False
choose the exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to output	0.0	0.0	0.0	2.0	0.0000000000	False
output the effective value	0.0	0.0	0.0	2.0	0.0000000000	False
features in the website	0.0	0.0	0.0	2.0	0.0000000000	False
ve given a set	0.0	0.0	0.0	2.0	0.0000000000	False
linked to your website	0.0	0.0	0.0	2.0	0.0000000000	False
assume that our goal	0.0	0.0	0.0	2.0	0.0000000000	False
goal in our problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem is to estimate	0.0	0.0	0.0	2.0	0.0000000000	False
estimate the expected number	0.0	0.0	0.0	2.0	0.0000000000	False
expected number of people	0.0	0.0	0.0	2.0	0.0000000000	False
people that will arrive	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms hypothesis to output	0.0	0.0	0.0	2.0	0.0000000000	False
output the expected value	0.0	0.0	0.0	4.0	0.0000000000	False
last one i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
choice which is assume	0.0	0.0	0.0	2.0	0.0000000000	False
assume that the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
family with some parameter	0.0	0.0	0.0	2.0	0.0000000000	False
day will be parson	0.0	0.0	0.0	2.0	0.0000000000	False
parson or some parameter	0.0	0.0	0.0	2.0	0.0000000000	False
relationship between my input	0.0	0.0	0.0	2.0	0.0000000000	False
teachers and this parameter	0.0	0.0	0.0	2.0	0.0000000000	False
parameter parameterizing my parson	0.0	0.0	0.0	2.0	0.0000000000	False
parameterizing my parson distribution	0.0	0.0	0.0	2.0	0.0000000000	False
make this design choice	0.0	0.0	0.0	2.0	0.0000000000	False
linear model of machinery	0.0	0.0	0.0	2.0	0.0000000000	False
nice algorithms for fitting	0.0	0.0	0.0	2.0	0.0000000000	False
fitting say parson regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression models or performed	0.0	0.0	0.0	2.0	0.0000000000	False
models or performed regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression with a gamma	0.0	0.0	0.0	2.0	0.0000000000	False
outputs or exponential distribution	0.0	0.0	0.0	2.0	0.0000000000	False
theta transpose x works	0.0	0.0	0.0	2.0	0.0000000000	False
works for the case	0.0	0.0	0.0	2.0	0.0000000000	False
real number all right	0.0	0.0	0.0	2.0	0.0000000000	False
family with natural parameter	0.0	0.0	0.0	2.0	0.0000000000	False
first example we worked	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm will make	0.0	0.0	0.0	2.0	0.0000000000	False
make  will sort	0.0	0.0	0.0	2.0	0.0000000000	False
watch our learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to output	0.0	0.0	0.0	4.0	0.0000000000	False
equal to the probability	0.0	0.0	0.0	4.0	0.0000000000	False
parameter of my distribution	0.0	0.0	0.0	2.0	0.0000000000	False
probability of my distribution	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution in the form	0.0	0.0	0.0	2.0	0.0000000000	False
out what the relationship	0.0	0.0	0.0	2.0	0.0000000000	False
relationship was between phi	0.0	0.0	0.0	2.0	0.0000000000	False
worked out the relationship	0.0	0.0	0.0	2.0	0.0000000000	False
relationship between the expected	0.0	0.0	0.0	2.0	0.0000000000	False
made the design choice	0.0	0.0	0.0	2.0	0.0000000000	False
assumption that and theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta are linearly related	0.0	0.0	0.0	2.0	0.0000000000	False
variable y that takes	0.0	0.0	0.0	2.0	0.0000000000	False
takes on two values	0.0	0.0	0.0	4.0	0.0000000000	False
make sense ? raise	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	0.0	2.0	0.0000000000	False
cool so i hope	0.0	0.0	0.0	2.0	0.0000000000	False
sort of the power	0.0	0.0	0.0	2.0	0.0000000000	False
make is i chose	0.0	0.0	0.0	2.0	0.0000000000	False
assume y is distributed	0.0	0.0	0.0	2.0	0.0000000000	False
choose a different distribution	0.0	0.0	0.0	2.0	0.0000000000	False
choose y as parson	0.0	0.0	0.0	2.0	0.0000000000	False
follow a similar process	0.0	0.0	0.0	2.0	0.0000000000	False
model and different learning	0.0	0.0	0.0	2.0	0.0000000000	False
model for whatever learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm you re	0.0	0.0	0.0	0.0	0.0000000000	False
algorithm you re faced	0.0	0.0	0.0	0.0	0.0000000000	False
faced with this tiny	0.0	0.0	0.0	2.0	0.0000000000	False
function g that relates	0.0	0.0	0.0	2.0	0.0000000000	False
relates the natural parameter	0.0	0.0	0.0	2.0	0.0000000000	False
parameter to the expected	0.0	0.0	0.0	2.0	0.0000000000	False
function and g inverse	0.0	0.0	0.0	2.0	0.0000000000	False
nt a huge deal	0.0	0.0	0.0	0.0	0.0000000000	False
nt use this terminology	0.0	0.0	0.0	0.0	0.0000000000	False
mentioning those in case	0.0	0.0	0.0	2.0	0.0000000000	False
hear about  people	0.0	0.0	0.0	2.0	0.0000000000	False
people talk about generalized	0.0	0.0	0.0	2.0	0.0000000000	False
talk about canonical response	0.0	0.0	0.0	2.0	0.0000000000	False
functions or canonical link	0.0	0.0	0.0	2.0	0.0000000000	False
consistent with other algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms in machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
nt use the terms	0.0	0.0	0.0	0.0	0.0000000000	False
terms canonical response functions	0.0	0.0	0.0	2.0	0.0000000000	False
functions and canonical link	0.0	0.0	0.0	2.0	0.0000000000	False
link functions in lecture	0.0	0.0	0.0	2.0	0.0000000000	False
big on memorizing lots	0.0	0.0	0.0	2.0	0.0000000000	False
memorizing lots of names	0.0	0.0	0.0	2.0	0.0000000000	False
out there in case	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian distribution and end	0.0	0.0	0.0	2.0	0.0000000000	False
squares model the problem	0.0	0.0	0.0	2.0	0.0000000000	False
confusing than the model	0.0	0.0	0.0	2.0	0.0000000000	False
skip that and leave	0.0	0.0	0.0	2.0	0.0000000000	False
leave you to read	0.0	0.0	0.0	4.0	0.0000000000	False
likelihood of your training	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the parameters	0.0	0.0	0.0	4.0	0.0000000000	False
parameters does that make	0.0	0.0	0.0	2.0	0.0000000000	False
theta is exactly maximum	0.0	0.0	0.0	2.0	0.0000000000	False
method or gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
model that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
examples of generalized linear	0.0	0.0	0.0	2.0	0.0000000000	False
give you the gist	0.0	0.0	0.0	2.0	0.0000000000	False
skip or details omitted	0.0	0.0	0.0	2.0	0.0000000000	False
carefully in the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
outcomes imagine you re	0.0	0.0	0.0	0.0	0.0000000000	False
problem where the value	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to classify emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails into or predicting	0.0	0.0	0.0	2.0	0.0000000000	False
predicting if the patient	0.0	0.0	0.0	2.0	0.0000000000	False
lots of multi-cause classification	0.0	0.0	0.0	2.0	0.0000000000	False
two causes you model	0.0	0.0	0.0	2.0	0.0000000000	False
set and you find	0.0	0.0	0.0	2.0	0.0000000000	False
find a decision boundary	0.0	0.0	0.0	2.0	0.0000000000	False
decision boundary that separates	0.0	0.0	0.0	2.0	0.0000000000	False
re going to entertain	0.0	0.0	0.0	2.0	0.0000000000	False
taking on multiple values	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm will learn	0.0	0.0	0.0	2.0	0.0000000000	False
write in the form	0.0	0.0	0.0	2.0	0.0000000000	False
distribution so the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
derive the last parameter	0.0	0.0	0.0	2.0	0.0000000000	False
nt think of phi	0.0	0.0	0.0	0.0	0.0000000000	False
rest of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
vector of all zeros	0.0	0.0	0.0	2.0	0.0000000000	False
good point to introduce	0.0	0.0	0.0	2.0	0.0000000000	False
write a true statement	0.0	0.0	0.0	2.0	0.0000000000	False
indicator of that statement	0.0	0.0	0.0	2.0	0.0000000000	False
write a false statement	0.0	0.0	0.0	2.0	0.0000000000	False
value of this indicator	0.0	0.0	0.0	2.0	0.0000000000	False
write indicator two equals	0.0	0.0	0.0	2.0	0.0000000000	False
indicator plus one equals	0.0	0.0	0.0	2.0	0.0000000000	False
indicator of the statement	0.0	0.0	0.0	2.0	0.0000000000	False
notation for indicating sort	0.0	0.0	0.0	2.0	0.0000000000	False
indicating sort of truth	0.0	0.0	0.0	2.0	0.0000000000	False
falsehood of the statement	0.0	0.0	0.0	2.0	0.0000000000	False
carve out a bit	0.0	0.0	0.0	2.0	0.0000000000	False
element of the vector	0.0	0.0	4.99732477261	10.0	0.2880354505	False
make sure you understand	0.0	0.0	0.0	4.0	0.0000000000	False
understand all that notation	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this equation	0.0	0.0	0.0	2.0	0.0000000000	False
equal to this vector	0.0	0.0	0.0	2.0	0.0000000000	False
element of this vector	0.0	0.0	2.99839486356	6.0	0.0000000000	False
rest of the elements	0.0	0.0	0.0	2.0	0.0000000000	False
write out the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
equals one times phi	0.0	0.0	0.0	4.0	0.0000000000	False
times phi two indicator	0.0	0.0	0.0	2.0	0.0000000000	False
two indicator y equals	0.0	0.0	0.0	2.0	0.0000000000	False
phi k times indicator	0.0	0.0	0.0	2.0	0.0000000000	False
times indicator y equals	0.0	0.0	0.0	2.0	0.0000000000	False
shorthand for one minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus phi one minus	0.0	0.0	0.0	2.0	0.0000000000	False
phi one minus phi	0.0	0.0	0.0	2.0	0.0000000000	False
minus phi two minus	0.0	0.0	0.0	2.0	0.0000000000	False
two minus the rest	0.0	0.0	0.0	2.0	0.0000000000	False
equation on the left	0.0	0.0	0.0	2.0	0.0000000000	False
write this as phi	0.0	0.0	0.0	2.0	0.0000000000	False
dot phi k minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus one times phi	0.0	0.0	0.0	2.0	0.0000000000	False
out  it takes	0.0	0.0	0.0	2.0	0.0000000000	False
out in a form	0.0	0.0	0.0	2.0	0.0000000000	False
family distribution it turns	0.0	0.0	0.0	2.0	0.0000000000	False
inverted that to write	0.0	0.0	0.0	2.0	0.0000000000	False
phi as a function	0.0	0.0	0.0	4.0	0.0000000000	False
defines as a function	0.0	0.0	0.0	4.0	0.0000000000	False
relationship between and phi	0.0	0.0	0.0	2.0	0.0000000000	False
equal to  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
function of the phi	0.0	0.0	0.0	2.0	0.0000000000	False
function of the axis	0.0	0.0	0.0	2.0	0.0000000000	False
sum over j equals	0.0	0.0	0.0	2.0	0.0000000000	False
fact that i equals	0.0	0.0	0.0	2.0	0.0000000000	False
choice from generalized linear	0.0	0.0	0.0	2.0	0.0000000000	False
models so we re	0.0	0.0	0.0	0.0	0.0000000000	False
minus one all right	0.0	0.0	0.0	2.0	0.0000000000	False
value of this vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector of indicator functions	0.0	0.0	0.0	2.0	0.0000000000	False
indicator functions the expected	0.0	0.0	0.0	2.0	0.0000000000	False
functions the expected value	0.0	0.0	0.0	2.0	0.0000000000	False
expected value of indicator	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm will output	0.0	0.0	0.0	2.0	0.0000000000	False
parameterized by these functions	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm is called softmax	0.0	0.0	0.0	2.0	0.0000000000	False
generalization of logistic regression	0.0	0.0	0.0	4.0	0.0000000000	False
regression of two classes	0.0	0.0	0.0	2.0	0.0000000000	False
classes is widely thought	0.0	0.0	0.0	2.0	0.0000000000	False
regression to the case	0.0	0.0	0.0	2.0	0.0000000000	False
case of k classes	0.0	0.0	0.0	2.0	0.0000000000	False
family then you sort	0.0	0.0	0.0	2.0	0.0000000000	False
choice of using distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution as your choice	0.0	0.0	0.0	2.0	0.0000000000	False
choice of exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
re doing the training	0.0	0.0	0.0	2.0	0.0000000000	False
training set we re	0.0	0.0	0.0	0.0	0.0000000000	False
re now the value	0.0	0.0	0.0	2.0	0.0000000000	False
value of y takes	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the model	0.0	0.0	0.0	2.0	0.0000000000	False
model by maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood so you write	0.0	0.0	0.0	2.0	0.0000000000	False
write down the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
product of your training	0.0	0.0	0.0	4.0	0.0000000000	False
phi two of indicator	0.0	0.0	0.0	2.0	0.0000000000	False
phi k of indicator	0.0	0.0	0.0	2.0	0.0000000000	False
theta through this formula	0.0	0.0	0.0	2.0	0.0000000000	False
shorthand for this formula	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of this formula	0.0	0.0	0.0	2.0	0.0000000000	False
apply say gradient ascent	0.0	0.0	0.0	2.0	0.0000000000	False
gradient ascent to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
theta one through theta	0.0	0.0	0.0	2.0	0.0000000000	False
minus one i ve	0.0	0.0	0.0	0.0	0.0000000000	False
set of parameters comprising	0.0	0.0	0.0	2.0	0.0000000000	False
parameters comprising k minus	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of k minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus one parameter vectors	0.0	0.0	0.0	2.0	0.0000000000	False
answer that as sort	0.0	0.0	0.0	2.0	0.0000000000	False
feature  yeah sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of similar interpretation	0.0	0.0	0.0	2.0	0.0000000000	False
running a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
close for the day	0.0	0.0	0.0	2.0	0.0000000000	False
quick announcement and reminder	0.0	0.0	0.0	2.0	0.0000000000	False
guidelines handout was posted	0.0	0.0	0.0	2.0	0.0000000000	False
downloaded it and looked	0.0	0.0	0.0	2.0	0.0000000000	False
guidelines for the project	0.0	0.0	0.0	2.0	0.0000000000	False
proposal and the project	0.0	0.0	0.0	2.0	0.0000000000	False
type of learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	True
talk about generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm called gaussian discriminant	0.0	0.0	0.0	4.0	0.0000000000	False
analysis take a slight	0.0	0.0	0.0	2.0	0.0000000000	False
briefly discuss generative versus	0.0	0.0	0.0	2.0	0.0000000000	False
generative versus discriminative learning	0.0	0.0	0.0	2.0	0.0000000000	False
versus discriminative learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
lecture with a discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion of naive bayes	0.0	0.0	0.0	2.0	0.0000000000	False
bayes and the laplace	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
source of classification algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
classification algorithms we ve	0.0	0.0	0.0	0.0	0.0000000000	False
re given a training	0.0	0.0	0.0	2.0	0.0000000000	False
progression on those training	0.0	0.0	0.0	2.0	0.0000000000	False
find a straight line	0.0	0.0	2.99850224663	6.0	0.0000000000	False
straight line to divide	0.0	0.0	0.0	2.0	0.0000000000	False
days a bit noisier	0.0	0.0	0.0	2.0	0.0000000000	False
noisier trying to find	0.0	0.0	0.0	2.0	0.0000000000	False
straight line that separates	0.0	0.0	0.0	2.0	0.0000000000	False
set with logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
ve initialized the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
shown in the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
iteration and creating descent	0.0	0.0	0.0	2.0	0.0000000000	False
line changes a bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit after two iterations	0.0	0.0	0.0	2.0	0.0000000000	False
converges and has found	0.0	0.0	0.0	2.0	0.0000000000	False
found the straight line	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative class	0.0	0.0	0.0	2.0	0.0000000000	False
searching for a line	0.0	0.0	0.0	2.0	0.0000000000	False
talk about an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
classify the team malignant	0.0	0.0	0.0	2.0	0.0000000000	False
malignant cancer and benign	0.0	0.0	0.0	2.0	0.0000000000	False
cancer and benign cancer	0.0	0.0	0.0	2.0	0.0000000000	False
meaning a harmless cancer	0.0	0.0	0.0	2.0	0.0000000000	False
find the straight line	0.0	0.0	0.0	2.0	0.0000000000	False
separate the two classes	0.0	0.0	0.0	2.0	0.0000000000	False
cases of malignant cancers	0.0	0.0	0.0	2.0	0.0000000000	False
examples of malignant cancers	0.0	0.0	0.0	2.0	0.0000000000	False
examples of benign cancers	0.0	0.0	0.0	2.0	0.0000000000	False
ll build a model	0.0	0.0	0.99850224663	6.0	0.0000000000	False
model for what benign	0.0	0.0	0.0	2.0	0.0000000000	False
decide is this cancer	0.0	0.0	0.0	2.0	0.0000000000	False
cancer malignant or benign	0.0	0.0	0.0	2.0	0.0000000000	False
model of malignant cancers	0.0	0.0	0.0	2.0	0.0000000000	False
model of benign cancers	0.0	0.0	0.0	2.0	0.0000000000	False
depending on which model	0.0	0.0	0.0	2.0	0.0000000000	False
methods where you build	0.0	0.0	0.0	2.0	0.0000000000	False
build a second model	0.0	0.0	0.0	2.0	0.0000000000	False
model for malignant cancers	0.0	0.0	0.0	2.0	0.0000000000	False
separate model for benign	0.0	0.0	0.0	2.0	0.0000000000	False
model for benign cancers	0.0	0.0	0.0	2.0	0.0000000000	False
models that we ve	0.0	0.0	0.0	0.0	0.0000000000	False
hypothesis that outputs value	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm of models	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm of models pfx	0.0	0.0	0.0	2.0	0.0000000000	False
probability of the features	0.0	0.0	0.0	2.0	0.0000000000	False
features given the class	0.0	0.0	0.0	2.0	0.0000000000	False
builds a probabilistic model	0.0	0.0	0.0	2.0	0.0000000000	False
conditioned on the class	0.0	0.0	0.0	2.0	0.0000000000	False
features of the cancer	0.0	0.0	0.0	2.0	0.0000000000	False
model  having built	0.0	0.0	0.0	2.0	0.0000000000	False
model  generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
generative learning algorithm starts	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm starts in modeling	0.0	0.0	0.0	2.0	0.0000000000	False
starts in modeling pfx	0.0	0.0	0.0	2.0	0.0000000000	False
discriminative model a bit	0.0	0.0	0.0	2.0	0.0000000000	False
assume that your input	0.0	0.0	0.0	2.0	0.0000000000	False
re going to assume	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis model	0.0	0.0	4.99700449326	12.0	0.5308641975	False
model of that pfx	0.0	0.0	0.0	2.0	0.0000000000	False
ll be a refresher	0.0	0.0	0.0	2.0	0.0000000000	False
variable z is distributed	0.0	0.0	0.0	2.0	0.0000000000	False
formula for the density	0.0	0.0	0.0	4.0	0.0000000000	False
density as a generalization	0.0	0.0	0.0	2.0	0.0000000000	False
high dimension vector value	0.0	0.0	0.0	2.0	0.0000000000	False
dimension vector value random	0.0	0.0	0.0	2.0	0.0000000000	False
density you rarely end	0.0	0.0	0.0	2.0	0.0000000000	False
rarely end up needing	0.0	0.0	0.0	2.0	0.0000000000	False
quantities are this vector	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian and this matrix	0.0	0.0	0.0	2.0	0.0000000000	False
sigma is the covariance	0.0	0.0	0.0	2.0	0.0000000000	False
covariance matrix  covariance	0.0	0.0	0.0	2.0	0.0000000000	False
covariance of a vector	0.0	0.0	0.0	2.0	0.0000000000	False
re-watch the discussion section	0.0	0.0	0.0	2.0	0.0000000000	False
section that the tas	0.0	0.0	0.0	2.0	0.0000000000	False
tas held last friday	0.0	0.0	0.0	2.0	0.0000000000	False
holding later this week	0.0	0.0	0.0	2.0	0.0000000000	False
multi-grade gaussians is parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of a gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian with covariance matrix	0.0	0.0	0.0	2.0	0.0000000000	False
matrix equals the identity	0.0	0.0	0.0	2.0	0.0000000000	False
identity the covariance matrix	0.0	0.0	0.0	2.0	0.0000000000	False
covariance matrix is shown	0.0	0.0	0.0	2.0	0.0000000000	False
curve in two dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
shrink the covariance matrix	0.0	0.0	0.0	4.0	0.0000000000	False
diagonals of a covariance	0.0	0.0	0.0	2.0	0.0000000000	False
make the variables correlated	0.0	0.0	0.0	2.0	0.0000000000	False
show the same thing	0.0	0.0	0.0	2.0	0.0000000000	False
standard normal of distribution	0.0	0.0	0.0	2.0	0.0000000000	False
increase the off diagonals	0.0	0.0	0.0	2.0	0.0000000000	False
density with negative covariances	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian with negative entries	0.0	0.0	0.0	2.0	0.0000000000	False
entries on the diagonals	0.0	0.0	0.0	4.0	0.0000000000	False
changed the mean parameter	0.0	0.0	0.0	2.0	0.0000000000	False
primer on what gaussians	0.0	0.0	0.0	2.0	0.0000000000	False
described the gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis algorithm	0.0	0.0	0.0	4.0	0.0000000000	True
fit a gaussian distribution	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian densities will define	0.0	0.0	0.0	2.0	0.0000000000	False
out that the separator	0.0	0.0	0.0	2.0	0.0000000000	False
bound to be shown	0.0	0.0	0.0	2.0	0.0000000000	False
switch back to chalkboard	0.0	0.0	0.0	2.0	0.0000000000	False
put into model pfy	0.0	0.0	0.0	2.0	0.0000000000	False
pfy as a bernoulli	0.0	0.0	0.0	2.0	0.0000000000	False
parameterized by parameter phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi ; you ve	0.0	0.0	0.0	0.0	0.0000000000	False
excuse me i thought	0.0	0.0	0.0	2.0	0.0000000000	False
thought this looked strange	0.0	0.0	0.0	2.0	0.0000000000	False
determined in a sigma	0.0	0.0	0.0	2.0	0.0000000000	False
one-half of the denominator	0.0	0.0	0.0	2.0	0.0000000000	False
right i was listing	0.0	0.0	0.0	2.0	0.0000000000	False
sigma to the determining	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian with mean mew0	0.0	0.0	0.0	2.0	0.0000000000	False
mew0 and covariance sigma	0.0	0.0	0.0	2.0	0.0000000000	False
sigma to the sigma	0.0	0.0	0.0	2.0	0.0000000000	False
sigma to the minus	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of this model	0.0	0.0	0.0	2.0	0.0000000000	False
write down the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the parameters	0.0	0.0	3.99700449326	12.0	0.4154589372	False
parameters as the log	0.0	0.0	0.0	4.0	0.0000000000	False
write down the log	0.0	0.0	0.0	2.0	0.0000000000	False
probative probabilities of pfxi	0.0	0.0	0.0	2.0	0.0000000000	False
previously when we re	0.0	0.0	0.0	0.0	0.0000000000	False
talking about logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
log of a product	0.0	0.0	0.0	2.0	0.0000000000	False
parameterized by a theater	0.0	0.0	0.0	4.0	0.0000000000	False
back where we re	0.0	0.0	0.0	0.0	0.0000000000	False
re fitting logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
fitting logistic regression models	0.0	0.0	0.0	2.0	0.0000000000	False
regression models or generalized	0.0	0.0	0.0	2.0	0.0000000000	False
models or generalized learning	0.0	0.0	0.0	2.0	0.0000000000	False
re always modeling pfyi	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood which is pfxi	0.0	0.0	0.0	2.0	0.0000000000	False
analysis model to fit	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the model	0.0	0.0	5.99800299551	8.0	0.2935153584	False
ll do maximize likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
respect to the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
find the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimate of parameters	0.0	0.0	0.0	2.0	0.0000000000	False
practice for indicating notation	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimate for phi	0.0	0.0	3.99800299551	8.0	0.5243902439	False
phi would be sum	0.0	0.0	0.0	2.0	0.0000000000	False
written alternatively as sum	0.0	0.0	0.0	2.0	0.0000000000	False
training examples of indicator	0.0	0.0	0.0	2.0	0.0000000000	False
faction of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
training examples with label	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimate for mew0	0.0	0.0	0.0	2.0	0.0000000000	False
sum of your training	0.0	0.0	0.0	2.0	0.0000000000	False
means that you re	0.0	0.0	0.0	0.0	0.0000000000	False
including only the times	0.0	0.0	0.0	2.0	0.0000000000	False
examples where the class	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	0.0	2.0	0.0000000000	False
find all the examples	0.0	0.0	0.0	2.0	0.0000000000	False
average of the value	0.0	0.0	0.0	2.0	0.0000000000	False
notation divide the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
divide the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimate for sigma	0.0	0.0	0.0	2.0	0.0000000000	False
fit the parameters find	0.0	0.0	0.0	2.0	0.0000000000	False
sigma to your data	0.0	0.0	0.0	2.0	0.0000000000	False
write semicolon the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
pfx does nt depend	0.0	0.0	0.0	0.0	0.0000000000	False
uniform in other words	0.0	0.0	0.0	2.0	0.0000000000	False
takes the same value	0.0	0.0	0.0	2.0	0.0000000000	False
value for all values	0.0	0.0	0.0	2.0	0.0000000000	False
formula where you compute	0.0	0.0	0.0	2.0	0.0000000000	False
pfy using your model	0.0	0.0	0.0	2.0	0.0000000000	False
min of  arcomatics	0.0	0.0	0.0	2.0	0.0000000000	False
arcomatics means the value	0.0	0.0	0.0	2.0	0.0000000000	False
loose here i meant	0.0	0.0	0.0	2.0	0.0000000000	False
distribution over the set	0.0	0.0	0.0	2.0	0.0000000000	False
turns out gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
out gaussian discriminant analysis	0.0	0.0	0.0	2.0	0.0000000000	False
relationship to logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression let me illustrate	0.0	0.0	0.0	2.0	0.0000000000	False
draw 1d training set	0.0	0.0	0.0	2.0	0.0000000000	False
run gaussian discriminate analysis	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative training	0.0	0.0	0.0	2.0	0.0000000000	False
examples i will fit	0.0	0.0	0.0	2.0	0.0000000000	False
top of this plot	0.0	0.0	0.0	2.0	0.0000000000	False
examples it just makes	0.0	0.0	0.0	2.0	0.0000000000	False
equal to one conditioned	0.0	0.0	0.0	2.0	0.0000000000	False
gaussians and my phi	0.0	0.0	0.0	2.0	0.0000000000	False
belongs to the left	0.0	0.0	0.0	2.0	0.0000000000	False
study a different value	0.0	0.0	0.0	2.0	0.0000000000	False
ll be pretty small	0.0	0.0	0.0	2.0	0.0000000000	False
densities have equal value	0.0	0.0	0.0	2.0	0.0000000000	False
shown by the arrow	0.0	0.0	0.0	2.0	0.0000000000	False
fill in a bunch	0.0	0.0	0.0	2.0	0.0000000000	False
belongs to this rightmost	0.0	0.0	0.0	2.0	0.0000000000	False
exercise for a bunch	0.0	0.0	0.0	2.0	0.0000000000	False
connect up these points	0.0	0.0	0.0	2.0	0.0000000000	False
find that the curve	0.0	0.0	0.0	2.0	0.0000000000	False
plotted takes a form	0.0	0.0	0.0	2.0	0.0000000000	False
form of sigmoid function	0.0	0.0	0.0	2.0	0.0000000000	False
function that we re	0.0	0.0	0.0	0.0	0.0000000000	False
out the key difference	0.0	0.0	0.0	2.0	0.0000000000	False
discriminant analysis will end	0.0	0.0	0.0	2.0	0.0000000000	False
choosing a different position	0.0	0.0	0.0	2.0	0.0000000000	False
position and a steepness	0.0	0.0	0.0	2.0	0.0000000000	False
sigmoid than would logistic	0.0	0.0	0.0	2.0	0.0000000000	False
drawing all the dots	0.0	0.0	0.0	2.0	0.0000000000	False
out where to draw	0.0	0.0	0.0	2.0	0.0000000000	False
right ? the steps	0.0	0.0	0.0	2.0	0.0000000000	False
fit a gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
fit a bernoulli distribution	0.0	0.0	0.0	2.0	0.0000000000	False
bernoulli distribution to pfy	0.0	0.0	0.0	2.0	0.0000000000	False
pfy to my data	0.0	0.0	0.0	2.0	0.0000000000	False
ve chosen my parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of find mew0	0.0	0.0	0.0	2.0	0.0000000000	False
plot all these dots	0.0	0.0	0.0	2.0	0.0000000000	False
plug them into bayes	0.0	0.0	0.0	2.0	0.0000000000	False
right here so pfx	0.0	0.0	0.0	2.0	0.0000000000	False
pfx can be written	0.0	0.0	0.0	2.0	0.0000000000	False
talk a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit about the advantages	0.0	0.0	0.0	2.0	0.0000000000	False
case of gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
assume that x conditions	0.0	0.0	0.0	2.0	0.0000000000	False
implies that the posterior	0.0	0.0	0.0	2.0	0.0000000000	False
distribution or the form	0.0	0.0	0.0	2.0	0.0000000000	False
turns out this implication	0.0	0.0	0.0	2.0	0.0000000000	False
direction does not hold	0.0	0.0	0.0	2.0	0.0000000000	False
hessian with parameter lambda	0.0	0.0	0.0	4.0	0.0000000000	False
out if you assumed	0.0	0.0	0.0	2.0	0.0000000000	False
assumption than the assumption	0.0	0.0	0.0	2.0	0.0000000000	False
right ? that means	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian but not vice	0.0	0.0	0.0	2.0	0.0000000000	False
tradeoffs between gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
analysis and logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
right ? gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis makes	0.0	0.0	0.0	2.0	0.0000000000	False
makes a much stronger	0.0	0.0	0.0	2.0	0.0000000000	False
explicit to the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
information about the data	0.0	0.0	0.0	2.0	0.0000000000	False
holds or roughly holds	0.0	0.0	0.0	2.0	0.0000000000	False
turns out the data	0.0	0.0	0.0	2.0	0.0000000000	False
data was actually poisson	0.0	0.0	0.0	2.0	0.0000000000	False
data were actually poisson	0.0	0.0	0.0	2.0	0.0000000000	False
out that  right	0.0	0.0	0.0	2.0	0.0000000000	False
slightly different it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out the real advantage	0.0	0.0	0.0	2.0	0.0000000000	False
advantage of generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
right ? because data	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions are not met	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions about the data	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression by making	0.0	0.0	0.0	2.0	0.0000000000	False
robust to your modeling	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions because you re	0.0	0.0	0.0	0.0	0.0000000000	False
re making a weaker	0.0	0.0	0.0	2.0	0.0000000000	False
making a weaker assumption	0.0	0.0	0.0	2.0	0.0000000000	False
assumption ; you re	0.0	0.0	0.0	0.0	0.0000000000	False
re making less assumptions	0.0	0.0	0.0	2.0	0.0000000000	False
slightly larger training set	0.0	0.0	0.0	2.0	0.0000000000	False
training set to fit	0.0	0.0	0.0	2.0	0.0000000000	False
fit than gaussian discriminant	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis question	0.0	0.0	0.0	2.0	0.0000000000	False
question ? in order	0.0	0.0	0.0	2.0	0.0000000000	False
assumption about the number	0.0	0.0	0.0	2.0	0.0000000000	False
true when the number	0.0	0.0	0.0	2.0	0.0000000000	False
differently so the marving	0.0	0.0	0.0	2.0	0.0000000000	False
marving assumptions are made	0.0	0.0	0.0	2.0	0.0000000000	False
independently of the size	0.0	0.0	0.0	2.0	0.0000000000	False
size of your training	0.0	0.0	0.0	2.0	0.0000000000	False
models i m assuming	0.0	0.0	0.0	0.0	0.0000000000	False
flowing from some distribution	0.0	0.0	0.0	2.0	0.0000000000	False
giving a single training	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
back to the philosophy	0.0	0.0	0.0	2.0	0.0000000000	False
philosophy of mass molecular	0.0	0.0	0.0	2.0	0.0000000000	False
assuming that they re	0.0	0.0	0.0	0.0	0.0000000000	False
value of y generating	0.0	0.0	0.0	2.0	0.0000000000	False
generating all my data	0.0	0.0	0.0	2.0	0.0000000000	False
two values of phi	0.0	0.0	0.0	2.0	0.0000000000	False
underlying value of phi	0.0	0.0	5.99850224663	6.0	0.0000000000	False
estimate of the value	0.0	0.0	0.0	2.0	0.0000000000	False
right so maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
estimate is my attempt	0.0	0.0	0.0	2.0	0.0000000000	False
estimate the true value	0.0	0.0	0.0	2.0	0.0000000000	False
distinguish between the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
tease to the friday	0.0	0.0	0.0	2.0	0.0000000000	False
mention one more thing	0.0	0.0	0.0	2.0	0.0000000000	False
exponential family with parameter	0.0	0.0	0.0	4.0	0.0000000000	False
gamma because we ve	0.0	0.0	0.0	0.0	0.0000000000	False
ve seen gaussian right	0.0	0.0	0.0	2.0	0.0000000000	False
gamma exponential they re	0.0	0.0	0.0	0.0	0.0000000000	False
re actually a beta	0.0	0.0	0.0	2.0	0.0000000000	False
list of exponential family	0.0	0.0	0.0	2.0	0.0000000000	False
parameters than the posterior	0.0	0.0	0.0	2.0	0.0000000000	False
robustness of logistic regression	0.0	0.0	0.0	4.0	0.0000000000	False
regression to the choice	0.0	0.0	0.0	2.0	0.0000000000	False
choice of modeling assumptions	0.0	0.0	0.0	2.0	0.0000000000	False
out to be logistic	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression to modeling	0.0	0.0	0.0	2.0	0.0000000000	False
regression to modeling assumptions	0.0	0.0	0.0	2.0	0.0000000000	False
early on i promised	0.0	0.0	0.0	2.0	0.0000000000	False
pulled the logistic function	0.0	0.0	0.0	2.0	0.0000000000	False
out of the hat	0.0	0.0	0.0	2.0	0.0000000000	False
modeling assumptions also lead	0.0	0.0	0.0	2.0	0.0000000000	False
logistic then this implies	0.0	0.0	0.0	2.0	0.0000000000	False
exponential family distribution implies	0.0	0.0	0.0	2.0	0.0000000000	False
rise to logistic function	0.0	0.0	0.0	2.0	0.0000000000	False
first generative learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
spam classification all right	0.0	0.0	0.0	2.0	0.0000000000	False
build a spam classifier	0.0	0.0	0.0	2.0	0.0000000000	False
incoming stream of email	0.0	0.0	0.0	2.0	0.0000000000	False
email using a feature	0.0	0.0	0.0	2.0	0.0000000000	False
words or a list	0.0	0.0	0.0	2.0	0.0000000000	False
list of ascii characters	0.0	0.0	0.0	2.0	0.0000000000	False
email as a feature	0.0	0.0	0.0	2.0	0.0000000000	False
ll use a couple	0.0	0.0	0.0	2.0	0.0000000000	False
couple of different representations	0.0	0.0	0.0	2.0	0.0000000000	False
words in my dictionary	0.0	0.0	1.99850224663	6.0	0.0000000000	False
telling you to buy	0.0	0.0	0.0	2.0	0.0000000000	False
words via other emails	0.0	0.0	0.0	2.0	0.0000000000	False
technological chemistry that deals	0.0	0.0	0.0	2.0	0.0000000000	False
deals with the fermentation	0.0	0.0	0.0	2.0	0.0000000000	False
fermentation process in brewing	0.0	0.0	0.0	2.0	0.0000000000	False
scan through this list	0.0	0.0	0.0	2.0	0.0000000000	False
appears in my email	0.0	0.0	0.0	2.0	0.0000000000	False
email has the word	0.0	0.0	0.0	2.0	0.0000000000	False
nt have the words	0.0	0.0	0.0	0.0	0.0000000000	False
words ausworth or aardvark	0.0	0.0	0.0	2.0	0.0000000000	False
cs229 does nt occur	0.0	0.0	0.0	0.0	0.0000000000	False
creating a feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
feature vector to represent	0.0	0.0	0.0	2.0	0.0000000000	False
throw the generative model	0.0	0.0	0.0	2.0	0.0000000000	False
value vectors they re	0.0	0.0	0.0	0.0	0.0000000000	False
words in your dictionary	0.0	0.0	0.0	4.0	0.0000000000	False
50,000 possible bit vectors	0.0	0.0	0.0	2.0	0.0000000000	False
bit vectors of length	0.0	0.0	0.0	2.0	0.0000000000	False
two to 50,000 possibilities	0.0	0.0	0.0	4.0	0.0000000000	False
re going to make	0.0	0.0	0.0	2.0	0.0000000000	False
strong assumption on pfx	0.0	0.0	0.0	2.0	0.0000000000	False
out what it means	0.0	0.0	0.0	2.0	0.0000000000	False
key rule of probability	0.0	0.0	0.0	2.0	0.0000000000	False
pfx1 given y times	0.0	0.0	0.0	2.0	0.0000000000	False
ll just put dot	0.0	0.0	0.0	2.0	0.0000000000	False
chain were of probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability this always holds	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na meet	0.0	0.0	0.0	2.0	0.0000000000	False
assumption that x defies	0.0	0.0	0.0	2.0	0.0000000000	False
assume that that term	0.0	0.0	0.0	2.0	0.0000000000	False
means assume that pfx1	0.0	0.0	0.0	2.0	0.0000000000	False
informally what this means	0.0	0.0	0.0	2.0	0.0000000000	False
spam or not spam	0.0	0.0	8.99650524214	14.0	0.3355629877	False
knowing whether the word	0.0	0.0	0.0	2.0	0.0000000000	False
email does not affect	0.0	0.0	0.0	2.0	0.0000000000	False
appears in the email	0.0	0.0	4.99800299551	8.0	0.2935153584	False
knowing whether other words	0.0	0.0	0.0	2.0	0.0000000000	False
nt help you predict	0.0	0.0	0.0	0.0	0.0000000000	False
right ? this assumption	0.0	0.0	0.0	2.0	0.0000000000	False
nt possibly be true	0.0	0.0	0.0	0.0	0.0000000000	False
cs229 in an email	0.0	0.0	0.0	2.0	0.0000000000	False
effective algorithm for classifying	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for classifying text	0.0	0.0	0.0	2.0	0.0000000000	False
text documents into spam	0.0	0.0	0.0	2.0	0.0000000000	False
emails into different emails	0.0	0.0	0.0	2.0	0.0000000000	False
web pages and classifying	0.0	0.0	0.0	2.0	0.0000000000	False
classifying whether this webpage	0.0	0.0	0.0	2.0	0.0000000000	False
ll talk a bit	0.0	0.0	0.0	2.0	0.0000000000	False
digression that ll make	0.0	0.0	0.0	0.0	0.0000000000	False
important to our purposes	0.0	0.0	0.0	2.0	0.0000000000	False
find that the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of the parameters	0.0	0.0	5.99850224663	6.0	0.0000000000	False
expect maximum likelihood estimate	0.0	0.0	0.0	2.0	0.0000000000	False
count up the number	0.0	0.0	0.0	2.0	0.0000000000	False
times you saw word	0.0	0.0	0.0	2.0	0.0000000000	False
spam emails and count	0.0	0.0	0.0	2.0	0.0000000000	False
jay  appeared out	0.0	0.0	0.0	2.0	0.0000000000	False
number of spam emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails in your training	0.0	0.0	0.0	2.0	0.0000000000	False
fraction of these emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails did the word	0.0	0.0	0.0	2.0	0.0000000000	False
jay  you wrote	0.0	0.0	0.0	2.0	0.0000000000	False
wrote in your dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for the probability	0.0	0.0	0.0	4.0	0.0000000000	False
conditions on the piece	0.0	0.0	0.0	2.0	0.0000000000	False
similar to your maximum	0.0	0.0	0.0	2.0	0.0000000000	False
estimated all these parameters	0.0	0.0	0.0	2.0	0.0000000000	False
elaboration to this idea	0.0	0.0	0.0	2.0	0.0000000000	False
depend on the number	0.0	0.0	0.0	4.0	0.0000000000	False
number of training examples	0.0	0.0	5.99850224663	6.0	0.0000000000	False
formula for the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
compute the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimates is training	0.0	0.0	0.0	2.0	0.0000000000	False
estimates is training examples	0.0	0.0	0.0	2.0	0.0000000000	False
two months and label	0.0	0.0	0.0	2.0	0.0000000000	False
label them as spam	0.0	0.0	0.0	2.0	0.0000000000	False
emails labeled as spam	0.0	0.0	0.0	2.0	0.0000000000	False
comprise your training sets	0.0	0.0	0.0	2.0	0.0000000000	False
vectors representing which words	0.0	0.0	0.0	2.0	0.0000000000	False
representing which words appeared	0.0	0.0	0.0	2.0	0.0000000000	False
model does nt depend	0.0	0.0	0.0	0.0	0.0000000000	False
depend on the models	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions are nt made	0.0	0.0	0.0	0.0	0.0000000000	False
making the naive bayes	0.0	0.0	0.0	2.0	0.0000000000	False
model is an assumption	0.0	0.0	0.0	2.0	0.0000000000	False
fixed number of training	0.0	0.0	0.0	2.0	0.0000000000	False
write the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
common way to count	0.0	0.0	0.0	2.0	0.0000000000	False
times in your training	0.0	0.0	0.0	2.0	0.0000000000	False
training set so words	0.0	0.0	0.0	2.0	0.0000000000	False
times in the emails	0.0	0.0	0.0	2.0	0.0000000000	False
nice way of thinking	0.0	0.0	0.0	2.0	0.0000000000	False
creating your feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
compute all those parameters	0.0	0.0	0.0	2.0	0.0000000000	False
generous of learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
pfy will be parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
changing bullets my model	0.0	0.0	0.0	2.0	0.0000000000	False
product of these probabilities	0.0	0.0	0.0	2.0	0.0000000000	False
probability of each word	0.0	0.0	0.0	2.0	0.0000000000	False
occurring or not occurring	0.0	0.0	0.0	2.0	0.0000000000	False
conditions on the email	0.0	0.0	0.0	2.0	0.0000000000	False
defined the feature vectors	0.0	0.0	0.0	4.0	0.0000000000	False
depending on whether words	0.0	0.0	0.0	2.0	0.0000000000	False
right i should move	0.0	0.0	0.0	2.0	0.0000000000	False
out that this idea	0.0	0.0	0.0	2.0	0.0000000000	False
class and you start	0.0	0.0	0.0	2.0	0.0000000000	False
working on your class	0.0	0.0	0.0	2.0	0.0000000000	False
project for a bit	0.0	0.0	0.0	2.0	0.0000000000	False
submit your class project	0.0	0.0	0.0	2.0	0.0000000000	False
project to a conference	0.0	0.0	0.0	2.0	0.0000000000	False
year is the conference	0.0	0.0	0.0	2.0	0.0000000000	False
send your project partners	0.0	0.0	0.0	2.0	0.0000000000	False
partners or senior friends	0.0	0.0	0.0	2.0	0.0000000000	False
work on a project	0.0	0.0	0.0	2.0	0.0000000000	False
re getting these emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails with the word	0.0	0.0	0.0	2.0	0.0000000000	False
paper to the nips	0.0	0.0	0.0	2.0	0.0000000000	False
classifier will say pfx	0.0	0.0	0.0	2.0	0.0000000000	False
nips is also estimated	0.0	0.0	0.0	2.0	0.0000000000	False
classifier goes to compute	0.0	0.0	0.0	2.0	0.0000000000	False
right here  pfy	0.0	0.0	0.0	2.0	0.0000000000	False
turns out the denominator	0.0	0.0	0.0	2.0	0.0000000000	False
end up with pfy	0.0	0.0	0.0	2.0	0.0000000000	False
undefined and the problem	0.0	0.0	0.0	2.0	0.0000000000	False
statistically a bad idea	0.0	0.0	0.0	2.0	0.0000000000	False
nt seen the word	0.0	0.0	0.0	0.0	0.0000000000	False
last two months worth	0.0	0.0	0.0	2.0	0.0000000000	False
months worth of email	0.0	0.0	0.0	2.0	0.0000000000	False
nips in future emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails ; the chance	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na fix	0.0	0.0	0.0	2.0	0.0000000000	False
fix i ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
losses to gather statistics	0.0	0.0	0.0	2.0	0.0000000000	False
form a betting pool	0.0	0.0	0.0	2.0	0.0000000000	False
re likely to win	0.0	0.0	0.0	2.0	0.0000000000	False
lose the next game	0.0	0.0	0.0	2.0	0.0000000000	False
last season they played	0.0	0.0	0.0	2.0	0.0000000000	False
season they played washington	0.0	0.0	0.0	2.0	0.0000000000	False
22nd they played usc	0.0	0.0	0.0	2.0	0.0000000000	False
ll win or lose	0.0	0.0	0.0	2.0	0.0000000000	False
right ? so find	0.0	0.0	0.0	2.0	0.0000000000	False
find the four guys	0.0	0.0	0.0	2.0	0.0000000000	False
four guys last year	0.0	0.0	0.0	2.0	0.0000000000	False
year or five times	0.0	0.0	0.0	2.0	0.0000000000	False
idea behind laplace smoothing	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood is the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of ones divided	0.0	0.0	0.0	2.0	0.0000000000	False
divided by the number	0.0	0.0	0.0	2.0	0.0000000000	False
zeros plus the number	0.0	0.0	0.0	2.0	0.0000000000	False
hope this informal notation	0.0	0.0	0.0	2.0	0.0000000000	False
informal notation makes sense	0.0	0.0	0.0	2.0	0.0000000000	False
knowing the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
loss for bernoulli random	0.0	0.0	0.0	2.0	0.0000000000	False
total number of examples	0.0	0.0	0.0	2.0	0.0000000000	False
laplace smoothing we re	0.0	0.0	0.0	0.0	0.0000000000	False
winning the next game	0.0	0.0	13.9965052421	14.0	0.2590361446	False
chance of their winning	0.0	0.0	0.0	2.0	0.0000000000	False
games in a row	0.0	0.0	0.0	4.0	0.0000000000	False
probability that the sun	0.0	0.0	0.0	4.0	0.0000000000	False
sun will rise tomorrow	0.0	0.0	5.99850224663	6.0	0.0000000000	False
ve seen the sun	0.0	0.0	0.0	2.0	0.0000000000	False
absolutely certain the sun	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to estimate	0.0	0.0	0.0	2.0	0.0000000000	False
estimate will be sum	0.0	0.0	0.0	2.0	0.0000000000	False
two to the denominator	0.0	0.0	0.0	2.0	0.0000000000	False
friend sends you email	0.0	0.0	0.0	2.0	0.0000000000	False
email about the nips	0.0	0.0	0.0	2.0	0.0000000000	False
make a meaningful prediction	0.0	0.0	0.0	2.0	0.0000000000	False
right ? okay shoot	0.0	0.0	0.0	2.0	0.0000000000	False
games on the right	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions that the probability	0.0	0.0	0.0	2.0	0.0000000000	False
nt have a lot	0.0	0.0	0.0	0.0	0.0000000000	False
case anywhere the learning	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for the chance	0.0	0.0	0.0	2.0	0.0000000000	False
re saying the chances	0.0	0.0	0.0	2.0	0.0000000000	False
set of bayesian assumptions	0.0	0.0	0.0	2.0	0.0000000000	False
assumptions about the prior	0.0	0.0	0.0	2.0	0.0000000000	False
prior on the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
pretty good basketball team	0.0	0.0	0.0	2.0	0.0000000000	False
chose a losing streak	0.0	0.0	0.0	2.0	0.0000000000	False
good morning welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
quick announcement for today	0.0	0.0	0.0	2.0	0.0000000000	False
program matlab or octave	0.0	0.0	0.0	2.0	0.0000000000	False
direct terms and matlab	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that i started	0.0	0.0	0.0	2.0	0.0000000000	False
previous lecture and talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about a couple	0.0	0.0	0.0	2.0	0.0000000000	False
couple of different event	0.0	0.0	0.0	2.0	0.0000000000	False
talk about neural networks	0.0	0.0	0.0	4.0	0.0000000000	False
nt spend a lot	0.0	0.0	0.0	0.0	0.0000000000	False
talk about support vector	0.0	0.0	0.0	4.0	0.0000000000	False
machines is the learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that many people	0.0	0.0	0.0	2.0	0.0000000000	False
off-the-shelf supervised learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm that point	0.0	0.0	0.0	2.0	0.0000000000	False
start discussing that today	0.0	0.0	0.0	2.0	0.0000000000	False
started off describing spam	0.0	0.0	0.0	2.0	0.0000000000	False
words in a dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
based on what words	0.0	0.0	0.0	2.0	0.0000000000	False
represented as a feature	0.0	0.0	0.0	2.0	0.0000000000	False
modeled it as product	0.0	0.0	0.0	2.0	0.0000000000	False
product from i equals	0.0	0.0	0.0	2.0	0.0000000000	False
spam or not spam	0.0	0.0	6.99791558103	8.0	0.0000000000	False
bayes rule is rfx	0.0	0.0	0.0	2.0	0.0000000000	False
attention to two things	0.0	0.0	0.0	2.0	0.0000000000	False
indicating whether different words	0.0	0.0	0.0	2.0	0.0000000000	False
length or the feature	0.0	0.0	0.0	2.0	0.0000000000	False
vector was the number	0.0	0.0	0.0	2.0	0.0000000000	False
words in the dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
version on the order	0.0	0.0	0.0	2.0	0.0000000000	False
order of 50,000 words	0.0	0.0	0.0	2.0	0.0000000000	False
variations on this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
takes on more values	0.0	0.0	0.0	2.0	0.0000000000	False
takes on k values	0.0	0.0	0.0	2.0	0.0000000000	False
similar model where pfx	0.0	0.0	0.0	2.0	0.0000000000	False
probabilities rather than bernoulli	0.0	0.0	0.0	2.0	0.0000000000	False
situation where this arises	0.0	0.0	0.0	2.0	0.0000000000	False
value feature and dispertise	0.0	0.0	0.0	2.0	0.0000000000	False
set of k values	0.0	0.0	0.0	2.0	0.0000000000	False
first supervised learning problem	0.0	0.0	0.0	2.0	0.0000000000	False
learning problem of predicting	0.0	0.0	0.0	2.0	0.0000000000	False
problem on these houses	0.0	0.0	0.0	2.0	0.0000000000	False
features of a house	0.0	0.0	0.0	2.0	0.0000000000	False
house will be sold	0.0	0.0	0.0	2.0	0.0000000000	False
feature like the living	0.0	0.0	0.0	2.0	0.0000000000	False
continuous value living area	0.0	0.0	0.0	2.0	0.0000000000	False
area and just dispertise	0.0	0.0	0.0	2.0	0.0000000000	False
area of the house	0.0	0.0	0.0	2.0	0.0000000000	False
greater than 2,000 square	0.0	0.0	0.0	2.0	0.0000000000	False
first variation or generalization	0.0	0.0	0.0	2.0	0.0000000000	False
out that in practice	0.0	0.0	0.0	2.0	0.0000000000	False
ten buckets to dispertise	0.0	0.0	0.0	2.0	0.0000000000	False
dispertise a continuous value	0.0	0.0	0.0	2.0	0.0000000000	False
value feature i drew	0.0	0.0	0.0	2.0	0.0000000000	False
bayes is a variation	0.0	0.0	0.0	2.0	0.0000000000	False
specific to classifying text	0.0	0.0	0.0	2.0	0.0000000000	False
sequences so the text	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm as i ve	0.0	0.0	0.0	0.0	0.0000000000	False
binary vector value representation	0.0	0.0	0.0	2.0	0.0000000000	False
things that this loses	0.0	0.0	0.0	2.0	0.0000000000	False
times that different words	0.0	0.0	0.0	2.0	0.0000000000	False
word appears a lot	0.0	0.0	0.0	2.0	0.0000000000	False
buy  a lot	0.0	0.0	0.0	2.0	0.0000000000	False
word viagra a ton	0.0	0.0	0.0	2.0	0.0000000000	False
times in the email	0.0	0.0	0.0	2.0	0.0000000000	False
spam than it appears	0.0	0.0	0.0	2.0	0.0000000000	False
times a word appears	0.0	0.0	0.0	4.0	0.0000000000	False
appears in the email	0.0	0.0	0.0	2.0	0.0000000000	False
give this previous model	0.0	0.0	0.0	2.0	0.0000000000	False
model for text classification	0.0	0.0	0.0	2.0	0.0000000000	False
multivariate bernoulli event model	0.0	0.0	0.0	2.0	0.0000000000	True
refers to the fact	0.0	0.0	0.0	2.0	0.0000000000	False
multiple bernoulli random variables	0.0	0.0	0.0	2.0	0.0000000000	False
describe a different representation	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the feature	0.0	0.0	0.0	2.0	0.0000000000	False
email as a feature	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the number	0.0	0.0	0.0	2.0	0.0000000000	False
words in this email	0.0	0.0	0.0	2.0	0.0000000000	False
examples is an email	0.0	0.0	0.0	2.0	0.0000000000	False
email via a feature	0.0	0.0	0.0	2.0	0.0000000000	False
elements of the feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature vector  lets	0.0	0.0	0.0	2.0	0.0000000000	False
dictionary has 50,000 words	0.0	0.0	0.0	2.0	0.0000000000	False
position in my feature	0.0	0.0	0.0	2.0	0.0000000000	False
position of my email	0.0	0.0	0.0	2.0	0.0000000000	False
words in my email	0.0	0.0	0.0	2.0	0.0000000000	False
word in my dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
dictionary was each word	0.0	0.0	0.0	2.0	0.0000000000	False
word in the email	0.0	0.0	0.0	4.0	0.0000000000	False
indexed into the dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
components of the feature	0.0	0.0	0.0	2.0	0.0000000000	False
longer binary random variables	0.0	0.0	0.0	2.0	0.0000000000	False
variables ; they re	0.0	0.0	0.0	0.0	0.0000000000	False
larger set of values	0.0	0.0	0.0	2.0	0.0000000000	False
length of the email	0.0	0.0	0.0	2.0	0.0000000000	False
formula is you imagine	0.0	0.0	0.0	2.0	0.0000000000	False
random distribution that generates	0.0	0.0	0.0	2.0	0.0000000000	False
first the class label	0.0	0.0	0.0	2.0	0.0000000000	False
email or not spam	0.0	0.0	0.0	2.0	0.0000000000	False
spam emails is chosen	0.0	0.0	0.0	2.0	0.0000000000	False
class label of spam	0.0	0.0	0.0	2.0	0.0000000000	False
positions of the email	0.0	0.0	0.0	2.0	0.0000000000	False
compose them as email	0.0	0.0	0.0	2.0	0.0000000000	False
words from some distribution	0.0	0.0	0.0	2.0	0.0000000000	False
ll send you words	0.0	0.0	0.0	2.0	0.0000000000	False
ll tend to generate	0.0	0.0	0.0	2.0	0.0000000000	False
tend to generate words	0.0	0.0	0.0	2.0	0.0000000000	False
send you not spam	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of this model	0.0	0.0	0.0	2.0	0.0000000000	False
conditioned on someone deciding	0.0	0.0	0.0	2.0	0.0000000000	False
guess  and phi	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the model	0.0	0.0	0.0	2.0	0.0000000000	False
work out the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
out the maximum likelihood	0.0	0.0	0.0	4.0	0.0000000000	False
estimates of the parameters	0.0	0.0	3.99843668577	6.0	0.0000000000	False
parameters so the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
big indicator function things	0.0	0.0	0.0	2.0	0.0000000000	False
ll be a sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum over your training	0.0	0.0	0.0	2.0	0.0000000000	False
spam times the sum	0.0	0.0	0.0	2.0	0.0000000000	False
words in that email	0.0	0.0	0.0	2.0	0.0000000000	False
email where n subscript	0.0	0.0	0.0	2.0	0.0000000000	False
account all the emails	0.0	0.0	0.0	2.0	0.0000000000	False
emails that had class	0.0	0.0	0.0	2.0	0.0000000000	False
emails that were spam	0.0	0.0	0.0	2.0	0.0000000000	False
words in your spam	0.0	0.0	0.0	2.0	0.0000000000	False
counts up the number	0.0	0.0	0.0	2.0	0.0000000000	False
emails in your training	0.0	0.0	0.0	4.0	0.0000000000	False
training set and count	0.0	0.0	0.0	2.0	0.0000000000	False
total number of times	0.0	0.0	0.0	2.0	0.0000000000	False
appeared in this email	0.0	0.0	0.0	2.0	0.0000000000	False
denominator then is sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum up the length	0.0	0.0	0.0	2.0	0.0000000000	False
length of that spam	0.0	0.0	0.0	2.0	0.0000000000	False
ratio is just out	0.0	0.0	0.0	2.0	0.0000000000	False
emails that were word	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for the probability	0.0	0.0	2.99843668577	6.0	0.0000000000	False
piece of spam mail	0.0	0.0	0.0	2.0	0.0000000000	False
mail generating the word	0.0	0.0	0.0	2.0	0.0000000000	False
talked about laplace smoothing	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of this parameter	0.0	0.0	0.0	2.0	0.0000000000	False
work out the estimates	0.0	0.0	0.0	2.0	0.0000000000	False
words of your email	0.0	0.0	0.0	2.0	0.0000000000	False
email of their probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability the ith word	0.0	0.0	0.0	2.0	0.0000000000	False
word in your email	0.0	0.0	0.0	2.0	0.0000000000	False
apologize i just realized	0.0	0.0	0.0	2.0	0.0000000000	False
right so in laplace	0.0	0.0	0.0	2.0	0.0000000000	False
words in your dictionary	0.0	0.0	0.0	2.0	0.0000000000	False
great i stole notation	0.0	0.0	0.0	2.0	0.0000000000	False
nt translate it properly	0.0	0.0	0.0	0.0	0.0000000000	False
properly so laplace smoothing	0.0	0.0	0.0	2.0	0.0000000000	False
number of possible values	0.0	0.0	0.0	4.0	0.0000000000	False
values that the random	0.0	0.0	0.0	2.0	0.0000000000	False
cool raise your hand	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	0.0	4.0	0.0000000000	False
smoothing is a method	0.0	0.0	0.0	2.0	0.0000000000	False
estimates of their probability	0.0	0.0	0.0	2.0	0.0000000000	False
definition for the random	0.0	0.0	0.0	2.0	0.0000000000	False
variable y because suppose	0.0	0.0	0.0	2.0	0.0000000000	False
variable x which takes	0.0	0.0	0.0	2.0	0.0000000000	False
observations the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
observations of x equals	0.0	0.0	0.0	2.0	0.0000000000	False
total number of observations	0.0	0.0	0.0	2.0	0.0000000000	False
estimate and to add	0.0	0.0	0.0	2.0	0.0000000000	False
probability that x equals	0.0	0.0	0.0	2.0	0.0000000000	False
50,000 is the length	0.0	0.0	0.0	2.0	0.0000000000	False
50,000 to the denominator	0.0	0.0	0.0	2.0	0.0000000000	False
definition for a maximum	0.0	0.0	0.0	2.0	0.0000000000	False
estimation of a parameter	0.0	0.0	0.0	2.0	0.0000000000	False
parameter ? we ve	0.0	0.0	0.0	0.0	0.0000000000	False
right so the definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition of maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
definition for maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
lecture when i talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about gaussian discriminant	0.0	0.0	0.0	4.0	0.0000000000	False
throwing out the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
estimates on the board	0.0	0.0	0.0	2.0	0.0000000000	False
write down the likelihood	0.0	0.0	0.0	4.0	0.0000000000	False
estimates is to write	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
log of the product	0.0	0.0	0.0	2.0	0.0000000000	False
product of i equals	0.0	0.0	0.0	2.0	0.0000000000	False
parameterized by these things	0.0	0.0	0.0	2.0	0.0000000000	False
set of fixed iyi	0.0	0.0	0.0	2.0	0.0000000000	False
maximize this in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of these parameters	0.0	0.0	0.0	2.0	0.0000000000	False
estimates that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
ve been writing out	0.0	0.0	0.0	2.0	0.0000000000	False
previous section of today	0.0	0.0	0.0	2.0	0.0000000000	False
wrote out some maximum	0.0	0.0	0.0	2.0	0.0000000000	False
out some maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis model	0.0	0.0	0.0	2.0	0.0000000000	False
maximize the log likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
derived is by maximizing	0.0	0.0	0.0	2.0	0.0000000000	False
right so that wraps	0.0	0.0	0.0	2.0	0.0000000000	False
out that for text	0.0	0.0	0.0	4.0	0.0000000000	False
bayes model i presented	0.0	0.0	0.0	2.0	0.0000000000	False
bayes model i talked	0.0	0.0	0.0	2.0	0.0000000000	False
specific of text classification	0.0	0.0	0.0	2.0	0.0000000000	False
appears in a document	0.0	0.0	0.0	2.0	0.0000000000	False
truth that actually turns	0.0	0.0	0.0	2.0	0.0000000000	False
researchers are still debating	0.0	0.0	0.0	2.0	0.0000000000	False
model is still positioning	0.0	0.0	0.0	2.0	0.0000000000	False
care where the words	0.0	0.0	0.0	2.0	0.0000000000	False
care about the ordering	0.0	0.0	0.0	2.0	0.0000000000	False
ordering of the words	0.0	0.0	0.0	4.0	0.0000000000	False
words you can shuffle	0.0	0.0	0.0	2.0	0.0000000000	False
shuffle all the words	0.0	0.0	0.0	2.0	0.0000000000	False
model in natural language	0.0	0.0	0.0	2.0	0.0000000000	False
higher order markup models	0.0	0.0	0.0	2.0	0.0000000000	False
models like the bigram	0.0	0.0	0.0	2.0	0.0000000000	False
bigram models or trigram	0.0	0.0	0.0	2.0	0.0000000000	False
models or trigram models	0.0	0.0	0.0	2.0	0.0000000000	False
applying them to text	0.0	0.0	0.0	2.0	0.0000000000	False
start again to discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion of non-linear classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
classification algorithm we talked	0.0	0.0	0.0	2.0	0.0000000000	False
forming form for hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
right ? logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
find a straight line	0.0	0.0	0.0	2.0	0.0000000000	False
line that reasonably separates	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative classes	0.0	0.0	0.0	2.0	0.0000000000	False
sorts of non-linear division	0.0	0.0	0.0	2.0	0.0000000000	False
result is that remember	0.0	0.0	0.0	2.0	0.0000000000	False
talked about generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
build a generative learning	0.0	0.0	0.0	2.0	0.0000000000	False
family with natural parameter	0.0	0.0	0.0	2.0	0.0000000000	False
posterior it actually turns	0.0	0.0	0.0	2.0	0.0000000000	False
bayes model actually falls	0.0	0.0	0.0	4.0	0.0000000000	False
talk about one method	0.0	0.0	0.0	2.0	0.0000000000	False
briefly which is taking	0.0	0.0	0.0	2.0	0.0000000000	False
taking a simpler algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm like logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
suppose you have features	0.0	0.0	0.0	2.0	0.0000000000	False
follow our earlier convention	0.0	0.0	0.0	2.0	0.0000000000	False
denote our logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
denoting a computation note	0.0	0.0	0.0	2.0	0.0000000000	False
computation note that takes	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses that can output	0.0	0.0	0.0	2.0	0.0000000000	False
output non-linear division boundaries	0.0	0.0	0.0	2.0	0.0000000000	False
pictures that i drew	0.0	0.0	0.0	2.0	0.0000000000	False
output my final output	0.0	0.0	0.0	2.0	0.0000000000	False
final output h subscript	0.0	0.0	0.0	2.0	0.0000000000	False
output h subscript theta	0.0	0.0	0.0	2.0	0.0000000000	False
give these things names	0.0	0.0	0.0	2.0	0.0000000000	False
call the values output	0.0	0.0	0.0	2.0	0.0000000000	False
units in the middle	0.0	0.0	0.0	2.0	0.0000000000	False
ll write as theta	0.0	0.0	0.0	2.0	0.0000000000	False
vector is a vector	0.0	0.0	0.0	2.0	0.0000000000	False
theta one through theta	0.0	0.0	0.0	4.0	0.0000000000	False
parameters for this model	0.0	0.0	0.0	2.0	0.0000000000	False
model is to write	0.0	0.0	0.0	2.0	0.0000000000	False
write down the cost	0.0	0.0	0.0	2.0	0.0000000000	False
theta equals one-half sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum from y equals	0.0	0.0	0.0	2.0	0.0000000000	False
minus h subscript theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta of xi squared	0.0	0.0	0.0	2.0	0.0000000000	False
familiar quadratic cost function	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
gradient interscent to minimize	0.0	0.0	0.0	2.0	0.0000000000	False
minimize j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta as a function	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent to minimize	0.0	0.0	0.0	2.0	0.0000000000	False
minimize this square area	0.0	0.0	0.0	2.0	0.0000000000	False
means you use gradient	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent to make	0.0	0.0	0.0	2.0	0.0000000000	False
observed as the labels	0.0	0.0	0.0	2.0	0.0000000000	False
labels in your training	0.0	0.0	0.0	2.0	0.0000000000	False
turns out green descent	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that implements grand	0.0	0.0	0.0	2.0	0.0000000000	False
interscent on a cost	0.0	0.0	0.0	2.0	0.0000000000	False
intermediate notes are computing	0.0	0.0	0.0	2.0	0.0000000000	False
feed into these computation	0.0	0.0	0.0	2.0	0.0000000000	False
feed into more layers	0.0	0.0	0.0	2.0	0.0000000000	False
layers of computation units	0.0	0.0	0.0	2.0	0.0000000000	False
layer at the end	0.0	0.0	0.0	2.0	0.0000000000	False
end and one cool	0.0	0.0	0.0	2.0	0.0000000000	False
network do nt worry	0.0	0.0	0.0	0.0	0.0000000000	False
computations of the hidden	0.0	0.0	0.0	2.0	0.0000000000	False
computing the neural network	0.0	0.0	0.0	2.0	0.0000000000	False
sense of neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
show you a video	0.0	0.0	3.99843668577	6.0	0.0000000000	False
switch to the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
made by a friend	0.0	0.0	0.0	2.0	0.0000000000	False
professor at new york	0.0	0.0	0.0	2.0	0.0000000000	False
university can i show	0.0	0.0	0.0	2.0	0.0000000000	False
video on the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
video from yann lecun	0.0	0.0	0.0	2.0	0.0000000000	False
network that he developed	0.0	0.0	0.0	2.0	0.0000000000	False
developed for hammerton digit	0.0	0.0	0.0	2.0	0.0000000000	False
system is called lenet	0.0	0.0	0.0	2.0	0.0000000000	False
put on the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
laptop display ? hum	0.0	0.0	0.0	2.0	0.0000000000	False
put on the screen	0.0	0.0	0.0	2.0	0.0000000000	False
screen on the side	0.0	0.0	0.0	2.0	0.0000000000	False
screen is nt working	0.0	0.0	0.0	0.0	0.0000000000	False
entertained while we re	0.0	0.0	0.0	0.0	0.0000000000	False
waiting for the video	0.0	0.0	0.0	2.0	0.0000000000	False
things about neural network	0.0	0.0	0.0	2.0	0.0000000000	False
network so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
write a quadratic cost	0.0	0.0	0.0	2.0	0.0000000000	False
function like i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
respond to non-convex optimization	0.0	0.0	0.0	2.0	0.0000000000	False
regression if you run	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent or newton	0.0	0.0	0.0	2.0	0.0000000000	False
converse the global optimer	0.0	0.0	0.0	2.0	0.0000000000	False
true for neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
lots of local optimer	0.0	0.0	0.0	2.0	0.0000000000	False
problem so neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
re good at making	0.0	0.0	0.0	2.0	0.0000000000	False
good at making design	0.0	0.0	0.0	2.0	0.0000000000	False
choices like what learning	0.0	0.0	0.0	2.0	0.0000000000	False
vast majority of machine	0.0	0.0	0.0	2.0	0.0000000000	False
majority of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
machine learning researchers today	0.0	0.0	0.0	2.0	0.0000000000	False
today seem to perceive	0.0	0.0	0.0	2.0	0.0000000000	False
perceive support vector machines	0.0	0.0	0.0	2.0	0.0000000000	False
effective off-the-shelf learning algorithm	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm than neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
neural networks this point	0.0	0.0	0.0	2.0	0.0000000000	False
personally use a lot	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm before support	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm before support vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector machines were invented	0.0	0.0	0.0	2.0	0.0000000000	False
yann lecun s video	0.0	0.0	0.0	0.0	0.0000000000	False
happening what you re	0.0	0.0	0.0	0.0	0.0000000000	False
display where my mouse	0.0	0.0	0.0	2.0	0.0000000000	False
mouse pointer is pointing	0.0	0.0	0.0	2.0	0.0000000000	False
network so you re	0.0	0.0	0.0	0.0	0.0000000000	False
showing the neural network	0.0	0.0	0.0	2.0	0.0000000000	False
neural network this image	0.0	0.0	0.0	2.0	0.0000000000	False
network is this number	0.0	0.0	0.0	2.0	0.0000000000	False
neural network correctly recognizes	0.0	0.0	0.0	2.0	0.0000000000	False
correctly recognizes this image	0.0	0.0	0.0	2.0	0.0000000000	False
left of this image	0.0	0.0	0.0	2.0	0.0000000000	False
display on the left	0.0	0.0	0.0	2.0	0.0000000000	False
showing the intermediate computations	0.0	0.0	0.0	2.0	0.0000000000	False
network in other words	0.0	0.0	0.0	2.0	0.0000000000	False
right ? we re	0.0	0.0	0.0	0.0	0.0000000000	False
re just computing digits	0.0	0.0	0.0	2.0	0.0000000000	False
digits on the right-hand	0.0	0.0	0.0	2.0	0.0000000000	False
side of the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
display of the input	0.0	0.0	0.0	2.0	0.0000000000	False
noise all right multiple	0.0	0.0	0.0	2.0	0.0000000000	False
produced by wgbh television	0.0	0.0	0.0	2.0	0.0000000000	False
wgbh television in corporation	0.0	0.0	0.0	2.0	0.0000000000	False
corporation with british foreclass	0.0	0.0	0.0	2.0	0.0000000000	False
pbs a few years	0.0	0.0	0.0	2.0	0.0000000000	False
video describing the nettalk	0.0	0.0	0.0	2.0	0.0000000000	False
developed by terry sejnowski	0.0	0.0	0.0	2.0	0.0000000000	False
researcher and so nettalk	0.0	0.0	0.0	2.0	0.0000000000	False
milestones in the history	0.0	0.0	0.0	2.0	0.0000000000	False
history of neural network	0.0	0.0	3.99843668577	6.0	0.0000000000	False
neural network to read	0.0	0.0	0.0	2.0	0.0000000000	False
network to read text	0.0	0.0	0.0	2.0	0.0000000000	False
english to a computer	0.0	0.0	0.0	2.0	0.0000000000	False
sounds that could respond	0.0	0.0	0.0	2.0	0.0000000000	False
respond to the reading	0.0	0.0	0.0	2.0	0.0000000000	False
reading of the text	0.0	0.0	0.0	2.0	0.0000000000	False
text and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
history of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
video created a lot	0.0	0.0	0.0	2.0	0.0000000000	False
excitement about neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
networks and about machine	0.0	0.0	0.0	2.0	0.0000000000	False
part of the reason	0.0	0.0	0.0	2.0	0.0000000000	False
sejnowski had the foresight	0.0	0.0	0.0	2.0	0.0000000000	False
voice talking about visiting	0.0	0.0	0.0	2.0	0.0000000000	False
perception of  created	0.0	0.0	0.0	2.0	0.0000000000	False
learning how to speak	0.0	0.0	0.0	2.0	0.0000000000	False
helped generate a lot	0.0	0.0	0.0	2.0	0.0000000000	False
academia and outside academia	0.0	0.0	0.0	2.0	0.0000000000	False
academia on neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
early in the history	0.0	0.0	0.0	2.0	0.0000000000	False
show you the video	0.0	0.0	0.0	2.0	0.0000000000	False
re going to hear	0.0	0.0	0.0	2.0	0.0000000000	False
first what the network	0.0	0.0	0.0	2.0	0.0000000000	False
beginning of the training	0.0	0.0	0.0	2.0	0.0000000000	False
nt sound like words	0.0	0.0	0.0	0.0	0.0000000000	False
ll sound like attempts	0.0	0.0	0.0	2.0	0.0000000000	False
network takes the letters	0.0	0.0	0.0	2.0	0.0000000000	False
makes a random attempt	0.0	0.0	0.0	2.0	0.0000000000	False
random attempt at pronouncing	0.0	0.0	0.0	2.0	0.0000000000	False
house the phonetic difference	0.0	0.0	0.0	2.0	0.0000000000	False
difference between the guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess and the right	0.0	0.0	0.0	2.0	0.0000000000	False
back through the network	0.0	0.0	0.0	2.0	0.0000000000	False
adjusting the connection strengths	0.0	0.0	0.0	2.0	0.0000000000	False
strengths after each attempt	0.0	0.0	0.0	2.0	0.0000000000	False
associating letters with sounds	0.0	0.0	0.0	2.0	0.0000000000	False
amazing piece of work	0.0	0.0	0.0	2.0	0.0000000000	False
text to speech systems	0.0	0.0	0.0	2.0	0.0000000000	False
speech systems that work	0.0	0.0	0.0	2.0	0.0000000000	False
candy from your grandmother	0.0	0.0	0.0	2.0	0.0000000000	False
talking about the dow	0.0	0.0	0.0	2.0	0.0000000000	False
landmark in the history	0.0	0.0	0.0	2.0	0.0000000000	False
back to the chalkboard	0.0	0.0	0.0	2.0	0.0000000000	False
wraps up our discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on neural networks	0.0	0.0	0.0	2.0	0.0000000000	False
networks so i started	0.0	0.0	0.0	2.0	0.0000000000	False
neural networks by motivating	0.0	0.0	0.0	2.0	0.0000000000	False
drew on the chalkboard	0.0	0.0	0.0	2.0	0.0000000000	False
chalkboard earlier support vector	0.0	0.0	0.0	2.0	0.0000000000	False
earlier support vector machines	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that will give	0.0	0.0	0.0	2.0	0.0000000000	False
start off by describing	0.0	0.0	0.0	2.0	0.0000000000	False
describing yet another class	0.0	0.0	0.0	2.0	0.0000000000	False
class of linear classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers with linear division	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine idea	0.0	0.0	0.0	2.0	0.0000000000	False
generate non-linear division boundaries	0.0	0.0	0.0	2.0	0.0000000000	False
talking about linear classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
two intuitions about classification	0.0	0.0	0.0	2.0	0.0000000000	False
function that was outputting	0.0	0.0	0.0	2.0	0.0000000000	False
probability that y equals	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that computes theta	0.0	0.0	0.0	2.0	0.0000000000	False
right ? iff stands	0.0	0.0	0.0	2.0	0.0000000000	False
means the same thing	0.0	0.0	0.0	2.0	0.0000000000	False
case that theta transpose	0.0	0.0	0.0	2.0	0.0000000000	False
double greater than sign	0.0	0.0	0.0	2.0	0.0000000000	False
greater than sign means	0.0	0.0	0.0	2.0	0.0000000000	False
right so if theta	0.0	0.0	0.0	2.0	0.0000000000	False
right ? if theta	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na predict	0.0	0.0	0.0	2.0	0.0000000000	False
estimating that the probability	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers is your training	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm not only makes	0.0	0.0	0.0	2.0	0.0000000000	False
examples in a training	0.0	0.0	0.0	2.0	0.0000000000	False
talk about functional margins	0.0	0.0	0.0	2.0	0.0000000000	False
out for the rest	0.0	0.0	0.0	2.0	0.0000000000	False
assume that a training	0.0	0.0	0.0	2.0	0.0000000000	False
line that can separate	0.0	0.0	0.0	2.0	0.0000000000	False
separate your training set	0.0	0.0	0.0	2.0	0.0000000000	False
ll remove this assumption	0.0	0.0	0.0	2.0	0.0000000000	False
linearly separable training set	0.0	0.0	0.0	2.0	0.0000000000	False
separate the training set	0.0	0.0	0.0	2.0	0.0000000000	False
line in the middle	0.0	0.0	0.0	2.0	0.0000000000	False
negative examples and division	0.0	0.0	0.0	2.0	0.0000000000	False
examples and division boundary	0.0	0.0	0.0	2.0	0.0000000000	False
line that i drew	0.0	0.0	0.0	2.0	0.0000000000	False
distance from the training	0.0	0.0	0.0	2.0	0.0000000000	False
talk about geometric margins	0.0	0.0	0.0	2.0	0.0000000000	False
margins of our classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
order to describe support	0.0	0.0	0.0	2.0	0.0000000000	False
describe support vector machine	0.0	0.0	0.0	2.0	0.0000000000	False
pull a notation change	0.0	0.0	0.0	2.0	0.0000000000	False
slightly for linear classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
lectures to actually talk	0.0	0.0	0.0	2.0	0.0000000000	False
machine but the notation	0.0	0.0	0.0	2.0	0.0000000000	False
development of a support	0.0	0.0	0.0	2.0	0.0000000000	False
wrote g subscript theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta of x equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals g of theta	0.0	0.0	0.0	2.0	0.0000000000	False
letting x zero equals	0.0	0.0	0.0	2.0	0.0000000000	False
parameterize my linear classifier	0.0	0.0	0.0	2.0	0.0000000000	False
classifier as h subscript	0.0	0.0	0.0	2.0	0.0000000000	False
role of the rest	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
separating out the interceptor	0.0	0.0	0.0	2.0	0.0000000000	False
develop support vector machines	0.0	0.0	0.0	2.0	0.0000000000	False
notion of functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin and germesh margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin let me make	0.0	0.0	0.0	2.0	0.0000000000	False
margin of the hyper	0.0	0.0	0.0	2.0	0.0000000000	False
plane wb with respect	0.0	0.0	0.0	4.0	0.0000000000	False
xiyi is  wrt	0.0	0.0	0.0	2.0	0.0000000000	False
stands for with respect	0.0	0.0	0.0	2.0	0.0000000000	False
margin of a hyper	0.0	0.0	0.0	4.0	0.0000000000	False
xiyi has been defined	0.0	0.0	0.0	2.0	0.0000000000	False
defined as gamma hat	0.0	0.0	0.0	2.0	0.0000000000	False
gamma hat i equals	0.0	0.0	0.0	4.0	0.0000000000	False
defines a linear separating	0.0	0.0	0.0	2.0	0.0000000000	False
boundary that s defined	0.0	0.0	0.0	0.0	0.0000000000	False
defined by the parameters	0.0	0.0	0.0	4.0	0.0000000000	False
confused by the hyper	0.0	0.0	0.0	2.0	0.0000000000	False
ignore it the hyper	0.0	0.0	0.0	2.0	0.0000000000	False
respect to a training	0.0	0.0	0.0	2.0	0.0000000000	False
order for the function	0.0	0.0	0.0	4.0	0.0000000000	False
earlier about functional margins	0.0	0.0	0.0	2.0	0.0000000000	False
margins  the intuition	0.0	0.0	0.0	2.0	0.0000000000	False
practice of two cases	0.0	0.0	0.0	2.0	0.0000000000	False
cases into one statement	0.0	0.0	0.0	2.0	0.0000000000	False
margin to be large	0.0	0.0	5.99687337155	12.0	0.0000000000	False
long as yi times	0.0	0.0	0.0	2.0	0.0000000000	False
hyper plane with respect	0.0	0.0	0.0	2.0	0.0000000000	False
training examples of gamma	0.0	0.0	0.0	2.0	0.0000000000	False
define the functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
functional margin with respect	0.0	0.0	0.0	2.0	0.0000000000	False
function like an intuition	0.0	0.0	0.0	2.0	0.0000000000	False
problem with this intuition	0.0	0.0	0.0	2.0	0.0000000000	False
make the functional margin	0.0	0.0	2.99843668577	6.0	0.0000000000	False
times w times transpose	0.0	0.0	0.0	2.0	0.0000000000	False
double my functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin so this goal	0.0	0.0	0.0	2.0	0.0000000000	False
large just by scaling	0.0	0.0	0.0	2.0	0.0000000000	False
add a normalization condition	0.0	0.0	0.0	4.0	0.0000000000	False
alter-norm of the parameter	0.0	0.0	0.0	2.0	0.0000000000	False
margin of a training	0.0	0.0	0.0	4.0	0.0000000000	False
boundary of my classifier	0.0	0.0	0.0	2.0	0.0000000000	False
re going to draw	0.0	0.0	0.0	2.0	0.0000000000	False
draw relatively few training	0.0	0.0	0.0	2.0	0.0000000000	False
drawing deliberately few training	0.0	0.0	0.0	2.0	0.0000000000	False
deliberately few training examples	0.0	0.0	0.0	2.0	0.0000000000	False
define the geometric margin	0.0	0.0	5.99791558103	8.0	0.3445378151	False
distance between a point	0.0	0.0	0.0	2.0	0.0000000000	False
point between the training	0.0	0.0	0.0	2.0	0.0000000000	False
fairly quickly in case	0.0	0.0	0.0	2.0	0.0000000000	False
read through the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
carefully for details sort	0.0	0.0	0.0	2.0	0.0000000000	False
degrees to the separating	0.0	0.0	0.0	2.0	0.0000000000	False
divided by the norm	0.0	0.0	4.99843668577	6.0	0.0000000000	False
planes and high dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
work if this stuff	0.0	0.0	0.0	2.0	0.0000000000	False
notes on the website	0.0	0.0	0.0	2.0	0.0000000000	False
ll put a hat	0.0	0.0	0.0	2.0	0.0000000000	False
referring to functional margins	0.0	0.0	0.0	2.0	0.0000000000	False
top for geometric margins	0.0	0.0	0.0	2.0	0.0000000000	False
gamma i that means	0.0	0.0	0.0	2.0	0.0000000000	False
means that this point	0.0	0.0	0.0	2.0	0.0000000000	False
minus gamma i times	0.0	0.0	0.0	2.0	0.0000000000	False
normal to the separating	0.0	0.0	0.0	2.0	0.0000000000	False
subtract gamma i times	0.0	0.0	0.0	2.0	0.0000000000	False
times the unit vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector from this point	0.0	0.0	0.0	2.0	0.0000000000	False
point that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
satisfy w transpose times	0.0	0.0	0.0	2.0	0.0000000000	False
transpose times that point	0.0	0.0	0.0	2.0	0.0000000000	False
times that point equals	0.0	0.0	0.0	2.0	0.0000000000	False
separating hyper plane satisfy	0.0	0.0	0.0	2.0	0.0000000000	False
plane satisfy the equation	0.0	0.0	0.0	2.0	0.0000000000	False
fast in this geometry	0.0	0.0	0.0	2.0	0.0000000000	False
details in the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
ll solve for gamma	0.0	0.0	0.0	2.0	0.0000000000	False
equation i just wrote	0.0	0.0	0.0	2.0	0.0000000000	False
previous equation from gamma	0.0	0.0	0.0	2.0	0.0000000000	False
equals gamma i times	0.0	0.0	0.0	2.0	0.0000000000	False
transpose w over norm	0.0	0.0	0.0	2.0	0.0000000000	False
equal to gamma times	0.0	0.0	0.0	2.0	0.0000000000	False
gamma times the norm	0.0	0.0	0.0	2.0	0.0000000000	False
norm of w squared	0.0	0.0	0.0	2.0	0.0000000000	False
separating hyper plane defined	0.0	0.0	0.0	2.0	0.0000000000	False
computed by this formula	0.0	0.0	0.0	2.0	0.0000000000	False
classification of the training	0.0	0.0	0.0	2.0	0.0000000000	False
assuming that we ve	0.0	0.0	0.0	0.0	0.0000000000	False
classifying an example correctly	0.0	0.0	0.0	4.0	0.0000000000	False
find the geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
normalization by the norm	0.0	0.0	0.0	2.0	0.0000000000	False
long as we re	0.0	0.0	0.0	0.0	0.0000000000	False
side of the separating	0.0	0.0	0.0	2.0	0.0000000000	False
couple of easy facts	0.0	0.0	0.0	2.0	0.0000000000	False
geometric margin with respect	0.0	0.0	0.0	4.0	0.0000000000	False
training set as gamma	0.0	0.0	0.0	2.0	0.0000000000	False
set as gamma equals	0.0	0.0	0.0	2.0	0.0000000000	False
precursor to the support	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm that chooses	0.0	0.0	0.0	2.0	0.0000000000	False
maximize the geometric margin	0.0	0.0	5.99843668577	6.0	0.0000000000	False
maximum margin classified poses	0.0	0.0	0.0	2.0	0.0000000000	False
poses the following optimization	0.0	0.0	0.0	2.0	0.0000000000	False
problem it says choose	0.0	0.0	0.0	2.0	0.0000000000	False
minutes i m guessing	0.0	0.0	0.0	0.0	0.0000000000	False
classifier is the maximization	0.0	0.0	0.0	2.0	0.0000000000	False
maximization problem over parameter	0.0	0.0	0.0	2.0	0.0000000000	False
problem over parameter gamma	0.0	0.0	0.0	2.0	0.0000000000	False
margin does nt change	0.0	0.0	0.0	0.0	0.0000000000	False
depending on the norm	0.0	0.0	0.0	2.0	0.0000000000	False
notice that we re	0.0	0.0	0.0	0.0	0.0000000000	False
change the geometric margin	0.0	0.0	0.0	4.0	0.0000000000	False
impose any normalization constant	0.0	0.0	0.0	2.0	0.0000000000	False
scaling factor and replace	0.0	0.0	0.0	2.0	0.0000000000	False
margin at least gamma	0.0	0.0	0.0	2.0	0.0000000000	False
comparable to logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature spaces	0.0	0.0	0.0	2.0	0.0000000000	False
re given a fixed	0.0	0.0	0.0	4.0	0.0000000000	False
scaling of the training	0.0	0.0	0.0	2.0	0.0000000000	False
ll talk about authorization	0.0	0.0	0.0	2.0	0.0000000000	False
talk about authorization algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
students taking the classes	0.0	0.0	0.0	2.0	0.0000000000	False
remotely  to turn	0.0	0.0	0.0	2.0	0.0000000000	False
turn in the process	0.0	0.0	0.0	2.0	0.0000000000	False
solutions due this wednesday	0.0	0.0	0.0	2.0	0.0000000000	False
top of the problem	0.0	0.0	0.0	2.0	0.0000000000	False
longer for your solutions	0.0	0.0	0.0	2.0	0.0000000000	False
re not an scpd	0.0	0.0	0.0	2.0	0.0000000000	False
turn in hard copies	0.0	0.0	0.0	2.0	0.0000000000	False
copies of your solutions	0.0	0.0	0.0	2.0	0.0000000000	False
send them by fax	0.0	0.0	0.0	2.0	0.0000000000	False
fax unless you re	0.0	0.0	0.0	0.0	0.0000000000	False
re an scpd student	0.0	0.0	0.0	2.0	0.0000000000	False
last few office hours	0.0	0.0	0.0	2.0	0.0000000000	False
lively discussions with people	0.0	0.0	0.0	2.0	0.0000000000	False
holding extra office hours	0.0	0.0	0.0	2.0	0.0000000000	False
office hours in case	0.0	0.0	0.0	2.0	0.0000000000	False
people in the back	0.0	0.0	0.0	2.0	0.0000000000	False
turn up the volume	0.0	0.0	0.0	2.0	0.0000000000	False
announcements so welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
wan na do today	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on support vector	0.0	0.0	0.0	2.0	0.0000000000	False
classifier then i wan	0.0	0.0	0.0	2.0	0.0000000000	False
primal and duo optimization	0.0	0.0	0.0	4.0	0.0000000000	False
ll derive the duo	0.0	0.0	0.0	4.0	0.0000000000	False
duo to the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
lecture and as part	0.0	0.0	0.0	2.0	0.0000000000	False
spend some time talking	0.0	0.0	0.0	2.0	0.0000000000	False
talking about optimization problems	0.0	0.0	0.0	2.0	0.0000000000	False
topic justice i wan	0.0	0.0	0.0	2.0	0.0000000000	False
talk about convex optimization	0.0	0.0	0.0	4.0	0.0000000000	False
week s discussion session	0.0	0.0	0.0	0.0	0.0000000000	False
teach a discussion session	0.0	0.0	0.0	2.0	0.0000000000	False
discussion session  focus	0.0	0.0	0.0	2.0	0.0000000000	False
focus on convex optimization	0.0	0.0	0.0	2.0	0.0000000000	False
convex optimization  sort	0.0	0.0	0.0	2.0	0.0000000000	False
beautiful and useful theory	0.0	0.0	0.0	2.0	0.0000000000	False
listen to this friday	0.0	0.0	0.0	2.0	0.0000000000	False
friday s discussion session	0.0	0.0	0.0	0.0	0.0000000000	False
session just to recap	0.0	0.0	0.0	2.0	0.0000000000	False
developing on support vector	0.0	0.0	0.0	2.0	0.0000000000	False
represented as h sub	0.0	0.0	0.0	2.0	0.0000000000	False
development of support vector	0.0	0.0	0.0	2.0	0.0000000000	False
ll change the convention	0.0	0.0	0.0	2.0	0.0000000000	False
note the class labels	0.0	0.0	0.0	2.0	0.0000000000	False
correctly and very confidently	0.0	0.0	0.0	2.0	0.0000000000	False
large and it makes	0.0	0.0	0.0	2.0	0.0000000000	False
margins to be large	0.0	0.0	0.0	2.0	0.0000000000	False
margin is a strange	0.0	0.0	0.0	2.0	0.0000000000	False
defined the geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin had the interpretation	0.0	0.0	0.0	2.0	0.0000000000	False
examples the geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
interpretation as a distance	0.0	0.0	0.0	2.0	0.0000000000	False
distance between a training	0.0	0.0	0.0	2.0	0.0000000000	False
positive if you re	0.0	0.0	0.0	0.0	0.0000000000	False
classifying the example correctly	0.0	0.0	0.0	2.0	0.0000000000	False
ll be the minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus of the distance	0.0	0.0	0.0	2.0	0.0000000000	False
hyperplane where you re	0.0	0.0	0.0	0.0	0.0000000000	False
separating hyperplane is defined	0.0	0.0	0.0	2.0	0.0000000000	False
defined by the equation	0.0	0.0	0.0	2.0	0.0000000000	False
respect to training set	0.0	0.0	0.0	2.0	0.0000000000	False
training set i defined	0.0	0.0	0.0	2.0	0.0000000000	False
minimum functional geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm would choose	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm would choose parameters	0.0	0.0	0.0	2.0	0.0000000000	False
maximize the geometric margin	0.0	0.0	0.0	4.0	0.0000000000	False
margin so our goal	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to find	0.0	0.0	0.0	2.0	0.0000000000	False
find the separating hyperplane	0.0	0.0	0.0	2.0	0.0000000000	False
separating hyperplane that separates	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative examples	0.0	0.0	1.9978689398	8.0	0.0000000000	False
margin i can choose	0.0	0.0	0.0	2.0	0.0000000000	False
change my geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
line you re separating	0.0	0.0	0.0	0.0	0.0000000000	False
positive and negative training	0.0	0.0	0.0	2.0	0.0000000000	False
examples if i scale	0.0	0.0	0.0	2.0	0.0000000000	False
nt change the position	0.0	0.0	0.0	0.0	0.0000000000	False
position of this plane	0.0	0.0	0.0	2.0	0.0000000000	False
easily meet this condition	0.0	0.0	0.0	2.0	0.0000000000	False
condition that  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
find the absolute solution	0.0	0.0	0.0	2.0	0.0000000000	False
rescale w and meet	0.0	0.0	0.0	2.0	0.0000000000	False
choose any scaling condition	0.0	0.0	0.0	2.0	0.0000000000	False
break down the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to choose	0.0	0.0	0.0	2.0	0.0000000000	False
first attempt at writing	0.0	0.0	0.0	2.0	0.0000000000	False
writing down the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem actually wrote	0.0	0.0	0.0	2.0	0.0000000000	False
right at the end	0.0	0.0	0.0	2.0	0.0000000000	False
lecture begin to solve	0.0	0.0	0.0	2.0	0.0000000000	False
solve the parameters gamma	0.0	0.0	0.0	2.0	0.0000000000	False
add this normalization condition	0.0	0.0	0.0	2.0	0.0000000000	False
condition so the norm	0.0	0.0	0.0	2.0	0.0000000000	False
examples have functional margin	0.0	0.0	0.0	4.0	0.0000000000	False
greater than or equals	0.0	0.0	0.0	2.0	0.0000000000	False
margin and geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
gamma so you solve	0.0	0.0	0.0	2.0	0.0000000000	False
solve this optimization problem	0.0	0.0	1.9978689398	8.0	0.0000000000	False
derived the optimal margin	0.0	0.0	0.0	4.0	0.0000000000	False
parameters w that lie	0.0	0.0	0.0	2.0	0.0000000000	False
lie on the surface	0.0	0.0	0.0	2.0	0.0000000000	False
lies on a unicircle	0.0	0.0	0.0	2.0	0.0000000000	False
unicircle  a unisphere	0.0	0.0	0.0	2.0	0.0000000000	False
guaranteed that our descend	0.0	0.0	0.0	2.0	0.0000000000	False
optimal and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
change the optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
slightly different optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem let me maximize	0.0	0.0	0.0	2.0	0.0000000000	False
maximize the functional margin	0.0	0.0	3.9978689398	8.0	0.0000000000	False
examples has functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
greater than the gamma	0.0	0.0	0.0	2.0	0.0000000000	False
maximize gamma hat divided	0.0	0.0	0.0	4.0	0.0000000000	False
previously the function margin	0.0	0.0	0.0	2.0	0.0000000000	False
posing the same optimization	0.0	0.0	0.0	4.0	0.0000000000	False
functional margin y divided	0.0	0.0	0.0	2.0	0.0000000000	False
re saying the data	0.0	0.0	0.0	2.0	0.0000000000	False
functional margin is divided	0.0	0.0	0.0	2.0	0.0000000000	False
stage the functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem where gamma	0.0	0.0	0.0	2.0	0.0000000000	False
convex optimization software solves	0.0	0.0	0.0	2.0	0.0000000000	False
software solves this problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem for some set	0.0	0.0	0.0	2.0	0.0000000000	False
set of parameters gamma	0.0	0.0	0.0	2.0	0.0000000000	False
constraint that whatever values	0.0	0.0	0.0	2.0	0.0000000000	False
greater than gamma hat	0.0	0.0	0.0	2.0	0.0000000000	False
equal to gamma hat	0.0	0.0	0.0	4.0	0.0000000000	False
constraint to the function	0.0	0.0	0.0	2.0	0.0000000000	False
margin and a constraint	0.0	0.0	0.0	2.0	0.0000000000	False
constraint to the gamma	0.0	0.0	0.0	2.0	0.0000000000	False
hat does that make	0.0	0.0	0.0	2.0	0.0000000000	False
maximize gamma or gamma	0.0	0.0	0.0	2.0	0.0000000000	False
hat so that gamma	0.0	0.0	0.0	2.0	0.0000000000	False
write down an optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem in order	0.0	0.0	0.0	2.0	0.0000000000	False
out that the question	0.0	0.0	0.0	2.0	0.0000000000	False
gamma hat the function	0.0	0.0	0.0	2.0	0.0000000000	False
software for solving convex	0.0	0.0	0.0	2.0	0.0000000000	False
solving convex optimization problems	0.0	0.0	0.0	2.0	0.0000000000	False
optimization software to find	0.0	0.0	0.0	2.0	0.0000000000	False
subject to this constraint	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
choose to make gamma	0.0	0.0	0.0	2.0	0.0000000000	False
big can we make	0.0	0.0	0.0	2.0	0.0000000000	False
limited by use constraints	0.0	0.0	0.0	2.0	0.0000000000	False
bigger you can make	0.0	0.0	0.0	2.0	0.0000000000	False
margins of your training	0.0	0.0	0.0	2.0	0.0000000000	False
two class of data	0.0	0.0	2.99840170485	6.0	0.0000000000	False
draw me a line	0.0	0.0	0.0	2.0	0.0000000000	False
line and the data	0.0	0.0	0.0	2.0	0.0000000000	False
guess i m wondering	0.0	0.0	0.0	0.0	0.0000000000	False
formalization of the problem	0.0	0.0	0.0	2.0	0.0000000000	False
maximizing the worst-case distance	0.0	0.0	0.0	2.0	0.0000000000	False
distance between the point	0.0	0.0	0.0	2.0	0.0000000000	False
point and this line	0.0	0.0	0.0	2.0	0.0000000000	False
care about the worst-case	0.0	0.0	2.99840170485	6.0	0.0000000000	False
nice way to formulate	0.0	0.0	0.0	2.0	0.0000000000	False
formulate this optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	2.99840170485	6.0	0.0000000000	False
rid of this nasty	0.0	0.0	0.0	2.0	0.0000000000	False
convex function in parameters	0.0	0.0	0.0	2.0	0.0000000000	False
fairly bizarre scaling constraints	0.0	0.0	0.0	2.0	0.0000000000	False
choose any scaling constraint	0.0	0.0	0.0	2.0	0.0000000000	False
assume for the purposes	0.0	0.0	0.0	2.0	0.0000000000	False
assume that these examples	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative classes	0.0	0.0	0.0	2.0	0.0000000000	False
find that your worst-case	0.0	0.0	0.0	2.0	0.0000000000	False
scaling constraint would imply	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem and add	0.0	0.0	0.0	2.0	0.0000000000	False
add the scaling constraint	0.0	0.0	0.0	2.0	0.0000000000	False
maximization over gamma hats	0.0	0.0	0.0	2.0	0.0000000000	False
squared it was great	0.0	0.0	0.0	2.0	0.0000000000	False
normal w is min	0.0	0.0	0.0	2.0	0.0000000000	False
constraints since i ve	0.0	0.0	0.0	0.0	0.0000000000	False
ve added the constraint	0.0	0.0	0.0	2.0	0.0000000000	False
optimal margin classifier problem	0.0	0.0	0.0	4.0	0.0000000000	False
function and those pictures	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
number of linear constraints	0.0	0.0	0.0	2.0	0.0000000000	False
constraints in your parameters	0.0	0.0	0.0	2.0	0.0000000000	False
linear constraints that eliminates	0.0	0.0	0.0	2.0	0.0000000000	False
eliminates that half space	0.0	0.0	0.0	4.0	0.0000000000	False
space or linear constraint	0.0	0.0	0.0	2.0	0.0000000000	False
ruling out various half	0.0	0.0	0.0	2.0	0.0000000000	False
out various half spaces	0.0	0.0	0.0	2.0	0.0000000000	False
constraints and i hope	0.0	0.0	0.0	2.0	0.0000000000	False
hope you can convince	0.0	0.0	0.0	2.0	0.0000000000	False
great within this set	0.0	0.0	0.0	2.0	0.0000000000	False
optimal margin classifier algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
formulation of the problem	0.0	0.0	0.0	4.0	0.0000000000	False
program software this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic convex objective function	0.0	0.0	0.0	2.0	0.0000000000	False
objective function and constraints	0.0	0.0	0.0	2.0	0.0000000000	False
download software to solve	0.0	0.0	0.0	2.0	0.0000000000	False
solve these optimization problems	0.0	0.0	0.0	2.0	0.0000000000	False
form of this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
problem and the reason	0.0	0.0	0.0	4.0	0.0000000000	False
turns out this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
out this optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem has certain properties	0.0	0.0	0.0	2.0	0.0000000000	False
apply the optimal margin	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature spaces	0.0	0.0	0.0	2.0	0.0000000000	False
method of lagrange multipliers	0.0	0.0	7.9968034097	12.0	0.2600000000	False
lagrange multipliers for solving	0.0	0.0	0.0	2.0	0.0000000000	False
solving an optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
subject to some constraint	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
generalization of this method	0.0	0.0	0.0	2.0	0.0000000000	False
subject to some set	0.0	0.0	0.0	2.0	0.0000000000	False
multipliers where you construct	0.0	0.0	0.0	2.0	0.0000000000	False
objective plus some lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
multipliers the highest constraints	0.0	0.0	0.0	2.0	0.0000000000	False
constraints and these parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters  they derive	0.0	0.0	0.0	2.0	0.0000000000	False
derive  we call	0.0	0.0	0.0	2.0	0.0000000000	False
call the lagrange multipliers	0.0	0.0	0.0	2.0	0.0000000000	False
solve the optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
original parameters and set	0.0	0.0	0.0	2.0	0.0000000000	False
partial derivative with respect	0.0	0.0	0.998401704848	6.0	0.0000000000	False
respect to your lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
exists so there exists	0.0	0.0	0.0	2.0	0.0000000000	False
multipliers is to solve	0.0	0.0	0.0	2.0	0.0000000000	False
respect to the lagrange	0.0	0.0	0.0	4.0	0.0000000000	False
lagrange multipliers beta set	0.0	0.0	0.0	2.0	0.0000000000	False
set the partial derivatives	0.0	0.0	0.0	2.0	0.0000000000	False
solve for our solutions	0.0	0.0	0.0	2.0	0.0000000000	False
write down the generalization	0.0	0.0	0.0	4.0	0.0000000000	False
slightly more difficult type	0.0	0.0	0.0	2.0	0.0000000000	False
difficult type of constraint	0.0	0.0	0.0	2.0	0.0000000000	False
type of constraint optimization	0.0	0.0	0.0	2.0	0.0000000000	False
subject to the constraint	0.0	0.0	1.99840170485	6.0	0.0000000000	False
original optimization for parameters	0.0	0.0	0.0	2.0	0.0000000000	False
two sets of lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
sets of lagrange multipliers	0.0	0.0	0.0	4.0	0.0000000000	False
max of alpha beta	0.0	0.0	0.0	2.0	0.0000000000	False
constraints that the alphas	0.0	0.0	0.0	2.0	0.0000000000	False
max over alpha beta	0.0	0.0	0.0	2.0	0.0000000000	False
sense of primal problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem and that refers	0.0	0.0	0.0	2.0	0.0000000000	False
entire thing this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
thing this optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem that written	0.0	0.0	0.0	2.0	0.0000000000	False
primal problem this means	0.0	0.0	0.0	2.0	0.0000000000	False
problem in which solving	0.0	0.0	0.0	2.0	0.0000000000	False
derive in another version	0.0	0.0	0.0	2.0	0.0000000000	False
minimize w  minimize	0.0	0.0	0.0	2.0	0.0000000000	False
minimize as a function	0.0	0.0	0.0	2.0	0.0000000000	False
beta  the lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
setting the other responding	0.0	0.0	0.0	2.0	0.0000000000	False
make this arbitrarily large	0.0	0.0	0.0	2.0	0.0000000000	False
primal problem s constraints	0.0	0.0	0.0	0.0	0.0000000000	False
alpha of this lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
infinity or minus infinity	0.0	0.0	0.0	2.0	0.0000000000	False
depending on the sign	0.0	0.0	0.0	2.0	0.0000000000	False
make this plus infinity	0.0	0.0	0.0	2.0	0.0000000000	False
setting all the lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
theta p just left	0.0	0.0	0.0	2.0	0.0000000000	False
equal to plus infinity	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down that minimizes	0.0	0.0	0.0	2.0	0.0000000000	False
cool so all right	0.0	0.0	0.0	2.0	0.0000000000	False
problem to find theta	0.0	0.0	0.0	2.0	0.0000000000	False
function of the lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange and my duo	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
theta d over alpha	0.0	0.0	0.0	2.0	0.0000000000	False
beta so this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
previous prime optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem the only difference	0.0	0.0	0.0	2.0	0.0000000000	False
primal and the duo	0.0	0.0	7.99733617475	10.0	0.3255425710	False
star in other words	0.0	0.0	0.0	4.0	0.0000000000	False
defined p star previously	0.0	0.0	0.0	2.0	0.0000000000	False
star previously p star	0.0	0.0	0.0	2.0	0.0000000000	False
star was a value	0.0	0.0	0.0	2.0	0.0000000000	False
true that the max	0.0	0.0	0.0	2.0	0.0000000000	False
max of the min	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the min	0.0	0.0	0.0	2.0	0.0000000000	False
min of the max	0.0	0.0	0.0	2.0	0.0000000000	False
min of the set	0.0	0.0	0.0	2.0	0.0000000000	False
min so this equality	0.0	0.0	0.0	2.0	0.0000000000	False
equality  this inequality	0.0	0.0	0.0	2.0	0.0000000000	False
true for any function	0.0	0.0	0.0	4.0	0.0000000000	False
function you might find	0.0	0.0	0.0	2.0	0.0000000000	False
function you might put	0.0	0.0	0.0	2.0	0.0000000000	False
solve the duo problem	0.0	0.0	3.9978689398	8.0	0.0000000000	False
support vector machine problem	0.0	0.0	0.0	4.0	0.0000000000	False
properties that will make	0.0	0.0	0.0	2.0	0.0000000000	False
compared to the primal	0.0	0.0	0.0	2.0	0.0000000000	False
formally the certain conditions	0.0	0.0	0.0	2.0	0.0000000000	False
duo problems are equivalent	0.0	0.0	0.0	2.0	0.0000000000	False
out the of support	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
derive the duo optimization	0.0	0.0	0.0	2.0	0.0000000000	False
ll solve the duo	0.0	0.0	0.0	2.0	0.0000000000	False
problem and by modifying	0.0	0.0	0.0	2.0	0.0000000000	False
ll derive this support	0.0	0.0	0.0	2.0	0.0000000000	False
derive this support vector	0.0	0.0	0.0	2.0	0.0000000000	False
write down the conditions	0.0	0.0	0.0	2.0	0.0000000000	False
duo optimization problems give	0.0	0.0	0.0	2.0	0.0000000000	False
convex if you re	0.0	0.0	0.0	0.0	0.0000000000	False
purposes of this class	0.0	0.0	0.0	2.0	0.0000000000	False
learn more about optimization	0.0	0.0	0.0	2.0	0.0000000000	False
taught by the tas	0.0	0.0	0.0	2.0	0.0000000000	False
equals alpha i transpose	0.0	0.0	0.0	2.0	0.0000000000	False
means the same thing	0.0	0.0	0.0	2.0	0.0000000000	False
linear without the term	0.0	0.0	0.0	2.0	0.0000000000	False
stricter than the equality	0.0	0.0	0.0	2.0	0.0000000000	False
solves the primal problem	0.0	0.0	0.0	2.0	0.0000000000	False
primal problem and alpha	0.0	0.0	0.0	2.0	0.0000000000	False
problem and alpha star	0.0	0.0	0.0	2.0	0.0000000000	False
alpha star and beta	0.0	0.0	0.0	2.0	0.0000000000	False
star and beta star	0.0	0.0	0.0	2.0	0.0000000000	False
problem and the value	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the value	0.0	0.0	2.99840170485	6.0	0.0000000000	False
value of the duo	0.0	0.0	0.0	2.0	0.0000000000	False
value of your lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange multiplier  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
value of your generalized	0.0	0.0	0.0	2.0	0.0000000000	False
primal or the duo	0.0	0.0	0.0	2.0	0.0000000000	False
conditions partial derivative perspective	0.0	0.0	0.0	2.0	0.0000000000	False
partial derivative perspective parameters	0.0	0.0	0.0	2.0	0.0000000000	False
derive our duo problem	0.0	0.0	0.0	2.0	0.0000000000	False
out this will hold	0.0	0.0	0.0	2.0	0.0000000000	False
kkt complementary condition kkt	0.0	0.0	0.0	2.0	0.0000000000	False
complementary condition kkt stands	0.0	0.0	0.0	2.0	0.0000000000	False
kkt stands for karush-kuhn-tucker	0.0	0.0	0.0	2.0	0.0000000000	False
authors of this theorem	0.0	0.0	0.0	2.0	0.0000000000	False
constraints are actually satisfied	0.0	0.0	0.0	2.0	0.0000000000	False
optimal margin optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
word about this ktt	0.0	0.0	0.0	2.0	0.0000000000	False
alpha star i times	0.0	0.0	0.0	2.0	0.0000000000	False
product of two numbers	0.0	0.0	0.0	2.0	0.0000000000	False
product of two things	0.0	0.0	0.0	2.0	0.0000000000	False
karush-kuhn-tucker  most people	0.0	0.0	0.0	2.0	0.0000000000	False
people just say kkt	0.0	0.0	0.0	2.0	0.0000000000	False
show you the right	0.0	0.0	0.0	2.0	0.0000000000	False
spelling of their names	0.0	0.0	0.0	2.0	0.0000000000	False
kkt complementary condition implies	0.0	0.0	0.0	2.0	0.0000000000	False
implies that if alpha	0.0	0.0	0.0	2.0	0.0000000000	False
case that both alpha	0.0	0.0	0.0	2.0	0.0000000000	False
constraint because we call	0.0	0.0	0.0	2.0	0.0000000000	False
constraint  our constraint	0.0	0.0	0.0	2.0	0.0000000000	False
constraint once we talk	0.0	0.0	0.0	2.0	0.0000000000	False
idea a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit more board turn	0.0	0.0	0.0	2.0	0.0000000000	False
turn to this board	0.0	0.0	0.0	2.0	0.0000000000	False
classifier for the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
problem that we worked	0.0	0.0	0.0	2.0	0.0000000000	False
deriving the kkt conditions	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange multipliers were alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha i and beta	0.0	0.0	0.0	2.0	0.0000000000	False
out that when applied	0.0	0.0	0.0	2.0	0.0000000000	False
working out the kkt	0.0	0.0	0.0	2.0	0.0000000000	False
out the kkt conditions	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem i wanted	0.0	0.0	0.0	2.0	0.0000000000	False
problem finding the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
two sets of parameters	0.0	0.0	0.0	2.0	0.0000000000	False
sort of slight notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation change in mind	0.0	0.0	0.0	2.0	0.0000000000	False
half there by convention	0.0	0.0	0.0	2.0	0.0000000000	False
convention because it makes	0.0	0.0	0.0	2.0	0.0000000000	False
work  math work	0.0	0.0	0.0	2.0	0.0000000000	False
work a little nicer	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the kkt	0.0	0.0	0.0	2.0	0.0000000000	False
kkt duo complementary condition	0.0	0.0	0.0	2.0	0.0000000000	False
means that my training	0.0	0.0	0.0	2.0	0.0000000000	False
guess so in pictures	0.0	0.0	0.0	2.0	0.0000000000	False
ll have some separating	0.0	0.0	0.0	2.0	0.0000000000	False
examples with functional margin	0.0	0.0	5.99840170485	6.0	0.0000000000	False
closest to my separating	0.0	0.0	0.0	2.0	0.0000000000	False
examples of functional margin	0.0	0.0	0.0	4.0	0.0000000000	False
suggested by this picture	0.0	0.0	0.0	2.0	0.0000000000	False
out that we find	0.0	0.0	0.0	2.0	0.0000000000	False
picture i ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
distance to your separating	0.0	0.0	0.0	2.0	0.0000000000	False
re going to call	0.0	0.0	0.0	2.0	0.0000000000	False
call the support vectors	0.0	0.0	0.0	2.0	0.0000000000	False
vector machine there ll	0.0	0.0	0.0	0.0	0.0000000000	False
points with functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
re calling support vectors	0.0	0.0	0.0	2.0	0.0000000000	False
vectors and the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact that they re	0.0	0.0	0.0	0.0	0.0000000000	False
re relatively few support	0.0	0.0	0.0	2.0	0.0000000000	False
support vectors also means	0.0	0.0	0.0	2.0	0.0000000000	False
out the actual optimization	0.0	0.0	0.0	2.0	0.0000000000	False
write down the margin	0.0	0.0	0.0	2.0	0.0000000000	False
constraint we have inequality	0.0	0.0	0.0	2.0	0.0000000000	False
constraints and no equality	0.0	0.0	0.0	2.0	0.0000000000	False
ll only have lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange multipliers of type	0.0	0.0	0.0	2.0	0.0000000000	False
multipliers of type alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha  no betas	0.0	0.0	0.0	2.0	0.0000000000	False
betas in my generalized	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange but my lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
one-half w squared minus	0.0	0.0	0.0	2.0	0.0000000000	False
out what the duo	0.0	0.0	0.0	2.0	0.0000000000	False
figure out what theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta d of alpha	0.0	0.0	3.9968034097	12.0	0.3145161290	False
alpha so the duo	0.0	0.0	0.0	2.0	0.0000000000	False
problem is the maximize	0.0	0.0	0.0	2.0	0.0000000000	False
work out what theta	0.0	0.0	0.0	2.0	0.0000000000	False
give us our duo	0.0	0.0	0.0	2.0	0.0000000000	False
lagrange as a function	0.0	0.0	0.0	2.0	0.0000000000	False
write down the answer	0.0	0.0	0.0	2.0	0.0000000000	False
combination of your input	0.0	0.0	0.0	2.0	0.0000000000	False
examples in your training	0.0	0.0	0.0	2.0	0.0000000000	False
partial derivative of lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
equal to minus sum	0.0	0.0	0.0	2.0	0.0000000000	False
out what the lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
putting in the value	0.0	0.0	0.0	2.0	0.0000000000	False
angle brackets to denote	0.0	0.0	0.0	2.0	0.0000000000	False
brackets to denote end	0.0	0.0	0.0	2.0	0.0000000000	False
means the end product	0.0	0.0	0.0	2.0	0.0000000000	False
first and second terms	0.0	0.0	0.0	2.0	0.0000000000	False
half so to simplify	0.0	0.0	0.0	2.0	0.0000000000	False
alpha my duo problem	0.0	0.0	0.0	2.0	0.0000000000	False
maximize w of alpha	0.0	0.0	0.0	4.0	0.0000000000	False
notation is somewhat unfortunate	0.0	0.0	0.0	2.0	0.0000000000	False
capital w of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
problem lowercase w transpose	0.0	0.0	0.0	2.0	0.0000000000	False
transpose xi so uppercase	0.0	0.0	0.0	2.0	0.0000000000	False
subject to the alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha i is related	0.0	0.0	0.0	2.0	0.0000000000	False
constraint was the constraint	0.0	0.0	0.0	2.0	0.0000000000	False
years that i taught	0.0	0.0	0.0	2.0	0.0000000000	False
derivative of the lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
excuse me then theta	0.0	0.0	0.0	2.0	0.0000000000	False
equal to minus infinity	0.0	0.0	0.0	4.0	0.0000000000	False
minus infinity for minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
turns out my lagrange	0.0	0.0	0.0	2.0	0.0000000000	False
function of my parameters	0.0	0.0	0.0	2.0	0.0000000000	False
interpretation of that constraint	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
maximize as a function	0.0	0.0	0.0	2.0	0.0000000000	False
ve got ta choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose values of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha for which sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum of yi alpha	0.0	0.0	2.99840170485	6.0	0.0000000000	False
subject to that sum	0.0	0.0	0.0	2.0	0.0000000000	False
bit of extra notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation in our derivation	0.0	0.0	0.0	2.0	0.0000000000	False
derivation of the duo	0.0	0.0	0.0	2.0	0.0000000000	False
action of the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
cool so what derived	0.0	0.0	0.0	2.0	0.0000000000	False
derived a duo optimization	0.0	0.0	0.0	2.0	0.0000000000	False
worked out this constraint	0.0	0.0	0.0	2.0	0.0000000000	False
worked out the duo	0.0	0.0	0.0	2.0	0.0000000000	False
duo of the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
finding  to deriving	0.0	0.0	0.0	2.0	0.0000000000	False
margin classifier or support	0.0	0.0	0.0	2.0	0.0000000000	False
classifier or support vector	0.0	0.0	0.0	2.0	0.0000000000	False
solve along this duo	0.0	0.0	0.0	2.0	0.0000000000	False
problem for the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
equation that we worked	0.0	0.0	0.0	2.0	0.0000000000	False
derive w in parameters	0.0	0.0	0.0	2.0	0.0000000000	False
problem and we worked	0.0	0.0	0.0	2.0	0.0000000000	False
worked this out earlier	0.0	0.0	0.0	2.0	0.0000000000	False
interpretation of training set	0.0	0.0	0.0	2.0	0.0000000000	False
separating hyperplane s direction	0.0	0.0	0.0	0.0	0.0000000000	False
orientation and separating hyperplane	0.0	0.0	0.0	2.0	0.0000000000	False
decide where to place	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem and solve	0.0	0.0	0.0	2.0	0.0000000000	False
intuition behind this formula	0.0	0.0	0.0	2.0	0.0000000000	False
place the separating hyperplane	0.0	0.0	0.0	2.0	0.0000000000	False
duo problem we re	0.0	0.0	0.0	0.0	0.0000000000	False
re going to solve	0.0	0.0	0.0	2.0	0.0000000000	False
problem for the alpha	0.0	0.0	0.0	2.0	0.0000000000	False
wan na point out	0.0	0.0	0.0	2.0	0.0000000000	False
out as i lead	0.0	0.0	0.0	2.0	0.0000000000	False
express the entire algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
entire algorithm in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of inner products	0.0	0.0	0.0	2.0	0.0000000000	False
sum of your input	0.0	0.0	0.0	2.0	0.0000000000	False
value of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis on the value	0.0	0.0	0.0	2.0	0.0000000000	False
threshold function that outputs	0.0	0.0	0.0	2.0	0.0000000000	False
function that outputs minus	0.0	0.0	0.0	2.0	0.0000000000	False
expressed as a sum	0.0	0.0	0.0	2.0	0.0000000000	False
products between your training	0.0	0.0	0.0	2.0	0.0000000000	False
value of x value	0.0	0.0	0.0	2.0	0.0000000000	False
kernels and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
source of feature spaces	0.0	0.0	0.0	2.0	0.0000000000	False
machines  it turns	0.0	0.0	0.0	2.0	0.0000000000	False
case that the features	0.0	0.0	0.0	2.0	0.0000000000	False
products like these efficiently	0.0	0.0	0.0	2.0	0.0000000000	False
efficiently and this holds	0.0	0.0	0.0	2.0	0.0000000000	False
true for arbitrary sets	0.0	0.0	0.0	2.0	0.0000000000	False
arbitrary sets of features	0.0	0.0	0.0	2.0	0.0000000000	False
features but we talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the idea	0.0	0.0	0.0	2.0	0.0000000000	False
extremely high-dimensional feature vectors	0.0	0.0	0.0	2.0	0.0000000000	False
store in computer memory	0.0	0.0	0.0	2.0	0.0000000000	False
products between different feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature vectors very efficiently	0.0	0.0	0.0	2.0	0.0000000000	False
make predictions by making	0.0	0.0	0.0	2.0	0.0000000000	False
transpose you will compute	0.0	0.0	0.0	2.0	0.0000000000	False
compute these inner products	0.0	0.0	0.0	2.0	0.0000000000	False
alpha is again written	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem and step	0.0	0.0	0.0	2.0	0.0000000000	False
steps of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
products with your feature	0.0	0.0	0.0	2.0	0.0000000000	False
property of this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that s kinda	0.0	0.0	0.0	0.0	0.0000000000	False
previously that the alpha	0.0	0.0	0.0	2.0	0.0000000000	False
small fraction of training	0.0	0.0	0.0	2.0	0.0000000000	False
fraction of training examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples because mostly alpha	0.0	0.0	0.0	2.0	0.0000000000	False
summing up the sum	0.0	0.0	0.0	2.0	0.0000000000	False
fraction of your training	0.0	0.0	0.0	2.0	0.0000000000	False
make much more sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense when we talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about kernels quick	0.0	0.0	0.0	2.0	0.0000000000	False
questions before i close	0.0	0.0	0.0	2.0	0.0000000000	False
ve done the work	0.0	0.0	0.0	2.0	0.0000000000	False
today s lecture asks	0.0	0.0	0.0	0.0	0.0000000000	False
generalize this in multiple	0.0	0.0	0.0	2.0	0.0000000000	False
ll talk about kernels	0.0	0.0	0.0	2.0	0.0000000000	False
good morning welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
reminder  i ve	0.0	0.0	0.0	0.0	0.0000000000	False
ve actually seen project	0.0	0.0	0.0	2.0	0.0000000000	False
proposals start to trickle	0.0	0.0	0.0	2.0	0.0000000000	False
great as a reminder	0.0	0.0	0.0	2.0	0.0000000000	False
chat more about project	0.0	0.0	0.0	2.0	0.0000000000	False
hours immediately after lecture	0.0	0.0	0.0	2.0	0.0000000000	False
immediately after lecture today	0.0	0.0	0.0	2.0	0.0000000000	False
started today ? great	0.0	0.0	0.0	2.0	0.0000000000	False
great okay welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
wrap up our discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on support vector	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the idea	0.0	0.0	0.0	4.0	0.0000000000	False
kernels and then talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the smo	0.0	0.0	0.0	2.0	0.0000000000	False
solving the optimization problem	0.0	0.0	1.99833147942	6.0	0.0000000000	False
problem that we posed	0.0	0.0	0.0	2.0	0.0000000000	False
last time to recap	0.0	0.0	0.0	2.0	0.0000000000	False
assuming that the data	0.0	0.0	0.0	2.0	0.0000000000	False
find the optimal margin	0.0	0.0	0.0	4.0	0.0000000000	False
classifier for the data	0.0	0.0	0.0	2.0	0.0000000000	False
data set that maximizes	0.0	0.0	0.0	2.0	0.0000000000	False
maximizes this geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin from your training	0.0	0.0	0.0	2.0	0.0000000000	False
dual of this problem	0.0	0.0	0.0	2.0	0.0000000000	False
angle brackets to denote	0.0	0.0	0.0	2.0	0.0000000000	False
transpose xj for vectors	0.0	0.0	0.0	2.0	0.0000000000	False
worked out the ways	0.0	0.0	0.0	2.0	0.0000000000	False
sum over i alpha	0.0	0.0	3.9977753059	8.0	0.4180790960	False
value of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
threshold function that outputs	0.0	0.0	0.0	2.0	0.0000000000	False
terms of inner products	0.0	0.0	2.9977753059	8.0	0.0000000000	False
products between input vectors	0.0	0.0	0.0	2.0	0.0000000000	False
property because it turns	0.0	0.0	0.0	2.0	0.0000000000	False
dependers of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
write the entire algorithm	0.0	0.0	0.0	4.0	0.0000000000	False
vector between input feature	0.0	0.0	0.0	2.0	0.0000000000	False
vectors and the idea	0.0	0.0	0.0	2.0	0.0000000000	False
area of a house	0.0	0.0	0.0	2.0	0.0000000000	False
house that you re	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to make	0.0	0.0	0.0	2.0	0.0000000000	False
ll take this feature	0.0	0.0	0.0	2.0	0.0000000000	False
richer set of features	0.0	0.0	0.0	2.0	0.0000000000	False
acutely call this mapping	0.0	0.0	0.0	2.0	0.0000000000	False
call this mapping phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi of x denote	0.0	0.0	0.0	2.0	0.0000000000	False
dimensional set of features	0.0	0.0	0.0	2.0	0.0000000000	False
back to the learning	0.0	0.0	0.0	2.0	0.0000000000	False
running a support vector	0.0	0.0	3.99833147942	6.0	0.0000000000	False
machine with the features	0.0	0.0	0.0	2.0	0.0000000000	False
features given by phi	0.0	0.0	0.0	2.0	0.0000000000	False
original one-dimensional input feature	0.0	0.0	0.0	2.0	0.0000000000	False
high degree polynomial features	0.0	0.0	0.0	2.0	0.0000000000	False
polynomial features sometimes phi	0.0	0.0	0.0	2.0	0.0000000000	False
dimensional vector of features	0.0	0.0	0.0	2.0	0.0000000000	False
question is if phi	0.0	0.0	0.0	2.0	0.0000000000	False
computers need to represent	0.0	0.0	0.0	2.0	0.0000000000	False
extremely high dimensional feature	0.0	0.0	0.0	2.0	0.0000000000	False
high dimensional feature vector	0.0	0.0	2.9977753059	8.0	0.0000000000	False
call the kernel function	0.0	0.0	0.0	2.0	0.0000000000	False
product between those feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature vectors it turns	0.0	0.0	0.0	4.0	0.0000000000	False
special cases where computing	0.0	0.0	0.0	2.0	0.0000000000	False
cases where computing phi	0.0	0.0	0.0	2.0	0.0000000000	False
compute infinite dimensional vectors	0.0	0.0	0.0	2.0	0.0000000000	False
special cases where phi	0.0	0.0	0.0	2.0	0.0000000000	False
compute the inner product	0.0	0.0	6.9977753059	8.0	0.5285714286	False
two vectors very inexpensively	0.0	0.0	0.0	2.0	0.0000000000	False
idea of the support	0.0	0.0	0.0	2.0	0.0000000000	False
re going to replace	0.0	0.0	0.0	2.0	0.0000000000	False
work in feature spaces	0.0	0.0	0.0	2.0	0.0000000000	False
concrete examples of phi	0.0	0.0	0.0	2.0	0.0000000000	False
explicitly this best illustrates	0.0	0.0	0.0	2.0	0.0000000000	False
right ? x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
thing is x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
corresponds to the feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature mapping where phi	0.0	0.0	0.0	2.0	0.0000000000	False
case of n equals	0.0	0.0	0.0	2.0	0.0000000000	False
product between two vectors	0.0	0.0	3.99833147942	6.0	0.0000000000	False
elements of the vectors	0.0	0.0	0.0	4.0	0.0000000000	False
elements of this vector	0.0	0.0	2.9977753059	8.0	0.0000000000	False
times the corresponding elements	0.0	0.0	0.0	2.0	0.0000000000	False
order to compute phi	0.0	0.0	0.0	2.0	0.0000000000	False
vector of all pairs	0.0	0.0	0.0	2.0	0.0000000000	False
squared you need order	0.0	0.0	0.0	2.0	0.0000000000	False
compute the kernel function	0.0	0.0	0.0	2.0	0.0000000000	False
kernel function is defined	0.0	0.0	0.0	2.0	0.0000000000	False
defined as x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
ve computed this kernel	0.0	0.0	0.0	2.0	0.0000000000	False
computed this kernel function	0.0	0.0	0.0	2.0	0.0000000000	False
vectors where each vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector has n squared	0.0	0.0	0.0	2.0	0.0000000000	False
kernel later please raise	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	0.0	2.0	0.0000000000	False
couple of quick generalizations	0.0	0.0	0.0	2.0	0.0000000000	False
equal to x transpose	0.0	0.0	0.0	2.0	0.0000000000	False
turns out to correspond	0.0	0.0	0.0	2.0	0.0000000000	False
correspond to a feature	0.0	0.0	0.0	4.0	0.0000000000	False
elements at the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
bottom where you add	0.0	0.0	0.0	2.0	0.0000000000	False
creating a feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
meaning the first order	0.0	0.0	0.0	2.0	0.0000000000	False
control the relative waiting	0.0	0.0	0.0	2.0	0.0000000000	False
features of all monomials	0.0	0.0	0.0	2.0	0.0000000000	False
terms up to degree	0.0	0.0	0.0	2.0	0.0000000000	False
implicitly construct the feature	0.0	0.0	0.0	2.0	0.0000000000	False
construct the feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
number to the power	0.0	0.0	0.0	2.0	0.0000000000	False
extremely high dimensional computing	0.0	0.0	0.0	2.0	0.0000000000	False
high dimensional computing space	0.0	0.0	0.0	2.0	0.0000000000	False
specific examples of kernels	0.0	0.0	0.0	2.0	0.0000000000	False
generally if you re	0.0	0.0	0.0	0.0	0.0000000000	False
intuition that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
feature vector of phi	0.0	0.0	0.0	2.0	0.0000000000	False
input feature vector phi	0.0	0.0	0.0	2.0	0.0000000000	False
nt as rigorous intuition	0.0	0.0	0.0	0.0	0.0000000000	False
product would be large	0.0	0.0	0.0	2.0	0.0000000000	False
large whereas in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
product may be small	0.0	0.0	0.0	2.0	0.0000000000	False
give you some random	0.0	0.0	0.0	2.0	0.0000000000	False
random thing to classify	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to classify	0.0	0.0	0.0	4.0	0.0000000000	False
thing  you re	0.0	0.0	0.0	0.0	0.0000000000	False
classify or dna sequences	0.0	0.0	0.0	2.0	0.0000000000	False
write down the function	0.0	0.0	0.0	4.0	0.0000000000	False
write down this function	0.0	0.0	0.0	2.0	0.0000000000	False
phi such that kxz	0.0	0.0	0.0	4.0	0.0000000000	False
valid kernel it turns	0.0	0.0	0.0	2.0	0.0000000000	False
part of that result	0.0	0.0	0.0	2.0	0.0000000000	False
exist some function phi	0.0	0.0	0.0	2.0	0.0000000000	False
matrix k i apologize	0.0	0.0	0.0	2.0	0.0000000000	False
apologize for overloading notation	0.0	0.0	0.0	2.0	0.0000000000	False
denote both the kernel	0.0	0.0	0.0	2.0	0.0000000000	False
find the kernel matrix	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the kernel	0.0	0.0	0.0	2.0	0.0000000000	False
two of my examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples then it turns	0.0	0.0	0.0	2.0	0.0000000000	False
transpose kz by definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition of matrix multiplication	0.0	0.0	0.0	2.0	0.0000000000	False
kij is a kernel	0.0	0.0	0.0	2.0	0.0000000000	False
exist such a value	0.0	0.0	0.0	2.0	0.0000000000	False
product between two feature	0.0	0.0	0.0	2.0	0.0000000000	False
make that inner product	0.0	0.0	0.0	2.0	0.0000000000	False
sum over the elements	0.0	0.0	0.0	2.0	0.0000000000	False
denote the k element	0.0	0.0	0.0	2.0	0.0000000000	False
vector just rearrange sums	0.0	0.0	0.0	2.0	0.0000000000	False
sums you get sum	0.0	0.0	0.0	2.0	0.0000000000	False
steps and just make	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you buy	0.0	0.0	0.0	2.0	0.0000000000	False
product between the vector	0.0	0.0	0.0	2.0	0.0000000000	False
vectors is the sum	0.0	0.0	0.0	2.0	0.0000000000	False
transpose b equals sum	0.0	0.0	0.0	2.0	0.0000000000	False
make sure it makes	0.0	0.0	0.0	2.0	0.0000000000	False
definitions of a matrix	0.0	0.0	0.0	2.0	0.0000000000	False
matrix k being posisemidefinite	0.0	0.0	0.0	2.0	0.0000000000	False
posisemidefinite when a matrix	0.0	0.0	0.0	2.0	0.0000000000	False
matrix must be posisemidefinite	0.0	0.0	0.0	2.0	0.0000000000	False
out that the converse	0.0	0.0	0.0	2.0	0.0000000000	False
theorem due to mercer	0.0	0.0	0.0	2.0	0.0000000000	False
mercer kernels it means	0.0	0.0	0.0	2.0	0.0000000000	False
means the same thing	0.0	0.0	0.0	2.0	0.0000000000	False
thing it just means	0.0	0.0	0.0	2.0	0.0000000000	False
phi of x transpose	0.0	0.0	0.0	4.0	0.0000000000	False
set of m examples	0.0	0.0	0.0	2.0	0.0000000000	False
means for any set	0.0	0.0	0.0	2.0	0.0000000000	False
set of m points	0.0	0.0	0.0	4.0	0.0000000000	False
necessarily a training set	0.0	0.0	0.0	2.0	0.0000000000	False
points you may choose	0.0	0.0	0.0	2.0	0.0000000000	False
true that the kernel	0.0	0.0	0.0	2.0	0.0000000000	False
proved only one direction	0.0	0.0	0.0	2.0	0.0000000000	False
direction of this result	0.0	0.0	0.0	2.0	0.0000000000	False
nt show it turns	0.0	0.0	0.0	0.0	0.0000000000	False
choose is a kernel	0.0	0.0	0.0	2.0	0.0000000000	False
functions that will fail	0.0	0.0	0.0	2.0	0.0000000000	False
conditions of this theorem	0.0	0.0	0.0	2.0	0.0000000000	False
products of a vector	0.0	0.0	0.0	2.0	0.0000000000	False
explicitly to an svm	0.0	0.0	0.0	2.0	0.0000000000	False
machine with a kernel	0.0	0.0	0.0	2.0	0.0000000000	False
turns out that function	0.0	0.0	0.0	2.0	0.0000000000	False
galceans so you choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose some kernel function	0.0	0.0	0.0	2.0	0.0000000000	False
apply a support vector	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine kernel	0.0	0.0	0.0	2.0	0.0000000000	False
depend on your problem	0.0	0.0	0.0	2.0	0.0000000000	False
back to our formulation	0.0	0.0	0.0	2.0	0.0000000000	False
formulation of support vector	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine algorithm	0.0	0.0	0.0	2.0	0.0000000000	True
ve taken a support	0.0	0.0	0.0	2.0	0.0000000000	False
machine and you ve	0.0	0.0	0.0	0.0	0.0000000000	False
infinite dimensional feature vectors	0.0	0.0	0.0	4.0	0.0000000000	False
dimensional feature vectors explicitly	0.0	0.0	0.0	2.0	0.0000000000	False
idea ? it turns	0.0	0.0	0.0	2.0	0.0000000000	False
talking about support vector	0.0	0.0	0.0	2.0	0.0000000000	False
vector machines i started	0.0	0.0	0.0	2.0	0.0000000000	False
develop non-linear learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
nt mean to draw	0.0	0.0	0.0	0.0	0.0000000000	False
takes your original input	0.0	0.0	0.0	2.0	0.0000000000	False
input data and maps	0.0	0.0	0.0	2.0	0.0000000000	False
high dimensional feature space	0.0	0.0	3.99833147942	6.0	0.0000000000	False
space in the case	0.0	0.0	0.0	2.0	0.0000000000	False
case of galcean kernels	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature space	0.0	0.0	0.0	4.0	0.0000000000	False
ll draw two dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
takes all your data	0.0	0.0	0.0	2.0	0.0000000000	False
machine in this infinite	0.0	0.0	0.0	2.0	0.0000000000	False
exponentially high dimensional space	0.0	0.0	0.0	2.0	0.0000000000	False
largest possible geometric margin	0.0	0.0	0.0	2.0	0.0000000000	False
originally one dimensional space	0.0	0.0	0.0	2.0	0.0000000000	False
classifier to which data	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machines output	0.0	0.0	0.0	2.0	0.0000000000	False
machines output nonlinear decision	0.0	0.0	0.0	2.0	0.0000000000	False
output nonlinear decision boundaries	0.0	0.0	0.0	2.0	0.0000000000	False
solve complex optimization problems	0.0	0.0	0.0	2.0	0.0000000000	False
complex optimization problems questions	0.0	0.0	0.0	2.0	0.0000000000	False
amount of your data	0.0	0.0	0.0	2.0	0.0000000000	False
thirds of your data	0.0	0.0	0.0	2.0	0.0000000000	False
data try different values	0.0	0.0	0.0	2.0	0.0000000000	False
separate hold out cross	0.0	0.0	0.0	2.0	0.0000000000	False
hold out cross validation	0.0	0.0	0.0	2.0	0.0000000000	False
out cross validation set	0.0	0.0	0.0	2.0	0.0000000000	False
set that you re	0.0	0.0	0.0	0.0	0.0000000000	False
testing something about learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms we talked	0.0	0.0	0.0	2.0	0.0000000000	False
locally linear aggressions bandwidth	0.0	0.0	0.0	2.0	0.0000000000	False
linear aggressions bandwidth parameter	0.0	0.0	0.0	2.0	0.0000000000	False
choose ids by saving	0.0	0.0	0.0	2.0	0.0000000000	False
saving aside some data	0.0	0.0	0.0	2.0	0.0000000000	False
talk more about model	0.0	0.0	0.0	2.0	0.0000000000	False
separation ? good question	0.0	0.0	0.0	2.0	0.0000000000	False
separable if you tend	0.0	0.0	0.0	2.0	0.0000000000	False
linearly separated by mapping	0.0	0.0	0.0	2.0	0.0000000000	False
mapping a higher dimension	0.0	0.0	0.0	2.0	0.0000000000	False
work with a discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion of soft margin	0.0	0.0	0.0	2.0	0.0000000000	False
run an svm algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
svm algorithm that assumes	0.0	0.0	0.0	2.0	0.0000000000	False
linearly separable on data	0.0	0.0	0.0	2.0	0.0000000000	False
separable ? you guys	0.0	0.0	0.0	2.0	0.0000000000	False
guys are really giving	0.0	0.0	0.0	2.0	0.0000000000	False
data s linearly separable	0.0	0.0	0.0	0.0	0.0000000000	False
linearly separable it turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns out this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm wo nt work	0.0	0.0	0.0	0.0	0.0000000000	False
work if the data	0.0	0.0	0.0	2.0	0.0000000000	False
work if i move	0.0	0.0	0.0	2.0	0.0000000000	False
move on to talk	0.0	0.0	0.0	2.0	0.0000000000	False
final word about kernels	0.0	0.0	0.0	2.0	0.0000000000	False
kernels in a context	0.0	0.0	0.0	2.0	0.0000000000	False
context of support vector	0.0	0.0	0.0	2.0	0.0000000000	False
made support vector machines	0.0	0.0	0.0	2.0	0.0000000000	False
out that the idea	0.0	0.0	0.0	2.0	0.0000000000	False
general than support vector	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm and we derived	0.0	0.0	0.0	2.0	0.0000000000	False
entire algorithm in terms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms that you ve	0.0	0.0	0.0	0.0	0.0000000000	False
class  in fact	0.0	0.0	0.0	2.0	0.0000000000	False
linear algorithms we talked	0.0	0.0	0.0	2.0	0.0000000000	False
regression and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
means you can replace	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms and implicitly map	0.0	0.0	0.0	2.0	0.0000000000	False
implicitly map the features	0.0	0.0	0.0	2.0	0.0000000000	False
map the features vectors	0.0	0.0	0.0	2.0	0.0000000000	False
widely used with support	0.0	0.0	0.0	2.0	0.0000000000	False
write them in terms	0.0	0.0	0.0	2.0	0.0000000000	False
products and thereby kernalize	0.0	0.0	0.0	2.0	0.0000000000	False
apply them to infinite	0.0	0.0	0.0	2.0	0.0000000000	False
set let s talk	0.0	0.0	0.0	0.0	0.0000000000	False
talk about non-linear decision	0.0	0.0	0.0	2.0	0.0000000000	False
norm soft margin svm	0.0	0.0	3.99833147942	6.0	0.0000000000	False
soft margin svm machine	0.0	0.0	0.0	2.0	0.0000000000	False
svm machine only people	0.0	0.0	0.0	2.0	0.0000000000	False
nt great at coming	0.0	0.0	0.0	0.0	0.0000000000	False
linearly separable data set	0.0	0.0	0.0	4.0	0.0000000000	False
couple of other examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples there that makes	0.0	0.0	0.0	2.0	0.0000000000	False
decision boundary that separates	0.0	0.0	0.0	2.0	0.0000000000	False
linearly separate this data	0.0	0.0	0.0	2.0	0.0000000000	False
separate this data set	0.0	0.0	0.0	2.0	0.0000000000	False
slightly suspicious example skew	0.0	0.0	0.0	2.0	0.0000000000	False
skew my entire decision	0.0	0.0	0.0	2.0	0.0000000000	False
boundary by a lot	0.0	0.0	0.0	2.0	0.0000000000	False
formulation of the svm	0.0	0.0	0.0	2.0	0.0000000000	False
choose that original decision	0.0	0.0	0.0	2.0	0.0000000000	False
formulation our svm primal	0.0	0.0	0.0	2.0	0.0000000000	False
problem was to minimize	0.0	0.0	0.0	2.0	0.0000000000	False
minimize one-half w squared	0.0	0.0	0.0	2.0	0.0000000000	False
modify this by adding	0.0	0.0	0.0	2.0	0.0000000000	False
add these penalty terms	0.0	0.0	0.0	2.0	0.0000000000	False
training examples is separated	0.0	0.0	0.0	2.0	0.0000000000	False
separated with functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
equal to one minus	0.0	0.0	0.0	2.0	0.0000000000	False
misclassified it by setting	0.0	0.0	0.0	2.0	0.0000000000	False
examples with functional margin	0.0	0.0	0.0	2.0	0.0000000000	False
examples of the training	0.0	0.0	0.0	2.0	0.0000000000	False
ll encourage the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
adding to the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
sort of penalty term	0.0	0.0	0.0	2.0	0.0000000000	False
penalty term that penalizes	0.0	0.0	0.0	2.0	0.0000000000	False
term that penalizes setting	0.0	0.0	0.0	2.0	0.0000000000	False
cis to be large	0.0	0.0	0.0	4.0	0.0000000000	False
problem where the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem it turns	0.0	0.0	0.0	4.0	0.0000000000	False
dual of the support	0.0	0.0	0.0	2.0	0.0000000000	False
dual for this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
show you the steps	0.0	0.0	0.0	2.0	0.0000000000	False
optimization objective minus sum	0.0	0.0	0.0	2.0	0.0000000000	False
minus or plus alpha	0.0	0.0	0.0	2.0	0.0000000000	False
dual of this optimization	0.0	0.0	0.0	2.0	0.0000000000	False
out when you derive	0.0	0.0	0.0	2.0	0.0000000000	False
constraint that the alpha	0.0	0.0	5.99721913237	10.0	0.3752535497	False
essentially the same math	0.0	0.0	0.0	2.0	0.0000000000	False
constraints of the alphas	0.0	0.0	0.0	2.0	0.0000000000	False
out that  remember	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down the conditions	0.0	0.0	0.0	2.0	0.0000000000	False
lecture the necessary conditions	0.0	0.0	0.0	2.0	0.0000000000	False
optimal solution to constrain	0.0	0.0	0.0	2.0	0.0000000000	False
solution to constrain optimization	0.0	0.0	0.0	2.0	0.0000000000	False
solve this optimization problem	0.0	0.0	7.9977753059	8.0	0.0000000000	False
optimum ? it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out from the conditions	0.0	0.0	0.0	2.0	0.0000000000	False
conditions you can derive	0.0	0.0	0.0	2.0	0.0000000000	False
conditions for an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the alphas	0.0	0.0	0.0	2.0	0.0000000000	False
handle non-linearly separable data	0.0	0.0	0.0	2.0	0.0000000000	False
non-linearly separable data sets	0.0	0.0	0.0	2.0	0.0000000000	False
choose not to separate	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this stuff	0.0	0.0	0.0	2.0	0.0000000000	False
sense at all great	0.0	0.0	0.0	2.0	0.0000000000	False
talk about an algorithm	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm for actually solving	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem we wrote	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem with convergence	0.0	0.0	0.0	2.0	0.0000000000	False
problem with convergence criteria	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to actually solve	0.0	0.0	0.0	2.0	0.0000000000	False
give me an excuse	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm called coordinate assent	0.0	0.0	0.0	4.0	0.0000000000	False
apply in the simplest	0.0	0.0	0.0	2.0	0.0000000000	False
form to this problem	0.0	0.0	0.0	2.0	0.0000000000	False
efficient algorithm for solving	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm like the smo	0.0	0.0	0.0	2.0	0.0000000000	False
talk about coordinate assent	0.0	0.0	0.0	2.0	0.0000000000	False
optimization algorithm to describe	0.0	0.0	0.0	2.0	0.0000000000	False
alpha one through alpha	0.0	0.0	0.0	4.0	0.0000000000	False
forget about the constraint	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm it will repeat	0.0	0.0	0.0	2.0	0.0000000000	False
coordinate assent essentially holds	0.0	0.0	0.0	2.0	0.0000000000	False
holds all the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters let me write	0.0	0.0	0.0	2.0	0.0000000000	False
write that as alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha i gets updated	0.0	0.0	0.0	2.0	0.0000000000	False
updated as over alpha	0.0	0.0	0.0	2.0	0.0000000000	False
hat of w alpha	0.0	0.0	0.0	2.0	0.0000000000	False
hold everything except alpha	0.0	0.0	0.0	2.0	0.0000000000	False
optimize w by optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization objective with respect	0.0	0.0	0.0	2.0	0.0000000000	False
respect to only alpha	0.0	0.0	0.0	2.0	0.0000000000	False
fancy way of writing	0.0	0.0	0.0	2.0	0.0000000000	False
coordinate assent one picture	0.0	0.0	0.0	2.0	0.0000000000	False
picture that s kind	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to optimize	0.0	0.0	0.0	4.0	0.0000000000	False
optimize a quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
function and the minimums	0.0	0.0	0.0	2.0	0.0000000000	False
ll call this alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha one my alpha	0.0	0.0	0.0	2.0	0.0000000000	False
minimizing this with respect	0.0	0.0	0.0	2.0	0.0000000000	False
ll minimize with respect	0.0	0.0	0.0	4.0	0.0000000000	False
taking these axis-aligned steps	0.0	0.0	0.0	2.0	0.0000000000	False
order we always optimize	0.0	0.0	0.0	2.0	0.0000000000	False
alpha one then alpha	0.0	0.0	0.0	2.0	0.0000000000	False
choose to always visit	0.0	0.0	0.0	2.0	0.0000000000	False
order you may choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose which alphas update	0.0	0.0	0.0	2.0	0.0000000000	False
alphas update next depending	0.0	0.0	0.0	2.0	0.0000000000	False
make the most progress	0.0	0.0	0.0	2.0	0.0000000000	False
makes sense to alternate	0.0	0.0	0.0	2.0	0.0000000000	False
progress towards the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
out that coordinate assent	0.0	0.0	0.0	4.0	0.0000000000	False
advantage of coordinate assent	0.0	0.0	0.0	2.0	0.0000000000	False
assent when it works	0.0	0.0	0.0	2.0	0.0000000000	False
optimize w with respect	0.0	0.0	2.99833147942	6.0	0.0000000000	False
group of coordinate assent	0.0	0.0	0.0	2.0	0.0000000000	False
coordinate assent with optimizing	0.0	0.0	0.0	2.0	0.0000000000	False
true when we modify	0.0	0.0	0.0	2.0	0.0000000000	False
solve the svm optimization	0.0	0.0	0.0	2.0	0.0000000000	False
svm optimization problem questions	0.0	0.0	0.0	2.0	0.0000000000	False
vector machine dual optimization	0.0	0.0	0.0	2.0	0.0000000000	False
machine dual optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
form does not work	0.0	0.0	0.0	2.0	0.0000000000	False
constrains on the alpha	0.0	0.0	0.0	2.0	0.0000000000	False
constraint that the sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum of y alpha	0.0	0.0	0.0	2.0	0.0000000000	False
fix all the alphas	0.0	0.0	0.0	2.0	0.0000000000	False
nt change one alpha	0.0	0.0	0.0	0.0	0.0000000000	False
determined as a function	0.0	0.0	0.0	2.0	0.0000000000	False
due to john platt	0.0	0.0	0.0	2.0	0.0000000000	False
microsoft the smo algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
optimization and the term	0.0	0.0	0.0	2.0	0.0000000000	False
refers to the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact that we re	0.0	0.0	0.0	0.0	0.0000000000	False
choosing the smallest number	0.0	0.0	0.0	2.0	0.0000000000	False
smallest number of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha is to change	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm we will select	0.0	0.0	0.0	2.0	0.0000000000	False
two alphas to change	0.0	0.0	0.0	2.0	0.0000000000	False
alpha i and alpha	0.0	0.0	8.99388209121	22.0	0.2574320051	False
thumb we ll hold	0.0	0.0	0.0	0.0	0.0000000000	False
hold all the alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha j and optimize	0.0	0.0	0.0	2.0	0.0000000000	False
out the key step	0.0	0.0	0.0	2.0	0.0000000000	False
optimize w of alpha	0.0	0.0	0.0	4.0	0.0000000000	False
subject to the constraints	0.0	0.0	0.0	2.0	0.0000000000	False
satisfied these convergence criteria	0.0	0.0	0.0	2.0	0.0000000000	False
criteria up to epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
efficiently that the smo	0.0	0.0	0.0	2.0	0.0000000000	False
large number of iterations	0.0	0.0	0.0	2.0	0.0000000000	False
iteration is very cheap	0.0	0.0	0.0	2.0	0.0000000000	False
cheap let s talk	0.0	0.0	0.0	0.0	0.0000000000	False
step where we update	0.0	0.0	0.0	2.0	0.0000000000	False
alpha one and alpha	0.0	0.0	11.9949944383	18.0	0.3019038985	False
notation on the board	0.0	0.0	0.0	2.0	0.0000000000	False
analogous on every step	0.0	0.0	0.0	2.0	0.0000000000	False
step of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem this means	0.0	0.0	0.0	2.0	0.0000000000	False
constraints on our dual	0.0	0.0	0.0	2.0	0.0000000000	False
constraint that the values	0.0	0.0	0.0	2.0	0.0000000000	False
alpha two must lie	0.0	0.0	0.0	2.0	0.0000000000	False
lie within this box	0.0	0.0	0.0	2.0	0.0000000000	False
picture of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
two y2 must equal	0.0	0.0	0.0	2.0	0.0000000000	False
equal to zeta minus	0.0	0.0	0.0	4.0	0.0000000000	False
plug in my definition	0.0	0.0	0.0	2.0	0.0000000000	False
alphas  it turns	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic function of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
two if you hold	0.0	0.0	0.0	2.0	0.0000000000	False
simplified to some expression	0.0	0.0	0.0	2.0	0.0000000000	False
expression of the form	0.0	0.0	0.0	2.0	0.0000000000	False
squared plus b alpha	0.0	0.0	0.0	2.0	0.0000000000	False
high school or undergrad	0.0	0.0	0.0	2.0	0.0000000000	False
optimal value for alpha	0.0	0.0	0.0	2.0	0.0000000000	False
value for alpha two	0.0	0.0	0.0	2.0	0.0000000000	False
two the last step	0.0	0.0	0.0	2.0	0.0000000000	False
step with a bosk	0.0	0.0	0.0	2.0	0.0000000000	False
lie on this line	0.0	0.0	0.0	4.0	0.0000000000	False
ll be some sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
function over this line	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
lies in the box	0.0	0.0	0.0	2.0	0.0000000000	False
optimize your quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
solution just to map	0.0	0.0	0.0	2.0	0.0000000000	False
map it back inside	0.0	0.0	0.0	2.0	0.0000000000	False
back inside the box	0.0	0.0	0.0	2.0	0.0000000000	False
box that ll give	0.0	0.0	0.0	0.0	0.0000000000	False
quadratic optimization problem subject	0.0	0.0	0.0	2.0	0.0000000000	False
subject to your solution	0.0	0.0	0.0	2.0	0.0000000000	False
solution satisfying this box	0.0	0.0	0.0	2.0	0.0000000000	False
satisfying this box constraint	0.0	0.0	0.0	2.0	0.0000000000	False
box constraint and lying	0.0	0.0	0.0	2.0	0.0000000000	False
subject to the solution	0.0	0.0	0.0	2.0	0.0000000000	False
segment within the box	0.0	0.0	0.0	2.0	0.0000000000	False
back within the box	0.0	0.0	0.0	2.0	0.0000000000	False
makes the inner loop	0.0	0.0	0.0	2.0	0.0000000000	False
loop of the smo	0.0	0.0	0.0	2.0	0.0000000000	False
smo algorithm very efficient	0.0	0.0	0.0	2.0	0.0000000000	False
talking about ascent suppose	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum from i equals	0.0	0.0	0.0	4.0	0.0000000000	False
two to m alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha i yi divided	0.0	0.0	0.0	2.0	0.0000000000	False
alpha four through alpha	0.0	0.0	0.0	2.0	0.0000000000	False
choose to change alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha two must satisfy	0.0	0.0	0.0	2.0	0.0000000000	False
satisfy that linear constraint	0.0	0.0	0.0	2.0	0.0000000000	False
two accordingly to make	0.0	0.0	0.0	2.0	0.0000000000	False
make sure this satisfies	0.0	0.0	0.0	2.0	0.0000000000	False
setting of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
nt want to talk	0.0	0.0	0.0	0.0	0.0000000000	False
ll say a couple	0.0	0.0	0.0	2.0	0.0000000000	False
outline of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
iteration of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
re going to select	0.0	0.0	0.0	2.0	0.0000000000	False
alpha j to update	0.0	0.0	0.0	4.0	0.0000000000	False
procedure i just described	0.0	0.0	0.0	2.0	0.0000000000	False
described to actually update	0.0	0.0	0.0	2.0	0.0000000000	False
alpha is that function	0.0	0.0	0.0	2.0	0.0000000000	False
function we had previously	0.0	0.0	0.0	2.0	0.0000000000	False
previously w of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
alpha was the sum	0.0	0.0	0.0	2.0	0.0000000000	False
problem for the svm	0.0	0.0	0.0	2.0	0.0000000000	False
farther from its optimal	0.0	0.0	0.0	2.0	0.0000000000	False
optimal let me translate	0.0	0.0	0.0	2.0	0.0000000000	False
differently what we re	0.0	0.0	0.0	0.0	0.0000000000	False
optimize the objective function	0.0	0.0	0.0	2.0	0.0000000000	False
function w of alpha	0.0	0.0	0.0	2.0	0.0000000000	False
progress that we care	0.0	0.0	0.0	2.0	0.0000000000	False
true for coordinate assent	0.0	0.0	0.0	2.0	0.0000000000	False
assent and for smo	0.0	0.0	0.0	2.0	0.0000000000	False
alpha can only increase	0.0	0.0	0.0	2.0	0.0000000000	False
increase it may stay	0.0	0.0	0.0	2.0	0.0000000000	False
converge at some value	0.0	0.0	0.0	2.0	0.0000000000	False
true that in intervening	0.0	0.0	0.0	2.0	0.0000000000	False
true just a couple	0.0	0.0	0.0	2.0	0.0000000000	False
smo before i wrap	0.0	0.0	0.0	2.0	0.0000000000	False
platt s original algorithm	0.0	0.0	0.0	0.0	0.0000000000	False
john platt s paper	0.0	0.0	0.0	0.0	0.0000000000	False
paper on the smo	0.0	0.0	0.0	2.0	0.0000000000	False
pretty easy to read	0.0	0.0	0.0	2.0	0.0000000000	False
readings in more details	0.0	0.0	0.0	2.0	0.0000000000	False
details one other thing	0.0	0.0	0.0	2.0	0.0000000000	False
solving all your alphas	0.0	0.0	0.0	2.0	0.0000000000	False
ll let you read	0.0	0.0	0.0	2.0	0.0000000000	False
briefly about a couple	0.0	0.0	0.0	2.0	0.0000000000	False
handler s integer recognition	0.0	0.0	0.0	0.0	0.0000000000	False
integer recognition in handler	0.0	0.0	0.0	2.0	0.0000000000	False
re given a pixel	0.0	0.0	0.0	2.0	0.0000000000	False
array with a scanned	0.0	0.0	0.0	2.0	0.0000000000	False
code somewhere in britain	0.0	0.0	0.0	2.0	0.0000000000	False
character one the question	0.0	0.0	0.0	2.0	0.0000000000	False
ten pixels by ten	0.0	0.0	0.0	4.0	0.0000000000	False
pixels by ten pixels	0.0	0.0	0.0	4.0	0.0000000000	False
hundred dimensional feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
binary features of xb01	0.0	0.0	0.0	2.0	0.0000000000	False
out for many years	0.0	0.0	0.0	2.0	0.0000000000	False
champion algorithm for handler	0.0	0.0	0.0	2.0	0.0000000000	False
recognition and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
writing down this kernel	0.0	0.0	0.0	2.0	0.0000000000	False
neuronetworks this is surprising	0.0	0.0	0.0	2.0	0.0000000000	False
surprising because support vector	0.0	0.0	0.0	2.0	0.0000000000	False
nt take into account	0.0	0.0	0.0	0.0	0.0000000000	False
knowledge about the pixels	0.0	0.0	0.0	2.0	0.0000000000	False
representing the pixel intensity	0.0	0.0	0.0	2.0	0.0000000000	False
value as a vector	0.0	0.0	0.0	2.0	0.0000000000	False
shuffle all the pixels	0.0	0.0	0.0	2.0	0.0000000000	False
development for many years	0.0	0.0	0.0	2.0	0.0000000000	False
sequences into different classes	0.0	0.0	0.0	2.0	0.0000000000	False
biologists in the room	0.0	0.0	0.0	2.0	0.0000000000	False
proteins in our bodies	0.0	0.0	0.0	2.0	0.0000000000	False
made up by sequences	0.0	0.0	0.0	2.0	0.0000000000	False
sequences of amino acids	0.0	0.0	0.0	4.0	0.0000000000	False
acids by the alphabet	0.0	0.0	0.0	2.0	0.0000000000	False
apologizes to the biologists	0.0	0.0	0.0	2.0	0.0000000000	False
amino acid sequence represented	0.0	0.0	0.0	2.0	0.0000000000	False
represented by a series	0.0	0.0	0.0	2.0	0.0000000000	False
depending on what type	0.0	0.0	0.0	2.0	0.0000000000	False
construct my feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
challenging for many reasons	0.0	0.0	0.0	2.0	0.0000000000	False
nt have a feature	0.0	0.0	0.0	0.0	0.0000000000	False
position in this protein	0.0	0.0	0.0	2.0	0.0000000000	False
combinations of four alphabets	0.0	0.0	0.0	4.0	0.0000000000	False
aaac down to aaaz	0.0	0.0	0.0	2.0	0.0000000000	False
aaaz and then aaba	0.0	0.0	0.0	2.0	0.0000000000	False
alphabets and my feature	0.0	0.0	0.0	2.0	0.0000000000	False
scan through this sequence	0.0	0.0	0.0	2.0	0.0000000000	False
amino acids and count	0.0	0.0	0.0	2.0	0.0000000000	False
feature representation for protein	0.0	0.0	0.0	2.0	0.0000000000	False
protein this representation applies	0.0	0.0	0.0	2.0	0.0000000000	False
representation applies no matter	0.0	0.0	0.0	2.0	0.0000000000	False
long my protein sequence	0.0	0.0	0.0	2.0	0.0000000000	False
160,000 dimensional feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
dimensional feature vectors imagine	0.0	0.0	0.0	2.0	0.0000000000	False
examples and you store	0.0	0.0	0.0	2.0	0.0000000000	False
efficient dynamic programming algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
efficiently compute inner products	0.0	0.0	0.0	2.0	0.0000000000	False
products between these feature	0.0	0.0	0.0	2.0	0.0000000000	False
apply this feature representation	0.0	0.0	0.0	2.0	0.0000000000	False
ridiculously high feature vector	0.0	0.0	0.0	2.0	0.0000000000	False
feature vector to classify	0.0	0.0	0.0	2.0	0.0000000000	False
vector to classify protein	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for finding subsequences	0.0	0.0	0.0	2.0	0.0000000000	False
choose a standard kernel	0.0	0.0	0.0	2.0	0.0000000000	False
problem two last sentences	0.0	0.0	0.0	2.0	0.0000000000	False
effective off the shelf	0.0	0.0	0.0	2.0	0.0000000000	False
lot of learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
class by saying congrats	0.0	0.0	0.0	2.0	0.0000000000	False
re now well qualified	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms to a lot	0.0	0.0	0.0	2.0	0.0000000000	False
re still in week	0.0	0.0	0.0	2.0	0.0000000000	False
four of the quarter	0.0	0.0	0.0	2.0	0.0000000000	False
understand the learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
assume that your project	0.0	0.0	0.0	2.0	0.0000000000	False
working on your proposals	0.0	0.0	0.0	2.0	0.0000000000	False
working on your project	0.0	0.0	0.0	2.0	0.0000000000	False
session at the end	0.0	0.0	0.0	2.0	0.0000000000	False
end of the quarter	0.0	0.0	0.0	2.0	0.0000000000	False
start a new chapter	0.0	0.0	0.0	2.0	0.0000000000	False
talk about learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
learned about a lot	0.0	0.0	0.0	2.0	0.0000000000	False
lot of learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
powerful tools of machine	0.0	0.0	0.0	2.0	0.0000000000	False
tools of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
sort of well qualified	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to learn	0.0	0.0	0.0	2.0	0.0000000000	False
re going to carpentry	0.0	0.0	0.0	2.0	0.0000000000	False
carpentry school to learn	0.0	0.0	0.0	2.0	0.0000000000	False
tools if you learn	0.0	0.0	0.0	2.0	0.0000000000	False
walk in and pick	0.0	0.0	0.0	2.0	0.0000000000	False
pick up a tool	0.0	0.0	0.0	2.0	0.0000000000	False
give you a sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense of the mastery	0.0	0.0	0.0	2.0	0.0000000000	False
mastery of the machine	0.0	0.0	0.0	2.0	0.0000000000	False
deeply about the properties	0.0	0.0	0.0	2.0	0.0000000000	False
properties of different machine	0.0	0.0	0.0	2.0	0.0000000000	False
common scenarios in machine	0.0	0.0	0.0	2.0	0.0000000000	False
scenarios in machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
machine learning is someday	0.0	0.0	0.0	2.0	0.0000000000	False
ll be doing research	0.0	0.0	0.0	2.0	0.0000000000	False
research or a company	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms you learned	0.0	0.0	0.0	2.0	0.0000000000	False
people that really understand	0.0	0.0	0.0	2.0	0.0000000000	False
people that maybe read	0.0	0.0	0.0	2.0	0.0000000000	False
work through the math	0.0	0.0	0.0	2.0	0.0000000000	False
apply a support vector	0.0	0.0	0.0	2.0	0.0000000000	False
understand enough about support	0.0	0.0	0.0	2.0	0.0000000000	False
separates the great people	0.0	0.0	0.0	2.0	0.0000000000	False
great people in machine	0.0	0.0	0.0	2.0	0.0000000000	False
people in machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning versus the people	0.0	0.0	0.0	2.0	0.0000000000	False
people that like read	0.0	0.0	0.0	2.0	0.0000000000	False
read the text book	0.0	0.0	0.0	2.0	0.0000000000	False
ll have just understood	0.0	0.0	0.0	2.0	0.0000000000	False
ll start to talk	0.0	0.0	0.0	2.0	0.0000000000	False
theoretical results of machine	0.0	0.0	0.0	2.0	0.0000000000	False
results of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning the next lecture	0.0	0.0	0.0	2.0	0.0000000000	False
problems that the learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning theory will point	0.0	0.0	0.0	2.0	0.0000000000	False
first thing we re	0.0	0.0	0.0	0.0	0.0000000000	False
re gon na talk	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm we learned	0.0	0.0	0.0	2.0	0.0000000000	False
line through these datas	0.0	0.0	0.0	2.0	0.0000000000	False
structure in the data	0.0	0.0	0.0	4.0	0.0000000000	False
bias of the learning	0.0	0.0	0.0	4.0	0.0000000000	False
learning algorithm as representing	0.0	0.0	0.0	2.0	0.0000000000	False
infinite amount of training	0.0	0.0	0.0	4.0	0.0000000000	False
amount of training data	0.0	0.0	0.0	4.0	0.0000000000	False
tons of training data	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm would still fail	0.0	0.0	0.0	2.0	0.0000000000	False
fit the quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm with high bias	0.0	0.0	0.0	2.0	0.0000000000	False
dataset if you fit	0.0	0.0	0.0	2.0	0.0000000000	False
fourth of the polynomials	0.0	0.0	0.0	2.0	0.0000000000	False
polynomials into this dataset	0.0	0.0	0.0	2.0	0.0000000000	False
interpolate the five data	0.0	0.0	0.0	2.0	0.0000000000	False
model to the structure	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm has a problem	0.0	0.0	0.0	2.0	0.0000000000	False
alternatively that this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm has high variance	0.0	0.0	0.0	2.0	0.0000000000	False
overfitting a high variance	0.0	0.0	0.0	2.0	0.0000000000	False
patterns in the data	0.0	0.0	0.0	2.0	0.0000000000	False
dataset of housing prices	0.0	0.0	0.0	2.0	0.0000000000	False
happy medium of fitting	0.0	0.0	0.0	2.0	0.0000000000	False
fitting a quadratic function	0.0	0.0	0.0	2.0	0.0000000000	False
nt interpolate your data	0.0	0.0	0.0	0.0	0.0000000000	False
interpolate your data points	0.0	0.0	0.0	2.0	0.0000000000	False
multi-structure in your data	0.0	0.0	0.0	2.0	0.0000000000	False
model which under fits	0.0	0.0	0.0	2.0	0.0000000000	False
picture of classification problems	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative examples	0.0	0.0	0.0	2.0	0.0000000000	False
equals the sigmoid function	0.0	0.0	0.0	2.0	0.0000000000	False
applied to a tenth	0.0	0.0	0.0	2.0	0.0000000000	False
tenth of the polynomial	0.0	0.0	0.0	2.0	0.0000000000	False
boundary like this right	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative classes	0.0	0.0	0.0	2.0	0.0000000000	False
regression into this model	0.0	0.0	0.0	2.0	0.0000000000	False
problem of overfitting versus	0.0	0.0	0.0	2.0	0.0000000000	False
bias versus high variance	0.0	0.0	0.0	2.0	0.0000000000	False
formal model of machine	0.0	0.0	0.0	2.0	0.0000000000	False
model of machine learning	0.0	0.0	7.99783432593	8.0	0.0000000000	False
initial foray into learning	0.0	0.0	0.0	2.0	0.0000000000	False
foray into learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
talk about learning classification	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine lectures	0.0	0.0	0.0	2.0	0.0000000000	False
ll be a bit	0.0	0.0	0.0	2.0	0.0000000000	False
cleaner if i switch	0.0	0.0	0.0	2.0	0.0000000000	False
back to y equals	0.0	0.0	0.0	2.0	0.0000000000	False
model as a model	0.0	0.0	0.0	2.0	0.0000000000	False
forum as logistic regressions	0.0	0.0	0.0	2.0	0.0000000000	False
similar to logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
re going to force	0.0	0.0	0.0	2.0	0.0000000000	False
force the logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
involved in the probabilities	0.0	0.0	0.0	2.0	0.0000000000	False
re given a training	0.0	0.0	0.0	2.0	0.0000000000	False
set of m examples	0.0	0.0	0.0	4.0	0.0000000000	False
ranging from i equals	0.0	0.0	0.0	2.0	0.0000000000	False
assume that the training	0.0	0.0	0.0	2.0	0.0000000000	False
training example is xiyi	0.0	0.0	0.0	2.0	0.0000000000	False
xiyi i ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
identically and definitively distributed	0.0	0.0	0.0	2.0	0.0000000000	False
running a classification problem	0.0	0.0	0.0	2.0	0.0000000000	False
classification problem on houses	0.0	0.0	0.0	2.0	0.0000000000	False
features of the house	0.0	0.0	0.0	2.0	0.0000000000	False
house will be sold	0.0	0.0	0.0	2.0	0.0000000000	False
priority distribution over features	0.0	0.0	0.0	2.0	0.0000000000	False
assume that training examples	0.0	0.0	0.0	2.0	0.0000000000	False
training examples we ve	0.0	0.0	0.0	0.0	0.0000000000	False
examples we ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
iid from some probability	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to build	0.0	0.0	0.0	2.0	0.0000000000	False
build a spam classifier	0.0	0.0	0.0	2.0	0.0000000000	False
distribution of what emails	0.0	0.0	0.0	2.0	0.0000000000	False
simplify  to understand	0.0	0.0	0.0	2.0	0.0000000000	False
phenomena of bias invariance	0.0	0.0	0.0	2.0	0.0000000000	False
simplified model of machine	0.0	0.0	5.99837574445	6.0	0.0000000000	False
regression fits this parameters	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood but in order	0.0	0.0	0.0	2.0	0.0000000000	False
order to understand learning	0.0	0.0	0.0	2.0	0.0000000000	False
assume a simplified model	0.0	0.0	0.0	2.0	0.0000000000	False
error of a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis x subscript data	0.0	0.0	0.0	2.0	0.0000000000	False
data write this epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon hat of subscript	0.0	0.0	0.0	2.0	0.0000000000	False
hat of subscript data	0.0	0.0	0.0	2.0	0.0000000000	False
dependence on a training	0.0	0.0	0.0	2.0	0.0000000000	False
sum of indicator functions	0.0	0.0	0.0	2.0	0.0000000000	False
fraction of training examples	0.0	0.0	0.0	4.0	0.0000000000	False
training examples your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
examples your hypothesis classifies	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis classifies so defined	0.0	0.0	0.0	2.0	0.0000000000	False
defined as a training	0.0	0.0	0.0	2.0	0.0000000000	False
training error and training	0.0	0.0	0.0	2.0	0.0000000000	False
error and training error	0.0	0.0	0.0	2.0	0.0000000000	False
risk the simplified model	0.0	0.0	0.0	2.0	0.0000000000	False
minimize my training error	0.0	0.0	0.0	4.0	0.0000000000	False
minimizes your training error	0.0	0.0	0.0	2.0	0.0000000000	False
training error it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out that logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression and support	0.0	0.0	0.0	4.0	0.0000000000	False
regression and support vector	0.0	0.0	0.0	4.0	0.0000000000	False
formally viewed as approximation	0.0	0.0	0.0	2.0	0.0000000000	False
viewed as approximation cities	0.0	0.0	0.0	2.0	0.0000000000	False
solve this optimization problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem and logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
approximations to this nonconvex	0.0	0.0	0.0	2.0	0.0000000000	False
optimization problem by finding	0.0	0.0	0.0	2.0	0.0000000000	False
finding the convex approximation	0.0	0.0	0.0	2.0	0.0000000000	False
similar to what algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms like logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
definition of empirical risk	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm as not choosing	0.0	0.0	0.0	2.0	0.0000000000	False
define the hypothesis class	0.0	0.0	0.0	4.0	0.0000000000	False
class of all hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
words as the class	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm is choosing	0.0	0.0	0.0	2.0	0.0000000000	False
mapping from the input	0.0	0.0	0.0	2.0	0.0000000000	False
class of all functions	0.0	0.0	0.0	4.0	0.0000000000	False
logistic regression can choose	0.0	0.0	0.0	2.0	0.0000000000	False
redefine empirical risk minimization	0.0	0.0	0.0	2.0	0.0000000000	False
function into hypothesis class	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class of script	0.0	0.0	0.0	2.0	0.0000000000	False
script h that minimizes	0.0	0.0	0.0	2.0	0.0000000000	False
minimizes  that minimizes	0.0	0.0	0.0	2.0	0.0000000000	False
hand if it makes	0.0	0.0	0.0	2.0	0.0000000000	False
function from the class	0.0	0.0	0.0	2.0	0.0000000000	False
general case this set	0.0	0.0	0.0	2.0	0.0000000000	False
functions represented by viewer	0.0	0.0	0.0	2.0	0.0000000000	False
represented by viewer network	0.0	0.0	0.0	2.0	0.0000000000	False
functions the learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm wants to choose	0.0	0.0	0.0	2.0	0.0000000000	False
definition for empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
minimization will still apply	0.0	0.0	0.0	2.0	0.0000000000	False
understand whether empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
alex ? a function	0.0	0.0	0.0	2.0	0.0000000000	False
function that s defined	0.0	0.0	0.0	0.0	0.0000000000	False
question is h data	0.0	0.0	0.0	2.0	0.0000000000	False
purpose of this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
data is the class	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm or logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
mapping from the infa	0.0	0.0	0.0	2.0	0.0000000000	False
domain to the center	0.0	0.0	0.0	2.0	0.0000000000	False
center of class label	0.0	0.0	0.0	2.0	0.0000000000	False
perform empirical risk minimization	0.0	0.0	0.0	2.0	0.0000000000	False
minimization over any hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
class for the purpose	0.0	0.0	0.0	2.0	0.0000000000	False
restrict myself to talking	0.0	0.0	0.0	2.0	0.0000000000	False
talking about binary classification	0.0	0.0	0.0	2.0	0.0000000000	False
regression in other problem	0.0	0.0	0.0	2.0	0.0000000000	False
question ? yes cool	0.0	0.0	0.0	2.0	0.0000000000	False
right so i wan	0.0	0.0	0.0	2.0	0.0000000000	False
understand if empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
things we can prove	0.0	0.0	0.0	2.0	0.0000000000	False
care about training error	0.0	0.0	0.0	2.0	0.0000000000	False
predictions on the training	0.0	0.0	0.0	2.0	0.0000000000	False
goal the ultimate goal	0.0	0.0	0.0	2.0	0.0000000000	False
makes predictions on examples	0.0	0.0	0.0	2.0	0.0000000000	False
predicts prices or sale	0.0	0.0	0.0	2.0	0.0000000000	False
sale or no sale	0.0	0.0	0.0	2.0	0.0000000000	False
sale outcomes of houses	0.0	0.0	0.0	2.0	0.0000000000	False
care about is generalization	0.0	0.0	0.0	2.0	0.0000000000	False
defined as the probability	0.0	0.0	0.0	2.0	0.0000000000	False
terms of notational convention	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon hat training error	0.0	0.0	0.0	2.0	0.0000000000	False
error as an attempt	0.0	0.0	0.0	2.0	0.0000000000	False
attempt to approximate generalization	0.0	0.0	0.0	2.0	0.0000000000	False
things with the hats	0.0	0.0	0.0	2.0	0.0000000000	False
re using to estimate	0.0	0.0	0.0	2.0	0.0000000000	False
hat is a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis output by learning	0.0	0.0	0.0	2.0	0.0000000000	False
output by learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
estimate what the functions	0.0	0.0	0.0	2.0	0.0000000000	False
giving us low generalization	0.0	0.0	0.0	2.0	0.0000000000	False
care about in order	0.0	0.0	0.0	2.0	0.0000000000	False
prove our first learning	0.0	0.0	0.0	2.0	0.0000000000	False
first learning theory result	0.0	0.0	0.0	2.0	0.0000000000	False
first is the union	0.0	0.0	0.0	2.0	0.0000000000	False
events in a sense	0.0	0.0	0.0	2.0	0.0000000000	False
distribution over the events	0.0	0.0	0.0	2.0	0.0000000000	False
sort of just set	0.0	0.0	0.0	2.0	0.0000000000	False
set notation for probability	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the probability	0.0	0.0	0.0	4.0	0.0000000000	False
ve seen venn diagrams	0.0	0.0	0.0	2.0	0.0000000000	False
diagrams depictions of probability	0.0	0.0	0.0	2.0	0.0000000000	False
great  the probability	0.0	0.0	0.0	2.0	0.0000000000	False
mass in the union	0.0	0.0	0.0	2.0	0.0000000000	False
things to the sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum of the masses	0.0	0.0	0.0	2.0	0.0000000000	False
turns out that depending	0.0	0.0	0.0	2.0	0.0000000000	False
axioms that probably varies	0.0	0.0	0.0	2.0	0.0000000000	False
written as an axiom	0.0	0.0	0.0	2.0	0.0000000000	False
avitivity are probably measured	0.0	0.0	0.0	2.0	0.0000000000	False
commonly called the union	0.0	0.0	0.0	2.0	0.0000000000	False
variables with mean phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi so the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability of zi equals	0.0	0.0	0.0	2.0	0.0000000000	False
iid for newly random	0.0	0.0	0.0	2.0	0.0000000000	False
equals one through mzi	0.0	0.0	0.0	2.0	0.0000000000	False
random variables by sort	0.0	0.0	0.0	2.0	0.0000000000	False
true value of phi	0.0	0.0	0.0	2.0	0.0000000000	False
holds  this lemma	0.0	0.0	0.0	2.0	0.0000000000	False
variables you will remember	0.0	0.0	0.0	2.0	0.0000000000	False
undergraduate probability or statistics	0.0	0.0	0.0	2.0	0.0000000000	False
probability or statistics class	0.0	0.0	0.0	2.0	0.0000000000	False
average all the things	0.0	0.0	0.0	2.0	0.0000000000	False
coins with bias phi	0.0	0.0	0.0	2.0	0.0000000000	False
observe these m benuve	0.0	0.0	0.0	2.0	0.0000000000	False
probability distribution of phi	0.0	0.0	0.0	2.0	0.0000000000	False
distribution function of phi	0.0	0.0	0.0	2.0	0.0000000000	False
phi hat will converse	0.0	0.0	0.0	2.0	0.0000000000	False
discreet set of values	0.0	0.0	0.0	2.0	0.0000000000	False
roughly to a gaussian	0.0	0.0	0.0	2.0	0.0000000000	False
put s one interval	0.0	0.0	0.0	2.0	0.0000000000	False
mass of the details	0.0	0.0	0.0	2.0	0.0000000000	False
probability that my value	0.0	0.0	0.0	2.0	0.0000000000	False
mass in these tails	0.0	0.0	0.0	2.0	0.0000000000	False
negative two gamma squared	0.0	0.0	10.9967514889	12.0	0.4408352668	False
side of the bound	0.0	0.0	0.0	2.0	0.0000000000	False
two e to negative	0.0	0.0	0.0	2.0	0.0000000000	False
gamma squared so balance	0.0	0.0	0.0	2.0	0.0000000000	False
probability that you make	0.0	0.0	0.0	2.0	0.0000000000	False
variable and the cool	0.0	0.0	0.0	2.0	0.0000000000	False
thing about this bound	0.0	0.0	0.0	2.0	0.0000000000	False
thing behind this bound	0.0	0.0	0.0	2.0	0.0000000000	False
fixed value of gamma	0.0	0.0	0.0	4.0	0.0000000000	False
size of your training	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian will actually shrink	0.0	0.0	0.0	2.0	0.0000000000	False
left in the tails	0.0	0.0	0.0	2.0	0.0000000000	False
tend  are sort	0.0	0.0	0.0	2.0	0.0000000000	False
works for any finer	0.0	0.0	0.0	2.0	0.0000000000	False
central limit theorem approximation	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon to help explain	0.0	0.0	0.0	2.0	0.0000000000	False
reference to central limit	0.0	0.0	0.0	2.0	0.0000000000	False
limit theorem all right	0.0	0.0	0.0	2.0	0.0000000000	False
right so lets start	0.0	0.0	0.0	2.0	0.0000000000	False
lets start to understand	0.0	0.0	0.0	2.0	0.0000000000	False
understand empirical risk minimization	0.0	0.0	0.0	2.0	0.0000000000	False
studying empirical risk minimization	0.0	0.0	0.0	2.0	0.0000000000	False
minimization for a case	0.0	0.0	0.0	2.0	0.0000000000	False
case of finite hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
class of k hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
function mapping from inputs	0.0	0.0	0.0	2.0	0.0000000000	False
whichever of these functions	0.0	0.0	0.0	2.0	0.0000000000	False
continuous infinitely large class	0.0	0.0	0.0	2.0	0.0000000000	False
large class of hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
prove the first row	0.0	0.0	0.0	2.0	0.0000000000	False
describe our first learning	0.0	0.0	0.0	2.0	0.0000000000	False
classes so empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
empirical risk minimization takes	0.0	0.0	0.0	2.0	0.0000000000	False
minimization takes the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
bound on the generalization	0.0	0.0	2.99837574445	6.0	0.0000000000	False
error of h hat	0.0	0.0	5.9967514889	12.0	0.4408352668	False
prove that somehow minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
step in this prove	0.0	0.0	0.0	2.0	0.0000000000	False
show that training error	0.0	0.0	0.0	2.0	0.0000000000	False
good approximation to generalization	0.0	0.0	0.0	2.0	0.0000000000	False
approximation to generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
show that this implies	0.0	0.0	0.0	2.0	0.0000000000	False
error of the hypothesis	0.0	0.0	1.99783432593	8.0	0.5277777778	False
hypothesis of empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
slightly notation heavy class	0.0	0.0	0.0	2.0	0.0000000000	False
notation heavy class round	0.0	0.0	0.0	2.0	0.0000000000	False
set of new symbols	0.0	0.0	0.0	2.0	0.0000000000	False
understand what the notation	0.0	0.0	0.0	2.0	0.0000000000	False
notation i was defining	0.0	0.0	0.0	2.0	0.0000000000	False
errors that give approximation	0.0	0.0	0.0	2.0	0.0000000000	False
give approximation generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
imply that minimizing training	0.0	0.0	0.0	2.0	0.0000000000	False
pretty well in terms	0.0	0.0	0.0	2.0	0.0000000000	False
terms of minimizing generalization	0.0	0.0	0.0	2.0	0.0000000000	False
give us a bound	0.0	0.0	0.0	2.0	0.0000000000	False
output by empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
lets pick any hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
fixed hypothesis so pick	0.0	0.0	0.0	2.0	0.0000000000	False
pick any one hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis misclassifies the ife	0.0	0.0	0.0	2.0	0.0000000000	False
ife example  excuse	0.0	0.0	0.0	2.0	0.0000000000	False
training set is drawn	0.0	0.0	0.0	2.0	0.0000000000	False
drawn randomly from sum	0.0	0.0	0.0	2.0	0.0000000000	False
randomly from sum distribution	0.0	0.0	0.0	2.0	0.0000000000	False
depending on what training	0.0	0.0	0.0	2.0	0.0000000000	False
training examples i ve	0.0	0.0	0.0	0.0	0.0000000000	False
out what the probability	0.0	0.0	0.0	2.0	0.0000000000	False
takes on the value	0.0	0.0	0.0	2.0	0.0000000000	False
sample my training set	0.0	0.0	0.0	2.0	0.0000000000	False
set iid from distribution	0.0	0.0	0.0	2.0	0.0000000000	False
chance that my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
error of my hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
error of this hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis raise your hand	0.0	0.0	0.0	2.0	0.0000000000	False
hand if that made	0.0	0.0	0.0	2.0	0.0000000000	False
examples i ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
ve drawn are iid	0.0	0.0	0.0	2.0	0.0000000000	False
training examples were drawn	0.0	0.0	0.0	2.0	0.0000000000	False
assumption if you read	0.0	0.0	0.0	2.0	0.0000000000	False
definition of training error	0.0	0.0	0.0	2.0	0.0000000000	False
average of my zis	0.0	0.0	0.0	2.0	0.0000000000	False
drawn from benuve distribution	0.0	0.0	0.0	2.0	0.0000000000	False
average of miid benuve	0.0	0.0	0.0	2.0	0.0000000000	False
miid benuve random variables	0.0	0.0	0.0	2.0	0.0000000000	False
probability that the difference	0.0	0.0	0.0	2.0	0.0000000000	False
training and generalization error	0.0	0.0	0.0	4.0	0.0000000000	False
large than this thing	0.0	0.0	0.0	2.0	0.0000000000	False
thing on the right	0.0	0.0	0.0	4.0	0.0000000000	False
probability my training error	0.0	0.0	0.0	2.0	0.0000000000	False
bounded by this thing	0.0	0.0	0.0	2.0	0.0000000000	False
ve done is approve	0.0	0.0	0.0	2.0	0.0000000000	False
bound for one fixed	0.0	0.0	0.0	2.0	0.0000000000	False
prove is that training	0.0	0.0	0.0	2.0	0.0000000000	False
good estimate for generalization	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses in my hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
board so in order	0.0	0.0	0.0	2.0	0.0000000000	False
define a random event	0.0	0.0	0.0	2.0	0.0000000000	False
gamma on a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
bound is the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability that there exists	0.0	0.0	0.0	4.0	0.0000000000	False
hypothesis in my class	0.0	0.0	0.0	4.0	0.0000000000	False
make a large error	0.0	0.0	7.99729290742	10.0	0.3259005146	False
error in my estimate	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of generalization error	0.0	0.0	0.0	4.0	0.0000000000	False
hypothesis one and make	0.0	0.0	0.0	2.0	0.0000000000	False
large error in estimating	0.0	0.0	0.0	4.0	0.0000000000	False
estimating the generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis two and make	0.0	0.0	0.0	2.0	0.0000000000	False
error in estimating generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error in this estimate	0.0	0.0	0.0	2.0	0.0000000000	False
make a small error	0.0	0.0	0.0	2.0	0.0000000000	False
generalization error in taking	0.0	0.0	0.0	2.0	0.0000000000	False
minus on the right	0.0	0.0	0.0	2.0	0.0000000000	False
sign of the inequality	0.0	0.0	0.0	2.0	0.0000000000	False
sides the minus sign	0.0	0.0	0.0	2.0	0.0000000000	False
sign flips the sign	0.0	0.0	0.0	2.0	0.0000000000	False
sign of the equality	0.0	0.0	0.0	2.0	0.0000000000	False
probability  which abbreviates	0.0	0.0	0.0	2.0	0.0000000000	False
simultaneously for all hypotheses	0.0	0.0	0.0	4.0	0.0000000000	False
hypotheses in our class	0.0	0.0	0.0	2.0	0.0000000000	False
conversions  this sort	0.0	0.0	0.0	2.0	0.0000000000	False
alludes to the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact that this shows	0.0	0.0	0.0	2.0	0.0000000000	False
simultaneously converge to epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
close to generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
fact that this converges	0.0	0.0	0.0	2.0	0.0000000000	False
converges for all hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
value of gamma computed	0.0	0.0	0.0	4.0	0.0000000000	False
gamma computed ? right	0.0	0.0	0.0	2.0	0.0000000000	False
gamma is a constant	0.0	0.0	0.0	2.0	0.0000000000	False
constant imagine a gamma	0.0	0.0	0.0	2.0	0.0000000000	False
constant that we chose	0.0	0.0	0.0	2.0	0.0000000000	False
true for any fixed	0.0	0.0	0.0	2.0	0.0000000000	False
bound and then sort	0.0	0.0	0.0	2.0	0.0000000000	False
ll choose specific values	0.0	0.0	0.0	2.0	0.0000000000	False
specific values of gamma	0.0	0.0	0.0	2.0	0.0000000000	False
re proved this holds	0.0	0.0	0.0	2.0	0.0000000000	False
proved this holds true	0.0	0.0	0.0	2.0	0.0000000000	False
true for any value	0.0	0.0	0.0	2.0	0.0000000000	False
labs in the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
result wo nt work	0.0	0.0	0.0	0.0	0.0000000000	False
work in this present	0.0	0.0	0.0	2.0	0.0000000000	False
lecture to infinite hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
talk concretely about algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
consequences of the understanding	0.0	0.0	0.0	2.0	0.0000000000	False
understanding of these things	0.0	0.0	0.0	2.0	0.0000000000	False
hand if the things	0.0	0.0	0.0	2.0	0.0000000000	False
things i ve proved	0.0	0.0	0.0	0.0	0.0000000000	False
proved so far make	0.0	0.0	0.0	2.0	0.0000000000	False
sense ? okay cool	0.0	0.0	0.0	2.0	0.0000000000	False
great thanks all right	0.0	0.0	0.0	2.0	0.0000000000	False
couple of other forms	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a bound	0.0	0.0	0.0	2.0	0.0000000000	False
fix my training set	0.0	0.0	0.0	4.0	0.0000000000	False
set and then fix	0.0	0.0	0.0	2.0	0.0000000000	False
training set  fix	0.0	0.0	0.0	2.0	0.0000000000	False
probability that uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
probability of something happening	0.0	0.0	0.0	2.0	0.0000000000	False
value of this error	0.0	0.0	0.0	2.0	0.0000000000	False
two other equivalent forms	0.0	0.0	0.0	2.0	0.0000000000	False
forms of the bounds	0.0	0.0	0.0	2.0	0.0000000000	False
proved was given gamma	0.0	0.0	0.0	2.0	0.0000000000	False
probability of uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
gamma and the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability delta of making	0.0	0.0	0.0	2.0	0.0000000000	False
large a training set	0.0	0.0	4.99729290742	10.0	0.2883156297	False
give a uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
conversions bound with parameters	0.0	0.0	0.0	2.0	0.0000000000	False
bound with parameters gamma	0.0	0.0	0.0	2.0	0.0000000000	False
parameters gamma and delta	0.0	0.0	0.0	2.0	0.0000000000	False
form of this result	0.0	0.0	0.0	2.0	0.0000000000	False
long as your training	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee that with probability	0.0	0.0	0.0	2.0	0.0000000000	False
error is within gamma	0.0	0.0	0.0	2.0	0.0000000000	False
gamma of generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
undergrad computer science classes	0.0	0.0	0.0	2.0	0.0000000000	False
heard of computational complexity	0.0	0.0	0.0	2.0	0.0000000000	False
sample complexity just means	0.0	0.0	0.0	2.0	0.0000000000	False
achieve a certain bound	0.0	0.0	0.0	2.0	0.0000000000	False
error and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out you can pose	0.0	0.0	0.0	2.0	0.0000000000	False
pose them in sort	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a form	0.0	0.0	0.0	2.0	0.0000000000	False
form of probability bound	0.0	0.0	0.0	2.0	0.0000000000	False
bound or a sample	0.0	0.0	0.0	2.0	0.0000000000	False
find the sample complexity	0.0	0.0	0.0	2.0	0.0000000000	False
give a certain bound	0.0	0.0	0.0	2.0	0.0000000000	False
bound on the errors	0.0	0.0	0.0	2.0	0.0000000000	False
errors and in fact	0.0	0.0	0.0	2.0	0.0000000000	False
complexity bounds often sort	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to achieve	0.0	0.0	0.0	2.0	0.0000000000	False
grows like the log	0.0	0.0	0.0	4.0	0.0000000000	False
log of k grows	0.0	0.0	0.0	2.0	0.0000000000	False
slowly as a function	0.0	0.0	0.0	2.0	0.0000000000	False
right  i learned	0.0	0.0	0.0	2.0	0.0000000000	False
purposes for all values	0.0	0.0	0.0	2.0	0.0000000000	False
fact that m sample	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses in your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
class quite a lot	0.0	0.0	0.0	2.0	0.0000000000	False
lot and the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of the training	0.0	0.0	0.0	2.0	0.0000000000	False
talk about infinite hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
classes the final form	0.0	0.0	0.0	2.0	0.0000000000	False
hold m and delta	0.0	0.0	0.0	2.0	0.0000000000	False
delta fixed and solved	0.0	0.0	0.0	2.0	0.0000000000	False
difference in the training	0.0	0.0	0.0	2.0	0.0000000000	False
result of the training	0.0	0.0	0.0	2.0	0.0000000000	False
essentially that uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
uniform conversions will hold	0.0	0.0	0.0	2.0	0.0000000000	False
true with high probability	0.0	0.0	0.0	2.0	0.0000000000	False
assume that uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon of h minus	0.0	0.0	0.0	2.0	0.0000000000	False
prove about the bound	0.0	0.0	0.0	2.0	0.0000000000	False
prove about the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
suppose this holds true	0.0	0.0	0.0	2.0	0.0000000000	False
hat was the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
selected by empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
make one more definition	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense of minimizing generalization	0.0	0.0	0.0	2.0	0.0000000000	False
sort of makes sense	0.0	0.0	0.0	2.0	0.0000000000	False
makes sense to compare	0.0	0.0	0.0	2.0	0.0000000000	False
performance of our learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to the performance	0.0	0.0	0.0	2.0	0.0000000000	False
performance of h star	0.0	0.0	0.0	2.0	0.0000000000	False
class is a class	0.0	0.0	0.0	2.0	0.0000000000	False
hope that your learning	0.0	0.0	0.0	2.0	0.0000000000	False
result in three steps	0.0	0.0	0.0	2.0	0.0000000000	False
steps so the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
hat will then gamma	0.0	0.0	0.0	2.0	0.0000000000	False
chosen to minimize training	0.0	0.0	0.0	2.0	0.0000000000	False
nt be any hypothesis	0.0	0.0	0.0	0.0	0.0000000000	False
hypothesis with lower training	0.0	0.0	0.0	2.0	0.0000000000	False
error than h hat	0.0	0.0	0.0	2.0	0.0000000000	False
hat so the training	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the training	0.0	0.0	0.0	2.0	0.0000000000	False
error of h star	0.0	0.0	5.99783432593	8.0	0.0000000000	False
hypothesis that minimizes training	0.0	0.0	0.0	2.0	0.0000000000	False
training error h hat	0.0	0.0	0.0	2.0	0.0000000000	False
apply this uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
hat of h star	0.0	0.0	0.0	2.0	0.0000000000	False
star must be moving	0.0	0.0	0.0	4.0	0.0000000000	False
moving gamma of epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
epsilon of h star	0.0	0.0	1.99837574445	6.0	0.0000000000	False
gamma of the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
generalization error with estimate	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis  a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
training examples it misclassifies	0.0	0.0	0.0	2.0	0.0000000000	False
misclassifies ? and generalization	0.0	0.0	0.0	2.0	0.0000000000	False
hat is the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis that s chosen	0.0	0.0	0.0	0.0	0.0000000000	False
chosen by empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm that minimizes training	0.0	0.0	0.0	2.0	0.0000000000	False
defined as the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
out of all hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses in my class	0.0	0.0	0.0	2.0	0.0000000000	False
minimizes training error epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
tie all these things	0.0	0.0	0.0	2.0	0.0000000000	False
set of k hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
fixed m and delta	0.0	0.0	0.0	4.0	0.0000000000	False
form of the theorem	0.0	0.0	0.0	2.0	0.0000000000	False
minimum over all hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
prove this we set	0.0	0.0	0.0	2.0	0.0000000000	False
two times the square	0.0	0.0	0.0	2.0	0.0000000000	False
times the square root	0.0	0.0	0.0	2.0	0.0000000000	False
root term to prove	0.0	0.0	0.0	2.0	0.0000000000	False
theorem we set gamma	0.0	0.0	0.0	2.0	0.0000000000	False
equal to that square	0.0	0.0	0.0	2.0	0.0000000000	False
great so set gamma	0.0	0.0	0.0	2.0	0.0000000000	False
gamma to that square	0.0	0.0	0.0	2.0	0.0000000000	False
board holds with probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability one minus delta	0.0	0.0	0.0	4.0	0.0000000000	False
minus delta right equation	0.0	0.0	0.0	2.0	0.0000000000	False
minus delta this uniform	0.0	0.0	0.0	2.0	0.0000000000	False
delta this uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
star  i guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess and whenever uniform	0.0	0.0	0.0	2.0	0.0000000000	False
boards that this result	0.0	0.0	0.0	2.0	0.0000000000	False
two  generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
star plus two times	0.0	0.0	0.0	2.0	0.0000000000	False
theorem so this result	0.0	0.0	0.0	2.0	0.0000000000	False
result sort of helps	0.0	0.0	0.0	2.0	0.0000000000	False
helps us to quantify	0.0	0.0	0.0	2.0	0.0000000000	False
quantify a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit that bias variance	0.0	0.0	0.0	2.0	0.0000000000	False
tradeoff that i talked	0.0	0.0	0.0	2.0	0.0000000000	False
start of this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
functions and linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
functions and the subset	0.0	0.0	0.0	2.0	0.0000000000	False
subset of the class	0.0	0.0	0.0	2.0	0.0000000000	False
subset of h prime	0.0	0.0	0.0	2.0	0.0000000000	False
holds for infinite hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
linear to quadratic functions	0.0	0.0	0.0	2.0	0.0000000000	False
quadratic functions then epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis in my hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
sense of generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
error  the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
function so by switching	0.0	0.0	0.0	2.0	0.0000000000	False
larger class of hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
term k will increase	0.0	0.0	0.0	2.0	0.0000000000	False
finding a better function	0.0	0.0	0.0	2.0	0.0000000000	False
sort of not fitting	0.0	0.0	0.0	2.0	0.0000000000	False
size of your hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
bias of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
variance in your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
fit this hypothesis class	0.0	0.0	0.0	2.0	0.0000000000	False
class to the data	0.0	0.0	0.0	2.0	0.0000000000	False
data and by switching	0.0	0.0	0.0	2.0	0.0000000000	False
increases and your bias	0.0	0.0	0.0	2.0	0.0000000000	False
decreases as a note	0.0	0.0	0.0	2.0	0.0000000000	False
statistics class you ve	0.0	0.0	0.0	0.0	0.0000000000	False
terms of squared error	0.0	0.0	0.0	2.0	0.0000000000	False
out that for classification	0.0	0.0	0.0	2.0	0.0000000000	False
universally accepted formal definition	0.0	0.0	0.0	2.0	0.0000000000	False
formal definition of bias	0.0	0.0	0.0	2.0	0.0000000000	False
variance for classification problems	0.0	0.0	0.0	2.0	0.0000000000	False
classification problems for regression	0.0	0.0	0.0	2.0	0.0000000000	False
problems for regression problems	0.0	0.0	0.0	2.0	0.0000000000	False
error definition for classification	0.0	0.0	0.0	2.0	0.0000000000	False
definition for classification problems	0.0	0.0	0.0	2.0	0.0000000000	False
classification problems it turns	0.0	0.0	0.0	2.0	0.0000000000	False
ve been several competing	0.0	0.0	0.0	2.0	0.0000000000	False
competing proposals for definitions	0.0	0.0	0.0	2.0	0.0000000000	False
definitions okay the cartoon	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon associated with intuition	0.0	0.0	0.0	2.0	0.0000000000	False
fixed training set size	0.0	0.0	0.0	2.0	0.0000000000	False
size m vertical axis	0.0	0.0	0.0	2.0	0.0000000000	False
axis i ll plot	0.0	0.0	0.0	0.0	0.0000000000	False
ll plot model complexity	0.0	0.0	0.0	2.0	0.0000000000	False
complexity and by model	0.0	0.0	0.0	2.0	0.0000000000	False
complexity i mean sort	0.0	0.0	0.0	2.0	0.0000000000	False
remember the bandwidth parameter	0.0	0.0	0.0	2.0	0.0000000000	False
parameter from locally weighted	0.0	0.0	0.0	2.0	0.0000000000	False
locally weighted linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
similar effect in controlling	0.0	0.0	0.0	2.0	0.0000000000	False
model is model complexity	0.0	0.0	0.0	2.0	0.0000000000	False
complexity polynomial i guess	0.0	0.0	0.0	2.0	0.0000000000	False
training error will tend	0.0	0.0	0.0	2.0	0.0000000000	False
complexity of your model	0.0	0.0	0.0	2.0	0.0000000000	False
fit your training set	0.0	0.0	0.0	2.0	0.0000000000	False
find that generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
regime on the left	0.0	0.0	0.0	2.0	0.0000000000	False
re underfitting the data	0.0	0.0	0.0	2.0	0.0000000000	False
bias and this regime	0.0	0.0	0.0	2.0	0.0000000000	False
regime on the right	0.0	0.0	0.0	2.0	0.0000000000	False
variance or you re	0.0	0.0	0.0	0.0	0.0000000000	False
re overfitting the data	0.0	0.0	0.0	2.0	0.0000000000	False
sort of intermediate complexity	0.0	0.0	0.0	2.0	0.0000000000	False
talk about the number	0.0	0.0	0.0	2.0	0.0000000000	False
automatically select model complexities	0.0	0.0	0.0	2.0	0.0000000000	False
area of minimized generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error the last thing	0.0	0.0	0.0	2.0	0.0000000000	False
back to the theorem	0.0	0.0	0.0	2.0	0.0000000000	False
out was an error	0.0	0.0	0.0	2.0	0.0000000000	False
last thing i wan	0.0	0.0	0.0	4.0	0.0000000000	False
wan na do today	0.0	0.0	0.0	2.0	0.0000000000	False
back to this theorem	0.0	0.0	0.0	2.0	0.0000000000	False
fix my error bound	0.0	0.0	0.0	2.0	0.0000000000	False
fix delta and solve	0.0	0.0	0.0	2.0	0.0000000000	False
fixed with k hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
fixed then in order	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee that the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
choose with empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
two times gamma worse	0.0	0.0	0.0	2.0	0.0000000000	False
error i could obtain	0.0	0.0	0.0	2.0	0.0000000000	False
obtain with this hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hold true with probability	0.0	0.0	0.0	2.0	0.0000000000	False
solving for the error	0.0	0.0	0.0	2.0	0.0000000000	False
re going to convince	0.0	0.0	0.0	2.0	0.0000000000	False
set that term gamma	0.0	0.0	0.0	2.0	0.0000000000	False
term gamma and solve	0.0	0.0	0.0	2.0	0.0000000000	False
result really holds true	0.0	0.0	0.0	2.0	0.0000000000	False
theorem we ve proved	0.0	0.0	0.0	0.0	0.0000000000	False
proved in other words	0.0	0.0	0.0	2.0	0.0000000000	False
bounds in learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
learning theory it turns	0.0	0.0	0.0	2.0	0.0000000000	False
loose so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
bounds usually we re	0.0	0.0	0.0	0.0	0.0000000000	False
interested in the constants	0.0	0.0	0.0	2.0	0.0000000000	False
log k over delta	0.0	0.0	0.0	2.0	0.0000000000	False
size of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class is logarithmic	0.0	0.0	0.0	2.0	0.0000000000	False
cool so next lecture	0.0	0.0	0.0	2.0	0.0000000000	False
start from this result	0.0	0.0	0.0	2.0	0.0000000000	False
generalize these to infinite	0.0	0.0	0.0	2.0	0.0000000000	False
classes and then talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about practical algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
practical algorithms for model	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms for model spectrum	0.0	0.0	0.0	2.0	0.0000000000	False
ll see you guys	0.0	0.0	0.0	2.0	0.0000000000	False
guys in a couple	0.0	0.0	0.0	2.0	0.0000000000	False
couple of quick announcements	0.0	0.0	0.0	2.0	0.0000000000	False
ve all been graded	0.0	0.0	0.0	2.0	0.0000000000	False
end of lecture today	0.0	0.0	0.0	4.0	0.0000000000	False
today if you re	0.0	0.0	0.0	0.0	0.0000000000	False
re an sepd student	0.0	0.0	2.9985	6.0	0.0000000000	False
submitted your problem set	0.0	0.0	0.0	2.0	0.0000000000	False
copy of your homework	0.0	0.0	0.0	2.0	0.0000000000	False
late hand in box	0.0	0.0	0.0	4.0	0.0000000000	False
classroom at the end	0.0	0.0	0.0	2.0	0.0000000000	False
pick up and homework	0.0	0.0	0.0	2.0	0.0000000000	False
nt picked up today	0.0	0.0	0.0	0.0	0.0000000000	False
basement of the gates	0.0	0.0	0.0	2.0	0.0000000000	False
end of class today	0.0	0.0	0.0	2.0	0.0000000000	False
posted on the web	0.0	0.0	0.0	2.0	0.0000000000	False
posted online last week	0.0	0.0	0.0	2.0	0.0000000000	False
week so do make	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you download	0.0	0.0	0.0	2.0	0.0000000000	False
sort of personally gratifying	0.0	0.0	0.0	2.0	0.0000000000	False
people in this class	0.0	0.0	0.0	2.0	0.0000000000	False
late on monday night	0.0	0.0	0.0	2.0	0.0000000000	False
announcement just a reminder	0.0	0.0	0.0	2.0	0.0000000000	False
midterm for this class	0.0	0.0	0.0	2.0	0.0000000000	False
p.m so the midterm	0.0	0.0	0.0	2.0	0.0000000000	False
guess so the midterm	0.0	0.0	0.0	2.0	0.0000000000	False
bring but no laptops	0.0	0.0	0.0	2.0	0.0000000000	False
laptops and computers sepd	0.0	0.0	0.0	2.0	0.0000000000	False
live in the bay	0.0	0.0	0.0	2.0	0.0000000000	False
person on the evening	0.0	0.0	0.0	2.0	0.0000000000	False
november if you re	0.0	0.0	0.0	0.0	0.0000000000	False
live outside the bay	0.0	0.0	0.0	4.0	0.0000000000	False
nt drive to stanford	0.0	0.0	0.0	0.0	0.0000000000	False
usual class mailing address	0.0	0.0	0.0	2.0	0.0000000000	False
midterm because you live	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you email	0.0	0.0	0.0	2.0	0.0000000000	False
arrangements for the midterm	0.0	0.0	0.0	2.0	0.0000000000	False
taking this via sepd	0.0	0.0	0.0	2.0	0.0000000000	False
equal or greater importance	0.0	0.0	0.0	2.0	0.0000000000	False
midterm of another class	0.0	0.0	0.0	2.0	0.0000000000	False
usual staff mailing address	0.0	0.0	0.0	2.0	0.0000000000	False
showing up in person	0.0	0.0	0.0	2.0	0.0000000000	False
person for the midterm	0.0	0.0	0.0	2.0	0.0000000000	False
midterm okay any questions	0.0	0.0	0.0	2.0	0.0000000000	False
lecture s technical material	0.0	0.0	0.0	0.0	0.0000000000	False
week s discussion section	0.0	0.0	0.0	0.0	0.0000000000	False
talking about convex optimization	0.0	0.0	0.0	2.0	0.0000000000	False
last week s discussion	0.0	0.0	0.0	0.0	0.0000000000	False
discussion section they discussed	0.0	0.0	0.0	2.0	0.0000000000	False
discussed total convex optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization and this week	0.0	0.0	0.0	2.0	0.0000000000	False
week they ll wrap	0.0	0.0	0.0	0.0	0.0000000000	False
wrap up the material	0.0	0.0	0.0	2.0	0.0000000000	False
present on convex optimization	0.0	0.0	0.0	2.0	0.0000000000	False
today in this lecture	0.0	0.0	0.0	2.0	0.0000000000	False
talk a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit more about learning	0.0	0.0	0.0	2.0	0.0000000000	False
talk about vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
building on the issues	0.0	0.0	0.0	2.0	0.0000000000	False
issues of bias variance	0.0	0.0	0.0	2.0	0.0000000000	False
tradeoffs of under fitting	0.0	0.0	0.0	2.0	0.0000000000	False
fitting and over fitting	0.0	0.0	0.0	2.0	0.0000000000	False
talk about model selection	0.0	0.0	3.9985	6.0	0.0000000000	False
algorithms for automatically making	0.0	0.0	0.0	2.0	0.0000000000	False
decisions for this bias	0.0	0.0	0.0	2.0	0.0000000000	False
previous lecture and depending	0.0	0.0	0.0	2.0	0.0000000000	False
set of k hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee that this holds	0.0	0.0	2.9985	6.0	0.0000000000	False
minus delta it suffices	0.0	0.0	0.0	2.0	0.0000000000	False
talked about empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
simplified modern machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class of script	0.0	0.0	0.0	2.0	0.0000000000	False
empirical risk minimization-learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
attains the smallest error	0.0	0.0	0.0	2.0	0.0000000000	False
error on the training	0.0	0.0	0.0	2.0	0.0000000000	False
generalization error ; right	0.0	0.0	0.0	2.0	0.0000000000	False
probability of a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
distribution as the training	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee that the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error of the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
output by empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
class plus two times	0.0	0.0	0.0	2.0	0.0000000000	False
times gamma two times	0.0	0.0	0.0	2.0	0.0000000000	False
two times this error	0.0	0.0	0.0	2.0	0.0000000000	False
times this error threshold	0.0	0.0	0.0	2.0	0.0000000000	False
minus delta we show	0.0	0.0	0.0	2.0	0.0000000000	False
show that it suffices	0.0	0.0	0.0	2.0	0.0000000000	False
suffices for your training	0.0	0.0	0.0	2.0	0.0000000000	False
two gamma square log	0.0	0.0	0.0	2.0	0.0000000000	False
two k over delta	0.0	0.0	0.0	2.0	0.0000000000	False
size of your hypothesis	0.0	0.0	0.0	4.0	0.0000000000	False
bound in the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of training examples	0.0	0.0	9.994	24.0	0.3925233645	False
case of infinite hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
script h is sort	0.0	0.0	0.0	2.0	0.0000000000	False
model like logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
parameterized by real numbers	0.0	0.0	0.0	2.0	0.0000000000	False
first going to give	0.0	0.0	0.0	2.0	0.0000000000	False
argument that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
sort of formally broken	0.0	0.0	0.0	2.0	0.0000000000	False
formally broken just sort	0.0	0.0	0.0	2.0	0.0000000000	False
proof is somewhat involved	0.0	0.0	0.0	2.0	0.0000000000	False
apply this result analyzing	0.0	0.0	0.0	2.0	0.0000000000	False
result analyzing logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
script h is parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
re applying logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression to find	0.0	0.0	0.0	2.0	0.0000000000	False
find the linear position	0.0	0.0	0.0	2.0	0.0000000000	False
endless one real numbers	0.0	0.0	0.0	2.0	0.0000000000	False
class is really represented	0.0	0.0	0.0	2.0	0.0000000000	False
represented in a computer	0.0	0.0	0.0	2.0	0.0000000000	False
double position floating point	0.0	0.0	0.0	2.0	0.0000000000	False
position floating point numbers	0.0	0.0	0.0	2.0	0.0000000000	False
real number is represented	0.0	0.0	0.0	2.0	0.0000000000	False
64-bit representation ; right	0.0	0.0	0.0	2.0	0.0000000000	False
times d bits computers	0.0	0.0	0.0	2.0	0.0000000000	False
computers ca nt represent	0.0	0.0	0.0	0.0	0.0000000000	False
nt represent real numbers	0.0	0.0	0.0	0.0	0.0000000000	False
numbers they only represent	0.0	0.0	0.0	2.0	0.0000000000	False
represent used to speed	0.0	0.0	0.0	2.0	0.0000000000	False
class in your computer	0.0	0.0	0.0	2.0	0.0000000000	False
number of possible values	0.0	0.0	0.0	2.0	0.0000000000	False
ways you can flip	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee that a hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
returned by empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses in your hypotheses	0.0	0.0	0.0	4.0	0.0000000000	False
sort of error bound	0.0	0.0	0.0	2.0	0.0000000000	False
intuition that this conveys	0.0	0.0	0.0	2.0	0.0000000000	False
linear in the number	0.0	0.0	3.9985	6.0	0.0000000000	False
parameters of your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
order of something linear	0.0	0.0	0.0	2.0	0.0000000000	False
sense that it relies	0.0	0.0	0.0	2.0	0.0000000000	False
representation of 14-point numbers	0.0	0.0	0.0	2.0	0.0000000000	False
right way to show	0.0	0.0	0.0	4.0	0.0000000000	False
formally ; all right	0.0	0.0	0.0	2.0	0.0000000000	False
turns out the right	0.0	0.0	0.0	2.0	0.0000000000	False
involves a much longer	0.0	0.0	0.0	2.0	0.0000000000	False
longer because the proof	0.0	0.0	0.0	2.0	0.0000000000	False
proof is extremely involved	0.0	0.0	0.0	2.0	0.0000000000	False
prove it farther proof	0.0	0.0	0.0	2.0	0.0000000000	False
proof be a source	0.0	0.0	0.0	2.0	0.0000000000	False
source of learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis classes this definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition given a set	0.0	0.0	0.0	2.0	0.0000000000	False
set of d points	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class h shatters	0.0	0.0	0.0	2.0	0.0000000000	False
informal way of thinking	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class has shattered	0.0	0.0	0.0	2.0	0.0000000000	False
associate these d points	0.0	0.0	0.0	2.0	0.0000000000	False
points with any caught	0.0	0.0	0.0	2.0	0.0000000000	False
caught set of labels	0.0	0.0	0.0	2.0	0.0000000000	False
labels y ; right	0.0	0.0	0.0	2.0	0.0000000000	False
labels those d examples	0.0	0.0	0.0	2.0	0.0000000000	False
points you can choose	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class h classed	0.0	0.0	0.0	2.0	0.0000000000	False
classed all linear classifiers	0.0	0.0	0.0	2.0	0.0000000000	False
find a linear classifier	0.0	0.0	0.0	2.0	0.0000000000	False
linear classifier that attains	0.0	0.0	0.0	2.0	0.0000000000	False
attains zero training error	0.0	0.0	0.0	2.0	0.0000000000	False
labelings of this set	0.0	0.0	0.0	2.0	0.0000000000	False
set of two points	0.0	0.0	0.0	2.0	0.0000000000	False
class script h shatters	0.0	0.0	0.0	2.0	0.0000000000	False
set of three points	0.0	0.0	0.0	4.0	0.0000000000	False
hypothesis in the hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class that labels	0.0	0.0	0.0	2.0	0.0000000000	False
labels these examples correctly	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class also shatters	0.0	0.0	0.0	2.0	0.0000000000	False
terminology h can realize	0.0	0.0	0.0	2.0	0.0000000000	False
give it any set	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis that perfectly separates	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative examples	0.0	0.0	0.0	2.0	0.0000000000	False
set of four points	0.0	0.0	2.9985	6.0	0.0000000000	False
labelings we can choose	0.0	0.0	0.0	2.0	0.0000000000	False
boundary that can realize	0.0	0.0	0.0	2.0	0.0000000000	False
points that the class	0.0	0.0	0.0	2.0	0.0000000000	False
linear classifiers can shatter	0.0	0.0	0.0	2.0	0.0000000000	False
dimension these two people	0.0	0.0	0.0	2.0	0.0000000000	False
vapnik and chervonenkis dimension	0.0	0.0	0.0	2.0	0.0000000000	False
set that is shattered	0.0	0.0	0.0	2.0	0.0000000000	False
shattered by this set	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class can shatter	0.0	0.0	0.0	2.0	0.0000000000	False
shatter arbitrarily large sets	0.0	0.0	0.0	2.0	0.0000000000	False
dimension of the set	0.0	0.0	0.0	2.0	0.0000000000	False
set s of size	0.0	0.0	0.0	2.0	0.0000000000	False
absolutely so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
exists some other set	0.0	0.0	0.0	2.0	0.0000000000	False
size three being shattered	0.0	0.0	0.0	2.0	0.0000000000	False
four that can shatter	0.0	0.0	0.0	2.0	0.0000000000	False
nt shatter this set	0.0	0.0	0.0	0.0	0.0000000000	False
shatter and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns out this result	0.0	0.0	0.0	2.0	0.0000000000	False
out this result holds	0.0	0.0	0.0	2.0	0.0000000000	False
dimensions the vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
dimension of the class	0.0	0.0	0.0	4.0	0.0000000000	False
class of linear classifiers	0.0	0.0	2.9985	6.0	0.0000000000	False
classifiers in any dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
dimension in any dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
arguably the best-known result	0.0	0.0	0.0	2.0	0.0000000000	False
probability of one minus	0.0	0.0	0.0	4.0	0.0000000000	False
formula on the right	0.0	0.0	0.0	2.0	0.0000000000	False
right looks a bit	0.0	0.0	0.0	2.0	0.0000000000	False
out the essential aspects	0.0	0.0	0.0	2.0	0.0000000000	False
key to this result	0.0	0.0	0.0	2.0	0.0000000000	False
class with vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
vapnik and chervonenkis show	0.0	0.0	0.0	2.0	0.0000000000	False
minus delta you enjoy	0.0	0.0	0.0	2.0	0.0000000000	False
sort of uniform conversions	0.0	0.0	0.0	2.0	0.0000000000	False
hypotheses in your hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
error of h minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus the training error	0.0	0.0	0.0	2.0	0.0000000000	False
two things is bounded	0.0	0.0	0.0	2.0	0.0000000000	False
re probably one minus	0.0	0.0	0.0	2.0	0.0000000000	False
step to this step	0.0	0.0	0.0	4.0	0.0000000000	False
previous lecture we proved	0.0	0.0	0.0	2.0	0.0000000000	False
implies that it appears	0.0	0.0	0.0	2.0	0.0000000000	False
showed that if generalization	0.0	0.0	0.0	2.0	0.0000000000	False
generalization error and training	0.0	0.0	0.0	2.0	0.0000000000	False
error and training error	0.0	0.0	0.0	2.0	0.0000000000	False
error of the hypotheses	0.0	0.0	0.0	4.0	0.0000000000	False
times the best generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error plus two times	0.0	0.0	0.0	2.0	0.0000000000	False
notation so that formula	0.0	0.0	0.0	2.0	0.0000000000	False
minus delta we re	0.0	0.0	0.0	0.0	0.0000000000	False
put gamma and delta	0.0	0.0	0.0	2.0	0.0000000000	False
subscript error to denote	0.0	0.0	0.0	2.0	0.0000000000	False
treat gamma and delta	0.0	0.0	0.0	2.0	0.0000000000	False
absorb turns that depend	0.0	0.0	0.0	2.0	0.0000000000	False
dimension and hypotheses class	0.0	0.0	0.0	2.0	0.0000000000	False
empirical risk minimization algorithms	0.0	0.0	0.0	4.0	0.0000000000	False
algorithms in other words	0.0	0.0	0.0	2.0	0.0000000000	False
training error the intuition	0.0	0.0	0.0	2.0	0.0000000000	False
dimension of the hypotheses	0.0	0.0	0.0	2.0	0.0000000000	False
shows that sample complexity	0.0	0.0	0.0	2.0	0.0000000000	False
complexity is upper bounded	0.0	0.0	0.0	4.0	0.0000000000	False
model and logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression linear classification	0.0	0.0	0.0	2.0	0.0000000000	False
classification in any dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
regression in any dimensions	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of your model	0.0	0.0	0.0	4.0	0.0000000000	False
fit in those models	0.0	0.0	0.0	2.0	0.0000000000	False
parameters in your model	0.0	0.0	0.0	2.0	0.0000000000	False
source of not learning	0.0	0.0	0.0	2.0	0.0000000000	False
result shows the sample	0.0	0.0	0.0	2.0	0.0000000000	False
shows the sample complexity	0.0	0.0	0.0	2.0	0.0000000000	False
bounded by vc dimension	0.0	0.0	0.0	4.0	0.0000000000	False
worse case some complexity	0.0	0.0	0.0	2.0	0.0000000000	False
perfectly nasty learning problem	0.0	0.0	0.0	2.0	0.0000000000	False
bound so i guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess in the worse	0.0	0.0	0.0	2.0	0.0000000000	False
complexity in the number	0.0	0.0	0.0	2.0	0.0000000000	False
bounded and lower bounded	0.0	0.0	0.0	2.0	0.0000000000	False
proof of this assume	0.0	0.0	0.0	2.0	0.0000000000	False
entirety of the theorem	0.0	0.0	0.0	2.0	0.0000000000	False
theorem this is true	0.0	0.0	0.0	2.0	0.0000000000	False
out in the proof	0.0	0.0	0.0	2.0	0.0000000000	False
reconstruction called an epsilon	0.0	0.0	0.0	2.0	0.0000000000	False
working through this proof	0.0	0.0	0.0	2.0	0.0000000000	False
start reading the book	0.0	0.0	0.0	2.0	0.0000000000	False
thought i would inflict	0.0	0.0	0.0	2.0	0.0000000000	False
couple of loose ends	0.0	0.0	0.0	4.0	0.0000000000	False
mention a few things	0.0	0.0	0.0	2.0	0.0000000000	False
feel a little bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit like random facts	0.0	0.0	0.0	2.0	0.0000000000	False
proved for an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
minimizes 0-1 training error	0.0	0.0	0.0	2.0	0.0000000000	False
error so one question	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on support vector	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature space	0.0	0.0	0.0	2.0	0.0000000000	False
infinite so it turns	0.0	0.0	0.0	2.0	0.0000000000	False
out that the class	0.0	0.0	0.0	2.0	0.0000000000	False
class of linear separators	0.0	0.0	0.0	2.0	0.0000000000	False
separators with large margin	0.0	0.0	0.0	4.0	0.0000000000	False
give you a set	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class will comprise	0.0	0.0	0.0	2.0	0.0000000000	False
comprise only the linear	0.0	0.0	0.0	2.0	0.0000000000	False
nt allow a point	0.0	0.0	0.0	0.0	0.0000000000	False
point that comes closer	0.0	0.0	0.0	2.0	0.0000000000	False
nt allow that line	0.0	0.0	0.0	0.0	0.0000000000	False
data points all lie	0.0	0.0	0.0	2.0	0.0000000000	False
lie within some sphere	0.0	0.0	0.0	2.0	0.0000000000	False
data with a margin	0.0	0.0	0.0	2.0	0.0000000000	False
equal to r squared	0.0	0.0	0.0	2.0	0.0000000000	False
squared over four gamma	0.0	0.0	0.0	2.0	0.0000000000	False
symbol ; it means	0.0	0.0	0.0	2.0	0.0000000000	False
turns out you prove	0.0	0.0	0.0	2.0	0.0000000000	False
things about this result	0.0	0.0	0.0	2.0	0.0000000000	False
talk about but turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns they can prove	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers with large margins	0.0	0.0	0.0	2.0	0.0000000000	False
margins is actually bounded	0.0	0.0	0.0	2.0	0.0000000000	False
bounded the surprising thing	0.0	0.0	0.0	2.0	0.0000000000	False
bound on vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
dependents on the dimension	0.0	0.0	0.0	2.0	0.0000000000	False
dimension of the points	0.0	0.0	0.0	2.0	0.0000000000	False
data points x combine	0.0	0.0	0.0	2.0	0.0000000000	False
long as you restrict	0.0	0.0	0.0	2.0	0.0000000000	False
attention to the class	0.0	0.0	0.0	2.0	0.0000000000	False
class of your separators	0.0	0.0	0.0	2.0	0.0000000000	False
find a large margin	0.0	0.0	0.0	2.0	0.0000000000	False
examples with large margin	0.0	0.0	0.0	2.0	0.0000000000	False
automatically trying to find	0.0	0.0	0.0	2.0	0.0000000000	False
find a hypothesis class	0.0	0.0	0.0	2.0	0.0000000000	False
constantly infinite dimensional vectors	0.0	0.0	0.0	2.0	0.0000000000	False
equal to some equals	0.0	0.0	0.0	2.0	0.0000000000	False
tie empirical risk minimization	0.0	0.0	0.0	2.0	0.0000000000	False
strongly to the source	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms we ve talked	0.0	0.0	0.0	0.0	0.0000000000	False
talked about it turns	0.0	0.0	0.0	2.0	0.0000000000	False
minimization so that view	0.0	0.0	0.0	2.0	0.0000000000	False
training example your training	0.0	0.0	0.0	2.0	0.0000000000	False
value of this data	0.0	0.0	0.0	2.0	0.0000000000	False
guess if your training	0.0	0.0	0.0	2.0	0.0000000000	False
subscript x not equals	0.0	0.0	0.0	2.0	0.0000000000	False
minimize this step function	0.0	0.0	0.0	4.0	0.0000000000	False
step function ; right	0.0	0.0	0.0	2.0	0.0000000000	False
correct classification on setting	0.0	0.0	0.0	2.0	0.0000000000	False
turns out this step	0.0	0.0	0.0	2.0	0.0000000000	False
out this step function	0.0	0.0	0.0	2.0	0.0000000000	False
classifiers minimizing the training	0.0	0.0	0.0	2.0	0.0000000000	False
minimizing the training error	0.0	0.0	0.0	2.0	0.0000000000	False
error is an empty	0.0	0.0	0.0	2.0	0.0000000000	False
heart problem it turns	0.0	0.0	0.0	2.0	0.0000000000	False
machines can be viewed	0.0	0.0	0.0	2.0	0.0000000000	False
approximation for this problem	0.0	0.0	0.0	2.0	0.0000000000	False
out that logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
ll be a function	0.0	0.0	0.0	2.0	0.0000000000	False
approximation to this step	0.0	0.0	0.0	2.0	0.0000000000	False
approximate empirical risk minimization	0.0	0.0	0.0	4.0	0.0000000000	False
line above this curve	0.0	0.0	0.0	2.0	0.0000000000	False
problem you can find	0.0	0.0	0.0	2.0	0.0000000000	False
find the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
parameters for logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
viewed as approximated dysfunction	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine turns	0.0	0.0	0.0	2.0	0.0000000000	False
approximate this step function	0.0	0.0	0.0	2.0	0.0000000000	False
two over different approximation	0.0	0.0	0.0	2.0	0.0000000000	False
linear that our results	0.0	0.0	0.0	2.0	0.0000000000	False
regression and the support	0.0	0.0	0.0	2.0	0.0000000000	False
machine as different approximations	0.0	0.0	0.0	2.0	0.0000000000	False
developed even though svm	0.0	0.0	0.0	2.0	0.0000000000	False
due to empirical risk	0.0	0.0	0.0	2.0	0.0000000000	False
last of the loose	0.0	0.0	0.0	2.0	0.0000000000	False
move on to talk	0.0	0.0	0.0	2.0	0.0000000000	False
theory that we started	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
important not to choose	0.0	0.0	0.0	2.0	0.0000000000	False
simple or too complex	0.0	0.0	0.0	2.0	0.0000000000	False
choose a linear function	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis with high bias	0.0	0.0	0.0	2.0	0.0000000000	False
generalize well so model	0.0	0.0	0.0	2.0	0.0000000000	False
model selection algorithms provide	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms provide a class	0.0	0.0	0.0	2.0	0.0000000000	False
methods to automatically trade	0.0	0.0	0.0	2.0	0.0000000000	False
trade make these tradeoffs	0.0	0.0	0.0	2.0	0.0000000000	False
last time of generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error ? i drew	0.0	0.0	0.0	2.0	0.0000000000	False
x-axis was model complexity	0.0	0.0	0.0	2.0	0.0000000000	False
number of the degree	0.0	0.0	0.0	2.0	0.0000000000	False
polynomial ; the regression	0.0	0.0	0.0	2.0	0.0000000000	False
polynomial to five data	0.0	0.0	0.0	2.0	0.0000000000	False
selection in the abstract	0.0	0.0	0.0	2.0	0.0000000000	False
abstract ; all right	0.0	0.0	0.0	2.0	0.0000000000	False
right ? some examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples of model selection	0.0	0.0	0.0	4.0	0.0000000000	False
selection problems will include	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to choose	0.0	0.0	7.998	8.0	0.0000000000	False
right ? what degree	0.0	0.0	0.0	2.0	0.0000000000	False
parameter in locally awaited	0.0	0.0	0.0	2.0	0.0000000000	False
locally awaited linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
local way to regression	0.0	0.0	0.0	2.0	0.0000000000	False
optimization objective ; right	0.0	0.0	0.0	2.0	0.0000000000	False
penalize in this class	0.0	0.0	0.0	2.0	0.0000000000	False
specific examples of model	0.0	0.0	0.0	2.0	0.0000000000	False
method for semantically choosing	0.0	0.0	0.0	2.0	0.0000000000	False
finite set of models	0.0	0.0	0.0	2.0	0.0000000000	False
bandwidth parameter and discretize	0.0	0.0	0.0	2.0	0.0000000000	False
discrete of the values	0.0	0.0	0.0	2.0	0.0000000000	False
select an appropriate model	0.0	0.0	0.0	2.0	0.0000000000	False
model ; all right	0.0	0.0	0.0	2.0	0.0000000000	False
laughing that i asked	0.0	0.0	0.0	2.0	0.0000000000	False
terrible idea to choose	0.0	0.0	0.0	2.0	0.0000000000	False
complex model ; right	0.0	0.0	0.0	2.0	0.0000000000	False
choose a 10th degree	0.0	0.0	0.0	2.0	0.0000000000	False
fits the training set	0.0	0.0	0.0	2.0	0.0000000000	False
selection in a training	0.0	0.0	0.0	2.0	0.0000000000	False
set several standard procedures	0.0	0.0	0.0	2.0	0.0000000000	False
hold out cross validation	0.0	0.0	9.994	24.0	0.3559322034	False
teach a training set	0.0	0.0	0.0	2.0	0.0000000000	False
randomly split the training	0.0	0.0	0.0	2.0	0.0000000000	False
split the training set	0.0	0.0	0.0	2.0	0.0000000000	False
set into two subsets	0.0	0.0	0.0	2.0	0.0000000000	False
two subsets we call	0.0	0.0	0.0	2.0	0.0000000000	False
call it the training	0.0	0.0	0.0	2.0	0.0000000000	False
out cross validation subset	0.0	0.0	0.0	4.0	0.0000000000	False
model on just trading	0.0	0.0	0.0	2.0	0.0000000000	False
out cross validation set	0.0	0.0	0.0	4.0	0.0000000000	False
set and you pick	0.0	0.0	0.0	2.0	0.0000000000	False
error on the hold	0.0	0.0	0.0	2.0	0.0000000000	False
percent of the data	0.0	0.0	2.998	8.0	0.0000000000	False
smallest hold out cross	0.0	0.0	0.0	2.0	0.0000000000	False
out cross validation error	0.0	0.0	0.0	4.0	0.0000000000	False
error on your hold	0.0	0.0	0.0	2.0	0.0000000000	False
model that you selected	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis that was trained	0.0	0.0	0.0	2.0	0.0000000000	False
percent of your data	0.0	0.0	3.998	8.0	0.0000000000	False
cross validation does sort	0.0	0.0	0.0	2.0	0.0000000000	False
working with a company	0.0	0.0	0.0	2.0	0.0000000000	False
acquired at great cost	0.0	0.0	0.0	2.0	0.0000000000	False
great cost ; right	0.0	0.0	0.0	2.0	0.0000000000	False
acquired by medical experiments	0.0	0.0	0.0	2.0	0.0000000000	False
represents a sick man	0.0	0.0	0.0	2.0	0.0000000000	False
sick man in amounts	0.0	0.0	0.0	2.0	0.0000000000	False
amounts of physical human	0.0	0.0	0.0	2.0	0.0000000000	False
couple of other variations	0.0	0.0	0.0	2.0	0.0000000000	False
variations on hold out	0.0	0.0	0.0	2.0	0.0000000000	False
cross validation that makes	0.0	0.0	0.0	2.0	0.0000000000	False
train on k minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus one pieces test	0.0	0.0	0.0	2.0	0.0000000000	False
test on the remaining	0.0	0.0	0.0	4.0	0.0000000000	False
out i will hold	0.0	0.0	0.0	2.0	0.0000000000	False
train on the remaining	0.0	0.0	0.0	2.0	0.0000000000	False
remove the third piece	0.0	0.0	0.0	2.0	0.0000000000	False
estimate of the generalization	0.0	0.0	0.0	2.0	0.0000000000	False
error of my model	0.0	0.0	0.0	2.0	0.0000000000	False
selected on the entirety	0.0	0.0	0.0	2.0	0.0000000000	False
entirety of your training	0.0	0.0	0.0	2.0	0.0000000000	False
set so i drew	0.0	0.0	0.0	2.0	0.0000000000	False
validation and the advantage	0.0	0.0	0.0	2.0	0.0000000000	False
hold out cross option	0.0	0.0	0.0	2.0	0.0000000000	False
data into ten pieces	0.0	0.0	0.0	2.0	0.0000000000	False
out in simple hold	0.0	0.0	0.0	2.0	0.0000000000	False
simple hold out cross	0.0	0.0	0.0	4.0	0.0000000000	False
split is fairly common	0.0	0.0	0.0	4.0	0.0000000000	False
common choice the disadvantage	0.0	0.0	0.0	2.0	0.0000000000	False
disadvantage of k-fold cross	0.0	0.0	0.0	2.0	0.0000000000	False
train your model ten	0.0	0.0	0.0	2.0	0.0000000000	False
ten times per model	0.0	0.0	0.0	2.0	0.0000000000	False
expensive but k equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals ten works great	0.0	0.0	0.0	2.0	0.0000000000	False
examples and this procedure	0.0	0.0	0.0	2.0	0.0000000000	False
procedure is called leave	0.0	0.0	0.0	2.0	0.0000000000	False
leave one out cross	0.0	0.0	2.998	8.0	0.4158415842	False
out the first training	0.0	0.0	0.0	2.0	0.0000000000	False
train on the rest	0.0	0.0	0.0	4.0	0.0000000000	False
out the second training	0.0	0.0	0.0	2.0	0.0000000000	False
data than k-fold cross	0.0	0.0	0.0	2.0	0.0000000000	False
leave one example out	0.0	0.0	0.0	2.0	0.0000000000	False
run your learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm on m minus	0.0	0.0	0.0	2.0	0.0000000000	False
minus one training examples	0.0	0.0	0.0	2.0	0.0000000000	False
re extremely data scarce	0.0	0.0	0.0	2.0	0.0000000000	False
validation is maybe preferred	0.0	0.0	0.0	2.0	0.0000000000	False
proved that the difference	0.0	0.0	0.0	2.0	0.0000000000	False
examples in your training	0.0	0.0	0.0	2.0	0.0000000000	False
set and vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
dimension so maybe examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples into different groups	0.0	0.0	0.0	2.0	0.0000000000	False
compute the training error	0.0	0.0	0.0	2.0	0.0000000000	False
people in structure risk	0.0	0.0	0.0	2.0	0.0000000000	False
risk minimization that propose	0.0	0.0	0.0	2.0	0.0000000000	False
questions for cross validation	0.0	0.0	0.0	2.0	0.0000000000	False
points do you sort	0.0	0.0	0.0	2.0	0.0000000000	False
re proving learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
proving learning theory bounds	0.0	0.0	0.0	2.0	0.0000000000	False
loose because you re	0.0	0.0	0.0	0.0	0.0000000000	False
re sort of proving	0.0	0.0	0.0	2.0	0.0000000000	False
proving the worse case	0.0	0.0	0.0	2.0	0.0000000000	False
worse case upper bound	0.0	0.0	0.0	2.0	0.0000000000	False
upper bound that holds	0.0	0.0	0.0	2.0	0.0000000000	False
bounds that i proved	0.0	0.0	0.0	2.0	0.0000000000	False
right ? that holds	0.0	0.0	0.0	2.0	0.0000000000	False
absolutely any probability distribution	0.0	0.0	0.0	4.0	0.0000000000	False
probability distribution over training	0.0	0.0	0.0	2.0	0.0000000000	False
distribution over training examples	0.0	0.0	0.0	2.0	0.0000000000	False
assume the training examples	0.0	0.0	0.0	2.0	0.0000000000	False
training examples we ve	0.0	0.0	0.0	0.0	0.0000000000	False
examples we ve drawn	0.0	0.0	0.0	0.0	0.0000000000	False
iid from some distribution	0.0	0.0	0.0	2.0	0.0000000000	False
probability distribution over script	0.0	0.0	0.0	2.0	0.0000000000	False
script d and chances	0.0	0.0	0.0	2.0	0.0000000000	False
houses and their prices	0.0	0.0	0.0	2.0	0.0000000000	False
plug in the constants	0.0	0.0	0.0	2.0	0.0000000000	False
constants of learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
numbers take logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression you have ten	0.0	0.0	0.0	2.0	0.0000000000	False
probability how many training	0.0	0.0	0.0	2.0	0.0000000000	False
plug in actual constants	0.0	0.0	0.0	2.0	0.0000000000	False
constants into the text	0.0	0.0	0.0	2.0	0.0000000000	False
text for learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
estimates with the number	0.0	0.0	0.0	2.0	0.0000000000	False
training examples to fit	0.0	0.0	0.0	2.0	0.0000000000	False
examples to fit ten	0.0	0.0	0.0	2.0	0.0000000000	False
write papers on learning	0.0	0.0	0.0	2.0	0.0000000000	False
papers on learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
ignore the constant factors	0.0	0.0	0.0	2.0	0.0000000000	False
factors because the bounds	0.0	0.0	0.0	2.0	0.0000000000	False
bounds to give guidelines	0.0	0.0	0.0	2.0	0.0000000000	False
linearly in the number	0.0	0.0	0.0	2.0	0.0000000000	False
shape of the bounds	0.0	0.0	0.0	2.0	0.0000000000	False
fact that the number	0.0	0.0	0.0	2.0	0.0000000000	False
training examples the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact that some complexity	0.0	0.0	0.0	2.0	0.0000000000	False
magnitude of the bound	0.0	0.0	0.0	2.0	0.0000000000	False
looser than will hold	0.0	0.0	0.0	2.0	0.0000000000	False
problem you are working	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to fit	0.0	0.0	0.0	2.0	0.0000000000	False
fit a logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
training examples is ten	0.0	0.0	0.0	2.0	0.0000000000	False
examples is ten times	0.0	0.0	0.0	2.0	0.0000000000	False
ten times your number	0.0	0.0	0.0	2.0	0.0000000000	False
examples is like tiny	0.0	0.0	0.0	2.0	0.0000000000	False
tiny times the number	0.0	0.0	0.0	2.0	0.0000000000	False
bounds in cross validation	0.0	0.0	0.0	2.0	0.0000000000	False
validation do we assume	0.0	0.0	0.0	2.0	0.0000000000	False
convention we usually split	0.0	0.0	0.0	2.0	0.0000000000	False
split the train testers	0.0	0.0	0.0	2.0	0.0000000000	False
randomly one more thing	0.0	0.0	0.0	2.0	0.0000000000	False
talk about for model	0.0	0.0	0.0	2.0	0.0000000000	False
special case of model	0.0	0.0	0.0	2.0	0.0000000000	False
case of model selections	0.0	0.0	0.0	2.0	0.0000000000	False
high dimensional feature space	0.0	0.0	0.0	2.0	0.0000000000	False
classification and i wan	0.0	0.0	0.0	2.0	0.0000000000	False
talk about this text	0.0	0.0	0.0	2.0	0.0000000000	False
classification example that spam	0.0	0.0	0.0	2.0	0.0000000000	False
30,000 or 50,000 features	0.0	0.0	0.0	2.0	0.0000000000	False
depending on what learning	0.0	0.0	0.0	2.0	0.0000000000	False
risk of over fitting	0.0	0.0	3.9985	6.0	0.0000000000	False
variance of your learning	0.0	0.0	0.0	2.0	0.0000000000	False
specific case of text	0.0	0.0	0.0	2.0	0.0000000000	False
case of text classification	0.0	0.0	0.0	4.0	0.0000000000	False
number of relevant features	0.0	0.0	0.0	2.0	0.0000000000	False
smaller number of features	0.0	0.0	0.0	2.0	0.0000000000	False
relevant to the learning	0.0	0.0	0.0	2.0	0.0000000000	False
word buy or viagra	0.0	0.0	0.0	2.0	0.0000000000	False
non-spam so in feature	0.0	0.0	0.0	2.0	0.0000000000	False
subset of the features	0.0	0.0	0.0	2.0	0.0000000000	False
give ourselves a simpler	0.0	0.0	0.0	2.0	0.0000000000	False
simpler learning a simpler	0.0	0.0	0.0	2.0	0.0000000000	False
learning a simpler hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis class to choose	0.0	0.0	0.0	2.0	0.0000000000	False
space so in feature	0.0	0.0	0.0	2.0	0.0000000000	False
searcheristics sort of simple	0.0	0.0	0.0	2.0	0.0000000000	False
sort of simple search	0.0	0.0	0.0	2.0	0.0000000000	False
search through this space	0.0	0.0	0.0	2.0	0.0000000000	False
find a good subset	0.0	0.0	0.0	2.0	0.0000000000	False
good subset of features	0.0	0.0	0.0	2.0	0.0000000000	False
enumerate all possible feature	0.0	0.0	0.0	2.0	0.0000000000	False
initialize the sets script	0.0	0.0	0.0	2.0	0.0000000000	False
repeat for i equals	0.0	0.0	0.0	2.0	0.0000000000	False
model using cross validation	0.0	0.0	0.0	2.0	0.0000000000	False
validation and by cross	0.0	0.0	0.0	2.0	0.0000000000	False
validation or k-fold cross	0.0	0.0	0.0	2.0	0.0000000000	False
cross validation or leave	0.0	0.0	0.0	2.0	0.0000000000	False
equal to f union	0.0	0.0	0.0	2.0	0.0000000000	False
follow through the empty	0.0	0.0	0.0	2.0	0.0000000000	False
empty set of features	0.0	0.0	0.0	2.0	0.0000000000	False
feature to your set	0.0	0.0	0.0	4.0	0.0000000000	False
set then you train	0.0	0.0	0.0	2.0	0.0000000000	False
single feature to add	0.0	0.0	0.0	2.0	0.0000000000	False
add to your set	0.0	0.0	0.0	2.0	0.0000000000	False
script f in step	0.0	0.0	0.0	2.0	0.0000000000	False
feature or best model	0.0	0.0	0.0	2.0	0.0000000000	False
model according to hold	0.0	0.0	0.0	2.0	0.0000000000	False
feature addition that results	0.0	0.0	0.0	2.0	0.0000000000	False
lowest hold out cross	0.0	0.0	0.0	2.0	0.0000000000	False
lowest cross validation error	0.0	0.0	0.0	4.0	0.0000000000	False
added all the features	0.0	0.0	0.0	2.0	0.0000000000	False
entire set of features	0.0	0.0	0.0	4.0	0.0000000000	False
exceeded some threshold number	0.0	0.0	0.0	2.0	0.0000000000	False
threshold number of features	0.0	0.0	0.0	2.0	0.0000000000	False
re fitting logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
features added to set	0.0	0.0	0.0	2.0	0.0000000000	False
output of best hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
training lots of hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
testing them using cross	0.0	0.0	0.0	2.0	0.0000000000	False
output best hypothesis found	0.0	0.0	0.0	2.0	0.0000000000	False
selection and the term	0.0	0.0	0.0	2.0	0.0000000000	False
fact that this feature	0.0	0.0	0.0	2.0	0.0000000000	False
described is a forward	0.0	0.0	0.0	2.0	0.0000000000	False
selection or forward search	0.0	0.0	0.0	2.0	0.0000000000	False
software that you write	0.0	0.0	0.0	2.0	0.0000000000	False
wraps around your learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense that to perform	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to train	0.0	0.0	0.0	2.0	0.0000000000	False
wrapper model feature selection	0.0	1.0	0.0	4.0	0.0000000000	True
re performing the search	0.0	0.0	0.0	2.0	0.0000000000	False
performing the search process	0.0	0.0	0.0	2.0	0.0000000000	False
repeatedly training your learning	0.0	0.0	0.0	2.0	0.0000000000	False
training your learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
backward search or backward	0.0	0.0	0.0	4.0	0.0000000000	False
search or backward selection	0.0	0.0	0.0	4.0	0.0000000000	False
start with f equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals the entire set	0.0	0.0	0.0	2.0	0.0000000000	False
nt even make sense	0.0	0.0	0.0	0.0	0.0000000000	False
make sense to initialize	0.0	0.0	0.0	4.0	0.0000000000	False
set of all features	0.0	0.0	0.0	2.0	0.0000000000	False
examples and 10,000 features	0.0	0.0	0.0	2.0	0.0000000000	False
emails and 10,000 training	0.0	0.0	0.0	2.0	0.0000000000	False
10,000 training 10,000 features	0.0	0.0	0.0	2.0	0.0000000000	False
10,000 features in email	0.0	0.0	0.0	2.0	0.0000000000	False
training examples then depending	0.0	0.0	0.0	2.0	0.0000000000	False
depending on the learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm you re	0.0	0.0	0.0	0.0	0.0000000000	False
model feature selection algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
feature selection algorithms tend	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms tend to work	0.0	0.0	0.0	2.0	0.0000000000	False
re computationally very expensive	0.0	0.0	0.0	2.0	0.0000000000	False
right so forward search	0.0	0.0	0.0	2.0	0.0000000000	False
forward search and backward	0.0	0.0	0.0	2.0	0.0000000000	False
search and backward search	0.0	0.0	0.0	2.0	0.0000000000	False
guarantee they ll find	0.0	0.0	0.0	0.0	0.0000000000	False
find the best subset	0.0	0.0	0.0	4.0	0.0000000000	False
features it actually turns	0.0	0.0	0.0	2.0	0.0000000000	False
formulizations of the feature	0.0	0.0	0.0	2.0	0.0000000000	False
problems it actually turns	0.0	0.0	0.0	2.0	0.0000000000	False
features but in practice	0.0	0.0	0.0	2.0	0.0000000000	False
forward selection backward selection	0.0	0.0	0.0	2.0	0.0000000000	False
selection backward selection work	0.0	0.0	0.0	2.0	0.0000000000	False
envision other search algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms where you sort	0.0	0.0	0.0	2.0	0.0000000000	False
search through the space	0.0	0.0	0.0	2.0	0.0000000000	False
end possible feature subsets	0.0	0.0	0.0	2.0	0.0000000000	False
selection tends to work	0.0	0.0	0.0	2.0	0.0000000000	False
computationally but for problems	0.0	0.0	0.0	2.0	0.0000000000	False
problems such as text	0.0	0.0	0.0	2.0	0.0000000000	False
text classification it turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns out for text	0.0	0.0	0.0	2.0	0.0000000000	False
out for text classification	0.0	0.0	0.0	2.0	0.0000000000	False
easily have 50,000 features	0.0	0.0	0.0	2.0	0.0000000000	False
50,000 features forward selection	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms that will give	0.0	0.0	0.0	2.0	0.0000000000	False
sense of generalization error	0.0	0.0	0.0	2.0	0.0000000000	False
error so you tend	0.0	0.0	0.0	2.0	0.0000000000	False
computationally much less expensive	0.0	0.0	0.0	2.0	0.0000000000	False
filter feature selection methods	0.0	0.0	0.0	2.0	0.0000000000	False
feature i will compute	0.0	0.0	0.0	2.0	0.0000000000	False
compute some rough estimate	0.0	0.0	0.0	2.0	0.0000000000	False
rough estimate or compute	0.0	0.0	0.0	2.0	0.0000000000	False
top k most correlated	0.0	0.0	0.0	2.0	0.0000000000	False
ideas in problem sets	0.0	0.0	0.0	2.0	0.0000000000	False
major information between feature	0.0	0.0	0.0	2.0	0.0000000000	False
write out the definition	0.0	0.0	0.0	2.0	0.0000000000	False
values of y times	0.0	0.0	0.0	2.0	0.0000000000	False
times the distribution times	0.0	0.0	0.0	2.0	0.0000000000	False
estimate from your training	0.0	0.0	0.0	2.0	0.0000000000	False
estimate from the training	0.0	0.0	0.0	2.0	0.0000000000	False
standard information theoretic measure	0.0	0.0	0.0	2.0	0.0000000000	False
class in information theory	0.0	0.0	0.0	2.0	0.0000000000	False
concepts of mutual information	0.0	0.0	0.0	2.0	0.0000000000	False
information is a measure	0.0	0.0	0.0	2.0	0.0000000000	False
distribution and this distribution	0.0	0.0	0.0	2.0	0.0000000000	False
non-independent in other words	0.0	0.0	0.0	2.0	0.0000000000	False
divergence will be large	0.0	0.0	0.0	2.0	0.0000000000	False
non-independent then that means	0.0	0.0	0.0	2.0	0.0000000000	False
information and this measure	0.0	0.0	0.0	2.0	0.0000000000	False
correlation or major information	0.0	0.0	0.0	2.0	0.0000000000	False
meaning that you compute	0.0	0.0	0.0	2.0	0.0000000000	False
features of mutual information	0.0	0.0	0.0	2.0	0.0000000000	False
include in your learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm the k features	0.0	0.0	0.0	2.0	0.0000000000	False
correlation with the label	0.0	0.0	0.0	2.0	0.0000000000	False
largest mutual information label	0.0	0.0	0.0	2.0	0.0000000000	False
sort them in decreasing	0.0	0.0	0.0	2.0	0.0000000000	False
order of mutual information	0.0	0.0	0.0	2.0	0.0000000000	False
decide how many features	0.0	0.0	0.0	2.0	0.0000000000	False
features includes using cross	0.0	0.0	0.0	2.0	0.0000000000	False
includes using cross validation	0.0	0.0	0.0	2.0	0.0000000000	False
choose this by hand	0.0	0.0	0.0	2.0	0.0000000000	False
great so next lecture	0.0	0.0	0.0	2.0	0.0000000000	False
lecture i ll continue	0.0	0.0	0.0	0.0	0.0000000000	False
continue i ll wrap	0.0	0.0	0.0	0.0	0.0000000000	False
good morning welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
today is actually wrap	0.0	0.0	0.0	2.0	0.0000000000	False
wrap up our discussion	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
learning theory and sort	0.0	0.0	0.0	2.0	0.0000000000	False
talking about bayesian statistics	0.0	0.0	0.0	4.0	0.0000000000	False
bayesian statistics and regularization	0.0	0.0	0.0	4.0	0.0000000000	False
applying machine learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms to problems	0.0	0.0	0.0	2.0	0.0000000000	False
project or other problems	0.0	0.0	0.0	2.0	0.0000000000	False
graduate from this class	0.0	0.0	0.0	2.0	0.0000000000	False
regularization so you remember	0.0	0.0	0.0	2.0	0.0000000000	False
remember from last week	0.0	0.0	0.0	2.0	0.0000000000	False
talk about learning theory	0.0	0.0	0.0	2.0	0.0000000000	False
theory and we learned	0.0	0.0	0.0	2.0	0.0000000000	False
variance and i guess	0.0	0.0	0.0	2.0	0.0000000000	False
lecture talking about algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms for model selection	0.0	0.0	0.0	2.0	0.0000000000	False
selection and for feature	0.0	0.0	0.0	2.0	0.0000000000	False
feature selection we talked	0.0	0.0	0.0	2.0	0.0000000000	False
talked about cross-validation right	0.0	0.0	0.0	2.0	0.0000000000	False
previous lecture were ways	0.0	0.0	0.0	2.0	0.0000000000	False
selection algorithms we talked	0.0	0.0	0.0	2.0	0.0000000000	False
fit and thereby reduce	0.0	0.0	0.0	2.0	0.0000000000	False
feature selection algorithms choose	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms choose a subset	0.0	0.0	0.0	2.0	0.0000000000	False
subset of the features	0.0	0.0	0.0	2.0	0.0000000000	False
today is to talk	0.0	0.0	0.0	2.0	0.0000000000	False
first model we learned	0.0	0.0	0.0	2.0	0.0000000000	False
parameters via maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
choose the parameters theta	0.0	0.0	0.0	2.0	0.0000000000	False
parameters theta that maximized	0.0	0.0	0.0	4.0	0.0000000000	False
probability of the data	0.0	0.0	0.0	4.0	0.0000000000	False
philosophical view behind writing	0.0	0.0	0.0	2.0	0.0000000000	False
true parameter theta out	0.0	0.0	0.0	2.0	0.0000000000	False
out there that generated	0.0	0.0	0.0	2.0	0.0000000000	False
parameter theta that govern	0.0	0.0	0.0	2.0	0.0000000000	False
theta that govern housing	0.0	0.0	0.0	2.0	0.0000000000	False
estimating the unknown value	0.0	0.0	0.0	2.0	0.0000000000	False
unknown value for theta	0.0	0.0	0.0	2.0	0.0000000000	False
procedure called maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
maximum likelihood for estimating	0.0	0.0	0.0	2.0	0.0000000000	False
frequencies procedure the alternative	0.0	0.0	0.0	2.0	0.0000000000	False
frequency school of statistics	0.0	0.0	0.0	2.0	0.0000000000	False
nt know what theta	0.0	0.0	0.0	0.0	0.0000000000	False
matrix given by tau	0.0	0.0	0.0	2.0	0.0000000000	False
denote my training set	0.0	0.0	0.0	2.0	0.0000000000	False
theta represents my beliefs	0.0	0.0	0.0	2.0	0.0000000000	False
absence of any data	0.0	0.0	0.0	2.0	0.0000000000	False
theta it probably represents	0.0	0.0	0.0	2.0	0.0000000000	False
sort of bayesian procedure	0.0	0.0	0.0	2.0	0.0000000000	False
posterior probability by parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters given my training	0.0	0.0	0.0	4.0	0.0000000000	False
board so my posterior	0.0	0.0	0.0	2.0	0.0000000000	False
posterior on my parameters	0.0	0.0	0.0	2.0	0.0000000000	False
rule let s call	0.0	0.0	0.0	0.0	0.0000000000	False
posterior and this distribution	0.0	0.0	0.0	2.0	0.0000000000	False
beliefs about what theta	0.0	0.0	0.0	2.0	0.0000000000	False
ve seen the training	0.0	0.0	0.0	2.0	0.0000000000	False
make a new prediction	0.0	0.0	0.0	2.0	0.0000000000	False
prediction on the price	0.0	0.0	0.0	2.0	0.0000000000	False
size of the house	0.0	0.0	0.0	2.0	0.0000000000	False
features of the house	0.0	0.0	0.0	2.0	0.0000000000	False
integral over my parameters	0.0	0.0	0.0	2.0	0.0000000000	False
times the posterior distribution	0.0	0.0	0.0	2.0	0.0000000000	False
posterior distribution of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta given the training	0.0	0.0	0.0	2.0	0.0000000000	False
input x in training	0.0	0.0	0.0	2.0	0.0000000000	False
integrate over y times	0.0	0.0	0.0	2.0	0.0000000000	False
respect to your posterior	0.0	0.0	0.0	2.0	0.0000000000	False
theta because this formula	0.0	0.0	0.0	2.0	0.0000000000	False
property of y conditioned	0.0	0.0	0.0	2.0	0.0000000000	False
conditioned on the values	0.0	0.0	0.0	2.0	0.0000000000	False
values of the random	0.0	0.0	0.0	2.0	0.0000000000	False
variables x and theta	0.0	0.0	0.0	2.0	0.0000000000	False
longer writing semicolon theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta as a random	0.0	0.0	0.0	2.0	0.0000000000	False
check are there questions	0.0	0.0	0.0	2.0	0.0000000000	False
make this more concrete	0.0	0.0	0.0	2.0	0.0000000000	False
steps in the computation	0.0	0.0	0.0	2.0	0.0000000000	False
difficult to compute integrals	0.0	0.0	0.0	2.0	0.0000000000	False
computing a full posterior	0.0	0.0	0.0	2.0	0.0000000000	False
quantity on the right-hand	0.0	0.0	0.0	4.0	0.0000000000	False
side and just maximize	0.0	0.0	0.0	2.0	0.0000000000	False
computing the full posterior	0.0	0.0	0.0	2.0	0.0000000000	False
maximum a posteriori estimate	0.0	0.0	0.0	2.0	0.0000000000	False
posteriori estimate of theta	0.0	0.0	0.0	2.0	0.0000000000	False
probable value of theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta onto your posterior	0.0	0.0	0.0	2.0	0.0000000000	False
max chi of theta	0.0	0.0	0.0	2.0	0.0000000000	False
map value of theta	0.0	0.0	0.0	2.0	0.0000000000	False
vector you d choose	0.0	0.0	0.0	0.0	0.0000000000	False
standard maximum likelihood estimation	0.0	0.0	0.0	2.0	0.0000000000	False
choosing the maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood value for theta	0.0	0.0	0.0	2.0	0.0000000000	False
times this other quantity	0.0	0.0	0.0	2.0	0.0000000000	False
centered around the point	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on feature selection	0.0	0.0	0.0	2.0	0.0000000000	False
reminiscent of feature selection	0.0	0.0	0.0	2.0	0.0000000000	False
points and you fit	0.0	0.0	0.0	2.0	0.0000000000	False
mind if you fit	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood estimation all right	0.0	0.0	0.0	2.0	0.0000000000	False
right ? in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
sort of bayesian regularization	0.0	0.0	5.99890789953	6.0	0.0000000000	False
sort of a smoother	0.0	0.0	0.0	2.0	0.0000000000	False
smoother and smoother fit	0.0	0.0	0.0	2.0	0.0000000000	False
fit to the data	0.0	0.0	0.0	2.0	0.0000000000	False
data as you decrease	0.0	0.0	0.0	2.0	0.0000000000	False
re driving the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
practice it s sort	0.0	0.0	0.0	0.0	0.0000000000	False
fitting a large number	0.0	0.0	0.0	2.0	0.0000000000	False
large number of parameters	0.0	0.0	0.0	2.0	0.0000000000	False
last piece of intuition	0.0	0.0	0.0	2.0	0.0000000000	False
ideas more in problem	0.0	0.0	0.0	2.0	0.0000000000	False
online later this week	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood tries to minimize	0.0	0.0	0.0	2.0	0.0000000000	False
right ? whereas maximum	0.0	0.0	0.0	2.0	0.0000000000	False
out to be minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
add this prior term	0.0	0.0	0.0	2.0	0.0000000000	False
out that the authorization	0.0	0.0	0.0	2.0	0.0000000000	False
authorization objective you end	0.0	0.0	0.0	2.0	0.0000000000	False
end up optimizing turns	0.0	0.0	0.0	2.0	0.0000000000	False
add an extra term	0.0	0.0	0.0	2.0	0.0000000000	False
penalizes your parameter theta	0.0	0.0	0.0	2.0	0.0000000000	False
theta as being large	0.0	0.0	0.0	2.0	0.0000000000	False
similar to maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
expect that you tend	0.0	0.0	0.0	2.0	0.0000000000	False
parameters has the effect	0.0	0.0	0.0	2.0	0.0000000000	False
sense when you play	0.0	0.0	0.0	2.0	0.0000000000	False
play with these ideas	0.0	0.0	0.0	2.0	0.0000000000	False
models like logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression and linear regression	0.0	0.0	0.0	2.0	0.0000000000	False
generalized in your models	0.0	0.0	0.0	2.0	0.0000000000	False
sorts of smoothing effects	0.0	0.0	0.0	2.0	0.0000000000	False
smoothing effects all right	0.0	0.0	0.0	2.0	0.0000000000	False
effects all right cool	0.0	0.0	0.0	2.0	0.0000000000	False
out that for problems	0.0	0.0	0.0	2.0	0.0000000000	False
problems like text classification	0.0	0.0	0.0	2.0	0.0000000000	False
features or 50,000 features	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm like logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
right ? so imagine	0.0	0.0	0.0	2.0	0.0000000000	False
imagine trying to build	0.0	0.0	0.0	2.0	0.0000000000	False
build a spam classifier	0.0	0.0	0.0	2.0	0.0000000000	False
effective text classification algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm with this sort	0.0	0.0	0.0	2.0	0.0000000000	False
pick and to pick	0.0	0.0	0.0	2.0	0.0000000000	False
pick either tau squared	0.0	0.0	0.0	4.0	0.0000000000	False
tau squared or lambda	0.0	0.0	0.0	4.0	0.0000000000	False
relation is lambda equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals one over tau	0.0	0.0	0.0	2.0	0.0000000000	False
cool so all right	0.0	0.0	0.0	2.0	0.0000000000	False
methods for preventing overfitting	0.0	0.0	0.0	2.0	0.0000000000	False
minutes talking about online	0.0	0.0	0.0	2.0	0.0000000000	False
talking about online learning	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a digression	0.0	0.0	0.0	2.0	0.0000000000	False
re designing the syllabus	0.0	0.0	0.0	2.0	0.0000000000	False
syllabus of a class	0.0	0.0	0.0	2.0	0.0000000000	False
good place to fit	0.0	0.0	0.0	2.0	0.0000000000	False
disjointed from the rest	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the class	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms we ve	0.0	0.0	0.0	0.0	0.0000000000	False
algorithms we ve talked	0.0	0.0	0.0	0.0	0.0000000000	False
re given a training	0.0	0.0	0.0	2.0	0.0000000000	False
run your learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm on the training	0.0	0.0	0.0	2.0	0.0000000000	False
learning setting called online	0.0	0.0	0.0	2.0	0.0000000000	False
setting called online learning	0.0	0.0	0.0	2.0	0.0000000000	False
problem sees all right	0.0	0.0	0.0	2.0	0.0000000000	False
first gon na give	0.0	0.0	0.0	4.0	0.0000000000	False
make a guess right	0.0	0.0	0.0	2.0	0.0000000000	False
right ? you guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess we ll call	0.0	0.0	0.0	0.0	0.0000000000	False
ll call your guess	0.0	0.0	0.0	2.0	0.0000000000	False
ve made your prediction	0.0	0.0	0.0	2.0	0.0000000000	False
show you x two	0.0	0.0	0.0	2.0	0.0000000000	False
slightly more educated guess	0.0	0.0	0.0	2.0	0.0000000000	False
educated guess and call	0.0	0.0	0.0	2.0	0.0000000000	False
ve made your guess	0.0	0.0	0.0	2.0	0.0000000000	False
reveal the true label	0.0	0.0	0.0	2.0	0.0000000000	False
lot of machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
machine learning and batch	0.0	0.0	0.0	2.0	0.0000000000	False
learning and batch learning	0.0	0.0	0.0	2.0	0.0000000000	False
user likes or dislikes	0.0	0.0	0.0	2.0	0.0000000000	False
examples so in online	0.0	0.0	0.0	2.0	0.0000000000	False
learning what you care	0.0	0.0	0.0	2.0	0.0000000000	False
sum from i equals	0.0	0.0	0.0	2.0	0.0000000000	False
sequence of m examples	0.0	0.0	0.0	2.0	0.0000000000	False
total number of mistakes	0.0	0.0	0.0	2.0	0.0000000000	False
make on a sequence	0.0	0.0	0.0	2.0	0.0000000000	False
finish all the learning	0.0	0.0	0.0	2.0	0.0000000000	False
apply to this setting	0.0	0.0	0.0	2.0	0.0000000000	False
re asked to make	0.0	0.0	0.0	2.0	0.0000000000	False
asked to make prediction	0.0	0.0	0.0	2.0	0.0000000000	False
prediction on y hat	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm and run	0.0	0.0	0.0	2.0	0.0000000000	False
run the learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
previous to being asked	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to make	0.0	0.0	0.0	2.0	0.0000000000	False
remember the perceptron algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
initial the parameter theta	0.0	0.0	0.0	2.0	0.0000000000	False
ve see this reel	0.0	0.0	0.0	2.0	0.0000000000	False
standard perceptron learning rule	0.0	0.0	0.0	2.0	0.0000000000	False
run one-step stochastic gradient	0.0	0.0	0.0	2.0	0.0000000000	False
one-step stochastic gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
reason i ve put	0.0	0.0	0.0	0.0	0.0000000000	False
sort of learning theorysection	0.0	0.0	0.0	2.0	0.0000000000	False
theorysection of this class	0.0	0.0	0.0	2.0	0.0000000000	False
prove fairly amazing results	0.0	0.0	0.0	2.0	0.0000000000	False
online error using algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
main lecture to prove	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature vectors	0.0	0.0	0.0	2.0	0.0000000000	False
infinite feature dimensional vectors	0.0	0.0	0.0	2.0	0.0000000000	False
vectors may use kernel	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative examples	0.0	0.0	3.99854386604	8.0	0.0000000000	False
negative examples are separated	0.0	0.0	0.0	2.0	0.0000000000	False
separated by a margin	0.0	0.0	0.0	2.0	0.0000000000	False
margin down there separating	0.0	0.0	0.0	2.0	0.0000000000	False
prove that perceptron algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
perceptron algorithm will converge	0.0	0.0	0.0	2.0	0.0000000000	False
converge to a hypothesis	0.0	0.0	0.0	2.0	0.0000000000	False
hypothesis that perfectly separates	0.0	0.0	0.0	2.0	0.0000000000	False
finite number of examples	0.0	0.0	0.0	2.0	0.0000000000	False
ll converge to digital	0.0	0.0	0.0	2.0	0.0000000000	False
boundary that perfectly separates	0.0	0.0	0.0	2.0	0.0000000000	False
sort of other things	0.0	0.0	0.0	2.0	0.0000000000	False
notes that i posted	0.0	0.0	0.0	2.0	0.0000000000	False
online for the purposes	0.0	0.0	0.0	2.0	0.0000000000	False
purposes of this class	0.0	0.0	0.0	2.0	0.0000000000	False
proof of this result	0.0	0.0	0.0	2.0	0.0000000000	False
specifically in the problem	0.0	0.0	0.0	2.0	0.0000000000	False
svms can have bounded	0.0	0.0	0.0	2.0	0.0000000000	False
prove learning theory results	0.0	0.0	0.0	2.0	0.0000000000	False
infinite dimensional feature spaces	0.0	0.0	0.0	2.0	0.0000000000	False
read in like half	0.0	0.0	0.0	2.0	0.0000000000	False
bound is actually proved	0.0	0.0	0.0	2.0	0.0000000000	False
based on stochastic gradient	0.0	0.0	0.0	2.0	0.0000000000	False
switch to powerpoint slides	0.0	0.0	0.0	2.0	0.0000000000	False
spend most of today	0.0	0.0	0.0	2.0	0.0000000000	False
today s lecture sort	0.0	0.0	0.0	0.0	0.0000000000	False
lecture sort of talking	0.0	0.0	0.0	2.0	0.0000000000	False
applying different machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
tools known to humankind	0.0	0.0	0.0	2.0	0.0000000000	False
humankind in machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
give you some advice	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you re	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms to work	0.0	0.0	0.0	2.0	0.0000000000	False
work well in problems	0.0	0.0	0.0	2.0	0.0000000000	False
conceptually most difficult material	0.0	0.0	0.0	2.0	0.0000000000	False
material in this class	0.0	0.0	0.0	2.0	0.0000000000	False
good machine learning people	0.0	0.0	0.0	2.0	0.0000000000	False
learning people will agree	0.0	0.0	0.0	2.0	0.0000000000	False
advice for doing machine	0.0	0.0	0.0	2.0	0.0000000000	False
work if you work	0.0	0.0	0.0	2.0	0.0000000000	False
work in the company	0.0	0.0	0.0	2.0	0.0000000000	False
product or you re	0.0	0.0	0.0	0.0	0.0000000000	False
learning system to work	0.0	0.0	0.0	2.0	0.0000000000	False
advice if you goal	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to invent	0.0	0.0	2.99890789953	6.0	0.0000000000	False
invent a new machine	0.0	0.0	0.0	2.0	0.0000000000	False
make machine learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
machine learning algorithm work	0.0	0.0	0.0	2.0	0.0000000000	False
deploy a working system	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostics for debugging learning	0.0	0.0	0.0	2.0	0.0000000000	False
talk briefly about error	0.0	0.0	0.0	2.0	0.0000000000	False
briefly about error analyses	0.0	0.0	0.0	2.0	0.0000000000	False
analyses and ablative analysis	0.0	0.0	0.0	4.0	0.0000000000	False
talk about just advice	0.0	0.0	0.0	2.0	0.0000000000	False
problem and one theme	0.0	0.0	0.0	2.0	0.0000000000	False
turns out you ve	0.0	0.0	0.0	0.0	0.0000000000	False
out you ve heard	0.0	0.0	0.0	0.0	0.0000000000	False
ve heard about premature	0.0	0.0	0.0	2.0	0.0000000000	False
heard about premature optimization	0.0	0.0	0.0	2.0	0.0000000000	False
over-designs from the start	0.0	0.0	0.0	2.0	0.0000000000	False
writing piece of code	0.0	0.0	0.0	2.0	0.0000000000	False
code and they choose	0.0	0.0	0.0	2.0	0.0000000000	False
subroutine to optimize heavily	0.0	0.0	0.0	2.0	0.0000000000	False
guilty of premature optimization	0.0	0.0	0.0	2.0	0.0000000000	False
code to run faster	0.0	0.0	0.0	2.0	0.0000000000	False
faster and we choose	0.0	0.0	0.0	2.0	0.0000000000	False
choose probably a piece	0.0	0.0	0.0	2.0	0.0000000000	False
code and we implement	0.0	0.0	0.0	2.0	0.0000000000	False
quickly and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
bottleneck in the code	0.0	0.0	0.0	2.0	0.0000000000	False
call that premature optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization and in undergraduate	0.0	0.0	0.0	2.0	0.0000000000	False
premature optimization and people	0.0	0.0	0.0	2.0	0.0000000000	False
right ? and turns	0.0	0.0	0.0	2.0	0.0000000000	False
thing happens in building	0.0	0.0	0.0	2.0	0.0000000000	False
systems that many people	0.0	0.0	0.0	2.0	0.0000000000	False
part of a machine	0.0	0.0	0.0	2.0	0.0000000000	False
system and that turns	0.0	0.0	0.0	2.0	0.0000000000	False
first talk about debugging	0.0	0.0	0.0	2.0	0.0000000000	False
talk about debugging learning	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms as a motivating	0.0	0.0	0.0	2.0	0.0000000000	False
build an anti-spam system	0.0	0.0	1.99890789953	6.0	0.0000000000	False
chosen a small set	0.0	0.0	0.0	2.0	0.0000000000	False
implement bayesian logistic regression	0.0	0.0	0.0	4.0	0.0000000000	False
additional lambda squared term	0.0	0.0	0.0	2.0	0.0000000000	False
term and we re	0.0	0.0	0.0	0.0	0.0000000000	False
maximizing rather than minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
minus lambda theta square	0.0	0.0	0.0	2.0	0.0000000000	False
squared so the question	0.0	0.0	0.0	2.0	0.0000000000	False
bayesian logistic regression algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
ways you could improve	0.0	0.0	0.0	2.0	0.0000000000	False
ll try to improve	0.0	0.0	0.0	2.0	0.0000000000	False
examples maybe you suspect	0.0	0.0	0.0	2.0	0.0000000000	False
smaller set of features	0.0	0.0	0.0	4.0	0.0000000000	False
figure out better features	0.0	0.0	0.0	2.0	0.0000000000	False
emails or whatever right	0.0	0.0	0.0	2.0	0.0000000000	False
suspect that gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
gradient descent a bit	0.0	0.0	0.0	2.0	0.0000000000	False
descent a bit longer	0.0	0.0	0.0	2.0	0.0000000000	False
run gradient descent longer	0.0	0.0	0.0	2.0	0.0000000000	False
remember hearing from class	0.0	0.0	0.0	2.0	0.0000000000	False
class that maybe newton	0.0	0.0	0.0	2.0	0.0000000000	False
newton s method converges	0.0	0.0	0.0	0.0	0.0000000000	False
improve a learning system	0.0	0.0	0.0	2.0	0.0000000000	False
picking ways to improve	0.0	0.0	0.0	2.0	0.0000000000	False
improve the learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm and picking	0.0	0.0	0.0	2.0	0.0000000000	False
work in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
largely largely a matter	0.0	0.0	0.0	2.0	0.0000000000	False
fixing what the problem	0.0	0.0	0.0	2.0	0.0000000000	False
fix very different problems	0.0	0.0	0.0	2.0	0.0000000000	False
save yourself a lot	0.0	0.0	0.0	2.0	0.0000000000	False
industry and in research	0.0	0.0	0.0	2.0	0.0000000000	False
change a learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
randomly there are lots	0.0	0.0	0.0	2.0	0.0000000000	False
things that obviously improve	0.0	0.0	0.0	2.0	0.0000000000	False
improve your learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
good ones that run	0.0	0.0	0.0	2.0	0.0000000000	False
figure out the problem	0.0	0.0	0.0	4.0	0.0000000000	False
bayesian logistic regression test	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression test error	0.0	0.0	0.0	2.0	0.0000000000	False
problem is either overfitting	0.0	0.0	0.0	2.0	0.0000000000	False
forget forget the tables	0.0	0.0	0.0	2.0	0.0000000000	False
forget the tables suppose	0.0	0.0	0.0	2.0	0.0000000000	False
tables suppose you suspect	0.0	0.0	0.0	2.0	0.0000000000	False
bias or high variance	0.0	0.0	5.99854386604	8.0	0.0000000000	False
features classified as spam	0.0	0.0	0.0	2.0	0.0000000000	False
out whether the problem	0.0	0.0	0.0	2.0	0.0000000000	False
high variance ? right	0.0	0.0	0.0	2.0	0.0000000000	False
problem is high bias	0.0	0.0	0.0	2.0	0.0000000000	False
variance if you remember	0.0	0.0	0.0	2.0	0.0000000000	False
previously for high variance	0.0	0.0	0.0	2.0	0.0000000000	False
high variance the training	0.0	0.0	0.0	2.0	0.0000000000	False
variance the training error	0.0	0.0	0.0	2.0	0.0000000000	False
lower than the test	0.0	0.0	0.0	2.0	0.0000000000	False
test error all right	0.0	0.0	0.0	2.0	0.0000000000	False
re fitting your training	0.0	0.0	0.0	2.0	0.0000000000	False
fitting your training set	0.0	0.0	0.0	4.0	0.0000000000	False
data points all right	0.0	0.0	0.0	2.0	0.0000000000	False
fitting the data set	0.0	0.0	0.0	2.0	0.0000000000	False
lower than your test	0.0	0.0	0.0	2.0	0.0000000000	False
error and in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
fitting a linear function	0.0	0.0	0.0	2.0	0.0000000000	False
curve for high variance	0.0	0.0	0.0	2.0	0.0000000000	False
plotting the training set	0.0	0.0	0.0	2.0	0.0000000000	False
notice as the training	0.0	0.0	0.0	2.0	0.0000000000	False
increase the training set	0.0	0.0	0.0	2.0	0.0000000000	False
extrapolate the green curve	0.0	0.0	0.0	2.0	0.0000000000	False
set error will decrease	0.0	0.0	0.0	2.0	0.0000000000	False
right ? another thing	0.0	0.0	0.0	2.0	0.0000000000	False
line is the desired	0.0	0.0	0.0	2.0	0.0000000000	False
desired performance you re	0.0	0.0	0.0	0.0	0.0000000000	False
re trying to reach	0.0	0.0	0.0	2.0	0.0000000000	False
out that your training	0.0	0.0	0.0	2.0	0.0000000000	False
error will actually grow	0.0	0.0	0.0	4.0	0.0000000000	False
grow as a function	0.0	0.0	3.99890789953	6.0	0.0000000000	False
function of the training	0.0	0.0	0.0	2.0	0.0000000000	False
larger your training set	0.0	0.0	0.0	2.0	0.0000000000	False
training set perfectly right	0.0	0.0	0.0	2.0	0.0000000000	False
function of your training	0.0	0.0	0.0	4.0	0.0000000000	False
set size because smart	0.0	0.0	0.0	2.0	0.0000000000	False
size because smart training	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostic for high variance	0.0	0.0	0.0	2.0	0.0000000000	False
training versus test error	0.0	0.0	0.0	2.0	0.0000000000	False
case of high variance	0.0	0.0	0.0	2.0	0.0000000000	False
curve for test error	0.0	0.0	0.0	2.0	0.0000000000	False
test error has flattened	0.0	0.0	0.0	2.0	0.0000000000	False
property of high bias	0.0	0.0	0.0	2.0	0.0000000000	False
hold out test set	0.0	0.0	0.0	2.0	0.0000000000	False
out test set error	0.0	0.0	0.0	2.0	0.0000000000	False
error if you find	0.0	0.0	0.0	2.0	0.0000000000	False
right ? in fact	0.0	0.0	0.0	2.0	0.0000000000	False
level of desired performance	0.0	0.0	0.0	4.0	0.0000000000	False
reduce your training error	0.0	0.0	0.0	2.0	0.0000000000	False
desired level of performance	0.0	0.0	0.0	2.0	0.0000000000	False
level of performance right	0.0	0.0	0.0	2.0	0.0000000000	False
green curve on test	0.0	0.0	0.0	2.0	0.0000000000	False
curve on test error	0.0	0.0	0.0	2.0	0.0000000000	False
personally tend to find	0.0	0.0	0.0	2.0	0.0000000000	False
problem or a variance	0.0	0.0	0.0	2.0	0.0000000000	False
training and test error	0.0	0.0	0.0	4.0	0.0000000000	False
back to the list	0.0	0.0	0.0	2.0	0.0000000000	False
high variance all right	0.0	0.0	0.0	2.0	0.0000000000	False
larger set of features	0.0	0.0	0.0	2.0	0.0000000000	False
features or adding email	0.0	0.0	0.0	2.0	0.0000000000	False
nt have enough features	0.0	0.0	0.0	0.0	0.0000000000	False
people working on machine	0.0	0.0	0.0	2.0	0.0000000000	False
working on machine learning	0.0	0.0	0.0	4.0	0.0000000000	False
ll build a learning	0.0	0.0	0.0	2.0	0.0000000000	False
build a learning system	0.0	0.0	0.0	2.0	0.0000000000	False
money and effort collecting	0.0	0.0	0.0	2.0	0.0000000000	False
effort collecting more training	0.0	0.0	0.0	2.0	0.0000000000	False
collecting more training data	0.0	0.0	0.0	2.0	0.0000000000	False
months or six months	0.0	0.0	0.0	2.0	0.0000000000	False
silicon valley and companies	0.0	0.0	0.0	2.0	0.0000000000	False
people building various machine	0.0	0.0	0.0	2.0	0.0000000000	False
building various machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
people spending six months	0.0	0.0	0.0	2.0	0.0000000000	False
spending six months working	0.0	0.0	0.0	2.0	0.0000000000	False
months working on fixing	0.0	0.0	0.0	2.0	0.0000000000	False
fixing a learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
told them six months	0.0	0.0	0.0	2.0	0.0000000000	False
nt possibly have helped	0.0	0.0	0.0	0.0	0.0000000000	False
easily spend six months	0.0	0.0	0.0	2.0	0.0000000000	False
months trying to invent	0.0	0.0	0.0	2.0	0.0000000000	False
depressing you could ve	0.0	0.0	0.0	0.0	0.0000000000	False
told you six months	0.0	0.0	0.0	2.0	0.0000000000	False
two of these solutions	0.0	0.0	0.0	2.0	0.0000000000	False
save yourself many months	0.0	0.0	0.0	2.0	0.0000000000	False
months of fruitless effort	0.0	0.0	0.0	2.0	0.0000000000	False
four at the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
great so bias versus	0.0	0.0	0.0	2.0	0.0000000000	False
variance is one thing	0.0	0.0	0.0	2.0	0.0000000000	False
out your own diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
out what s wrong	0.0	0.0	0.0	0.0	0.0000000000	False
algorithm is nt working	0.0	0.0	0.0	0.0	0.0000000000	False
construct your own tests	0.0	0.0	0.0	2.0	0.0000000000	False
difference training and test	0.0	0.0	0.0	2.0	0.0000000000	False
construct your own diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
illustrate another common question	0.0	0.0	0.0	2.0	0.0000000000	False
percent error on spam	0.0	0.0	0.0	4.0	0.0000000000	False
error on spam mail	0.0	0.0	0.0	2.0	0.0000000000	False
percent error non-spam mail	0.0	0.0	0.0	2.0	0.0000000000	False
error non-spam mail right	0.0	0.0	0.0	2.0	0.0000000000	False
percent of your spam	0.0	0.0	0.0	2.0	0.0000000000	False
percent of all spam	0.0	0.0	0.0	2.0	0.0000000000	False
percent of the email	0.0	0.0	0.0	2.0	0.0000000000	False
email from your friends	0.0	0.0	0.0	2.0	0.0000000000	False
machine using a linear	0.0	0.0	0.0	2.0	0.0000000000	False
percent error on non-spam	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to build	0.0	0.0	0.0	2.0	0.0000000000	False
regression to your customers	0.0	0.0	0.0	2.0	0.0000000000	False
retrain overnight every day	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression just runs	0.0	0.0	0.0	2.0	0.0000000000	False
out well so question	0.0	0.0	0.0	2.0	0.0000000000	False
problem with logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
iterations and it turns	0.0	0.0	0.0	2.0	0.0000000000	False
optimizing j of theta	0.0	0.0	0.0	4.0	0.0000000000	False
objective as a function	0.0	0.0	0.0	2.0	0.0000000000	False
function of the number	0.0	0.0	0.0	2.0	0.0000000000	False
curve has already flattened	0.0	0.0	0.0	2.0	0.0000000000	False
flattened out all right	0.0	0.0	0.0	2.0	0.0000000000	False
run this ten times	0.0	0.0	0.0	4.0	0.0000000000	False
logistic regression is converged	0.0	0.0	0.0	4.0	0.0000000000	False
curve the other question	0.0	0.0	0.0	2.0	0.0000000000	False
thing you might suspect	0.0	0.0	0.0	2.0	0.0000000000	False
suspect is a problem	0.0	0.0	0.0	2.0	0.0000000000	False
optimizing the right function	0.0	0.0	0.0	2.0	0.0000000000	False
sum over your examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples of some weights	0.0	0.0	0.0	2.0	0.0000000000	False
non-spam than for spam	0.0	0.0	0.0	2.0	0.0000000000	False
mail because you care	0.0	0.0	0.0	2.0	0.0000000000	False
predictions correct for spam	0.0	0.0	0.0	2.0	0.0000000000	False
correct for spam email	0.0	0.0	0.0	2.0	0.0000000000	False
theta is the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
sort of maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
function to be optimizing	0.0	0.0	0.0	2.0	0.0000000000	False
switching to support vector	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine optimization	0.0	0.0	0.0	2.0	0.0000000000	False
vector machine optimization objective	0.0	0.0	0.0	2.0	0.0000000000	False
out is the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
problem with the optimization	0.0	0.0	0.0	4.0	0.0000000000	False
optimization objective i chose	0.0	0.0	0.0	2.0	0.0000000000	False
outperforms bayesian logistic regression	0.0	0.0	0.0	4.0	0.0000000000	False
deploy bayesian logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression to your problem	0.0	0.0	0.0	2.0	0.0000000000	False
learned by an svm	0.0	0.0	0.0	2.0	0.0000000000	False
ll let theta subscript	0.0	0.0	0.0	2.0	0.0000000000	False
blr be the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
regression so the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
optimization objective you care	0.0	0.0	0.0	2.0	0.0000000000	False
criteria that i talked	0.0	0.0	0.0	2.0	0.0000000000	False
support vector machine outperforms	0.0	0.0	0.0	2.0	0.0000000000	False
regression tries to optimize	0.0	0.0	0.0	2.0	0.0000000000	False
optimize an optimization objective	0.0	0.0	0.0	2.0	0.0000000000	False
less-than j of blr	0.0	0.0	0.0	2.0	0.0000000000	False
weighted accuracy of support	0.0	0.0	0.0	2.0	0.0000000000	False
accuracy of support vector	0.0	0.0	0.0	2.0	0.0000000000	False
bigger than this weighted	0.0	0.0	0.0	2.0	0.0000000000	False
regression so in order	0.0	0.0	0.0	2.0	0.0000000000	False
optimizing the wrong objective	0.0	0.0	0.0	2.0	0.0000000000	False
check if this equality	0.0	0.0	0.0	2.0	0.0000000000	False
copied over in case	0.0	0.0	0.0	2.0	0.0000000000	False
maximize j of theta	0.0	0.0	5.99890789953	6.0	0.0000000000	False
regression so this means	0.0	0.0	0.0	2.0	0.0000000000	False
value of theta output	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression actually fails	0.0	0.0	0.0	2.0	0.0000000000	False
support back to machine	0.0	0.0	5.99890789953	6.0	0.0000000000	False
optimization algorithm the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm the optimization algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm has nt converged	0.0	0.0	0.0	0.0	0.0000000000	False
converged the other case	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression actually attains	0.0	0.0	0.0	2.0	0.0000000000	False
attains the higher value	0.0	0.0	0.0	2.0	0.0000000000	False
value for the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
worse on your optimization	0.0	0.0	0.0	4.0	0.0000000000	False
maximizing your weighted accuracy	0.0	0.0	0.0	2.0	0.0000000000	False
objective to be maximizing	0.0	0.0	0.0	2.0	0.0000000000	False
nt a good objective	0.0	0.0	0.0	0.0	0.0000000000	False
objective to be choosing	0.0	0.0	0.0	2.0	0.0000000000	False
choosing if you care	0.0	0.0	0.0	2.0	0.0000000000	False
care about the weighted	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this made	0.0	0.0	0.0	2.0	0.0000000000	False
made sense ? cool	0.0	0.0	0.0	2.0	0.0000000000	False
good so that tells	0.0	0.0	0.0	2.0	0.0000000000	False
descent for more iterations	0.0	0.0	0.0	2.0	0.0000000000	False
fixes the optimization algorithm	0.0	0.0	5.99890789953	6.0	0.0000000000	False
method fixes the optimization	0.0	0.0	0.0	2.0	0.0000000000	False
times norm of data	0.0	0.0	0.0	2.0	0.0000000000	False
norm of data squared	0.0	0.0	0.0	2.0	0.0000000000	False
fixes the optimization objective	0.0	0.0	5.99890789953	6.0	0.0000000000	False
optimization objective and changing	0.0	0.0	0.0	2.0	0.0000000000	False
changing to an svm	0.0	0.0	0.0	2.0	0.0000000000	False
objective and be working	0.0	0.0	0.0	2.0	0.0000000000	False
pattern that the problem	0.0	0.0	0.0	2.0	0.0000000000	False
iterations of gradient descent	0.0	0.0	0.0	2.0	0.0000000000	False
descent like trying newton	0.0	0.0	0.0	2.0	0.0000000000	False
method and trying conjugate	0.0	0.0	0.0	2.0	0.0000000000	False
nt going to fix	0.0	0.0	0.0	0.0	0.0000000000	False
fixing your optimization algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
optimization algorithm or fixing	0.0	0.0	0.0	2.0	0.0000000000	False
work on flying helicopters	0.0	0.0	0.0	2.0	0.0000000000	False
draws on reinforcement learning	0.0	0.0	0.0	2.0	0.0000000000	False
close to the end	0.0	0.0	0.0	2.0	0.0000000000	False
ve talked about reinforcement	0.0	0.0	0.0	2.0	0.0000000000	False
talked about reinforcement learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning in the class	0.0	0.0	0.0	2.0	0.0000000000	False
understand it more deeply	0.0	0.0	0.0	2.0	0.0000000000	False
machine-learning algorithm to design	0.0	0.0	0.0	2.0	0.0000000000	False
step was you build	0.0	0.0	0.0	2.0	0.0000000000	False
simulator for a helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
screenshot of our simulator	0.0	0.0	0.0	2.0	0.0000000000	False
choose a cost function	0.0	0.0	0.0	2.0	0.0000000000	False
ll call it cost	0.0	0.0	0.0	2.0	0.0000000000	False
call it cost function	0.0	0.0	0.0	2.0	0.0000000000	False
error in your helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
run a reinforcement-learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
learn about rl algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
run reinforcement learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm in your simulator	0.0	0.0	0.0	2.0	0.0000000000	False
minimize this cost function	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the squared error	0.0	0.0	0.0	2.0	0.0000000000	False
re controlling your helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm will output	0.0	0.0	0.0	2.0	0.0000000000	False
run this learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
set of controller parameters	0.0	0.0	0.0	2.0	0.0000000000	False
capture the aerodynamic effects	0.0	0.0	0.0	2.0	0.0000000000	False
aerodynamic effects more accurately	0.0	0.0	0.0	2.0	0.0000000000	False
airflow and the turbulence	0.0	0.0	0.0	2.0	0.0000000000	False
affects around the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
modify the cost function	0.0	0.0	0.0	2.0	0.0000000000	False
function maybe your square	0.0	0.0	0.0	2.0	0.0000000000	False
error is nt cutting	0.0	0.0	0.0	0.0	0.0000000000	False
reasoning that i wanted	0.0	0.0	0.0	2.0	0.0000000000	False
reinforcement-learning algorithm does poorly	0.0	0.0	0.0	2.0	0.0000000000	False
things hold true suppose	0.0	0.0	0.0	2.0	0.0000000000	False
true suppose the contrary	0.0	0.0	0.0	2.0	0.0000000000	False
suppose that the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
model of our helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
suppose that the reinforcement	0.0	0.0	0.0	2.0	0.0000000000	False
correctly controls the helicopter	0.0	0.0	0.0	4.0	0.0000000000	False
run a learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm in simulation	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm can crash	0.0	0.0	0.0	2.0	0.0000000000	False
assume our reinforcement-learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
reinforcement-learning algorithm correctly controls	0.0	0.0	0.0	2.0	0.0000000000	False
minimize the cost function	0.0	0.0	3.99854386604	8.0	0.2914572864	False
function j of theta	0.0	0.0	0.0	4.0	0.0000000000	False
minimizing j of theta	0.0	0.0	2.99890789953	6.0	0.0000000000	False
theta does indeed correspond	0.0	0.0	0.0	2.0	0.0000000000	False
means that the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
fact that the learning	0.0	0.0	0.0	2.0	0.0000000000	False
flies well in simulation	0.0	0.0	0.0	2.0	0.0000000000	False
simulator of the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
tells me the problem	0.0	0.0	0.0	4.0	0.0000000000	False
right ? my simulator	0.0	0.0	0.0	2.0	0.0000000000	False
simulator predicts the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
spend out efforts improving	0.0	0.0	0.0	2.0	0.0000000000	False
efforts improving the accuracy	0.0	0.0	0.0	2.0	0.0000000000	False
accuracy of our simulator	0.0	0.0	0.0	2.0	0.0000000000	False
write theta subscript human	0.0	0.0	0.0	2.0	0.0000000000	False
control policy all right	0.0	0.0	0.0	2.0	0.0000000000	False
human pilot s flight	0.0	0.0	0.0	0.0	0.0000000000	False
optimizing this objective function	0.0	0.0	0.0	2.0	0.0000000000	False
good human pilot attains	0.0	0.0	0.0	2.0	0.0000000000	False
pilot attains a worse	0.0	0.0	0.0	2.0	0.0000000000	False
attains a worse value	0.0	0.0	0.0	2.0	0.0000000000	False
value on my optimization	0.0	0.0	0.0	2.0	0.0000000000	False
attains a lower value	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm s not managing	0.0	0.0	0.0	0.0	0.0000000000	False
theta and that tells	0.0	0.0	0.0	2.0	0.0000000000	False
attains a larger value	0.0	0.0	0.0	4.0	0.0000000000	False
larger value for theta	0.0	0.0	0.0	2.0	0.0000000000	False
value for theta excuse	0.0	0.0	0.0	2.0	0.0000000000	False
larger mean squared error	0.0	0.0	0.0	2.0	0.0000000000	False
error for the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
worse on my cost	0.0	0.0	0.0	2.0	0.0000000000	False
cost function but flies	0.0	0.0	0.0	2.0	0.0000000000	False
cost function it means	0.0	0.0	0.0	2.0	0.0000000000	False
cost function my learning	0.0	0.0	0.0	2.0	0.0000000000	False
function my learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
job minimizing the cost	0.0	0.0	0.0	2.0	0.0000000000	False
pilot so that tells	0.0	0.0	0.0	2.0	0.0000000000	False
tells you that minimizing	0.0	0.0	0.0	2.0	0.0000000000	False
function does nt correspond	0.0	0.0	0.0	0.0	0.0000000000	False
change j of theta	0.0	0.0	0.0	2.0	0.0000000000	False
work often reinforcement learning	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms just work	0.0	0.0	0.0	2.0	0.0000000000	False
focusing on the simulator	0.0	0.0	0.0	2.0	0.0000000000	False
changing the cost function	0.0	0.0	0.0	2.0	0.0000000000	False
changing the reinforcement learning	0.0	0.0	0.0	2.0	0.0000000000	False
building a better simulator	0.0	0.0	0.0	2.0	0.0000000000	False
simulator for your helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
helicopter but it turns	0.0	0.0	0.0	2.0	0.0000000000	False
turns out that modeling	0.0	0.0	0.0	2.0	0.0000000000	False
out that modeling helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
active area of research	0.0	0.0	0.0	2.0	0.0000000000	False
research there are people	0.0	0.0	0.0	2.0	0.0000000000	False
writing entire phd theses	0.0	0.0	0.0	2.0	0.0000000000	False
write a phd thesis	0.0	0.0	0.0	2.0	0.0000000000	False
phd thesis and build	0.0	0.0	0.0	2.0	0.0000000000	False
fixing the wrong problem	0.0	0.0	0.0	2.0	0.0000000000	False
out what s happening	0.0	0.0	0.0	0.0	0.0000000000	False
happening in an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
ve described are sort	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostics that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
learning algorithm is working	0.0	0.0	0.0	2.0	0.0000000000	False
good idea to run	0.0	0.0	0.0	2.0	0.0000000000	False
idea to run diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
understand your application problem	0.0	0.0	0.0	2.0	0.0000000000	False
high-paying job to apply	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms to some application	0.0	0.0	0.0	2.0	0.0000000000	False
specific important machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
important machine learning application	0.0	0.0	0.0	2.0	0.0000000000	False
application for many months	0.0	0.0	0.0	2.0	0.0000000000	False
understanding of what works	0.0	0.0	0.0	2.0	0.0000000000	False
nt work your problem	0.0	0.0	0.0	0.0	0.0000000000	False
work your problem sort	0.0	0.0	0.0	2.0	0.0000000000	False
problem sort of right	0.0	0.0	0.0	2.0	0.0000000000	False
companies with important machine	0.0	0.0	0.0	2.0	0.0000000000	False
important machine learning problems	0.0	0.0	0.0	2.0	0.0000000000	False
months or for years	0.0	0.0	0.0	2.0	0.0000000000	False
important problem using learning	0.0	0.0	0.0	2.0	0.0000000000	False
problem using learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
understanding of the problem	0.0	0.0	0.0	4.0	0.0000000000	False
understanding of these problems	0.0	0.0	0.0	2.0	0.0000000000	False
valley companies that outsource	0.0	0.0	0.0	2.0	0.0000000000	False
outsource their machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
company in silicon valley	0.0	0.0	0.0	2.0	0.0000000000	False
firm in new york	0.0	0.0	0.0	2.0	0.0000000000	False
run all their learning	0.0	0.0	0.0	2.0	0.0000000000	False
understanding of your data	0.0	0.0	0.0	2.0	0.0000000000	False
nt maintain that expertise	0.0	0.0	0.0	0.0	0.0000000000	False
problem you really care	0.0	0.0	0.0	2.0	0.0000000000	False
problem that you build	0.0	0.0	0.0	2.0	0.0000000000	False
build up over months	0.0	0.0	0.0	2.0	0.0000000000	False
ll be really valuable	0.0	0.0	0.0	2.0	0.0000000000	False
reason for running diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
right ? so diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostics and error analyses	0.0	0.0	0.0	2.0	0.0000000000	False
insight about the problem	0.0	0.0	0.0	2.0	0.0000000000	False
justify your research claims	0.0	0.0	0.0	2.0	0.0000000000	False
writing a research paper	0.0	0.0	0.0	4.0	0.0000000000	False
helicopter and it flies,or	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on error analysis	0.0	0.0	0.0	2.0	0.0000000000	False
good machine learning practice	0.0	0.0	0.0	2.0	0.0000000000	False
understanding what your sources	0.0	0.0	0.0	2.0	0.0000000000	False
wrong with the helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
flown so many times	0.0	0.0	0.0	2.0	0.0000000000	False
helicopter is actually building	0.0	0.0	0.0	2.0	0.0000000000	False
building an accurate simulator	0.0	0.0	0.0	2.0	0.0000000000	False
simulator of a helicopter	0.0	0.0	0.0	2.0	0.0000000000	False
helicopter is very hard	0.0	0.0	0.0	2.0	0.0000000000	False
out what is working	0.0	0.0	0.0	2.0	0.0000000000	False
working in your algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
working and we re	0.0	0.0	0.0	0.0	0.0000000000	False
re gon na talk	0.0	0.0	0.0	2.0	0.0000000000	False
sort of ia systems	0.0	0.0	0.0	2.0	0.0000000000	False
combine many different components	0.0	0.0	0.0	2.0	0.0000000000	False
components into a pipeline	0.0	0.0	0.0	2.0	0.0000000000	False
sort of a contrived	0.0	0.0	0.0	2.0	0.0000000000	False
dissimilar in many ways	0.0	0.0	0.0	2.0	0.0000000000	False
actual machine learning systems	0.0	0.0	0.0	2.0	0.0000000000	False
recognize people from images	0.0	0.0	0.0	2.0	0.0000000000	False
input in camera image	0.0	0.0	0.0	2.0	0.0000000000	False
run a face detection	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithm to detect	0.0	0.0	0.0	4.0	0.0000000000	False
algorithm to detect people	0.0	0.0	0.0	2.0	0.0000000000	False
detect people s faces	0.0	0.0	0.0	0.0	0.0000000000	False
people s faces right	0.0	0.0	0.0	0.0	0.0000000000	False
identity of the person	0.0	0.0	0.0	2.0	0.0000000000	False
segment of the eyes	0.0	0.0	0.0	2.0	0.0000000000	False
segment of the nose	0.0	0.0	0.0	2.0	0.0000000000	False
friend after she sees	0.0	0.0	0.0	2.0	0.0000000000	False
found all these features	0.0	0.0	0.0	2.0	0.0000000000	False
feed all the features	0.0	0.0	0.0	2.0	0.0000000000	False
regression or soft match	0.0	0.0	0.0	2.0	0.0000000000	False
identity of this person	0.0	0.0	0.0	2.0	0.0000000000	False
long complicated pipeline combining	0.0	0.0	0.0	2.0	0.0000000000	False
pipeline combining many machine	0.0	0.0	0.0	2.0	0.0000000000	False
combining many machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
error can be attributed	0.0	0.0	0.0	2.0	0.0000000000	False
typical error analysis procedure	0.0	0.0	0.0	2.0	0.0000000000	False
ground-truth for each component	0.0	0.0	0.0	2.0	0.0000000000	False
figure on the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
accuracy of the system	0.0	0.0	0.0	2.0	0.0000000000	False
implement my correct background	0.0	0.0	0.0	2.0	0.0000000000	False
correct background versus foreground	0.0	0.0	0.0	2.0	0.0000000000	False
giving that ground-truth data	0.0	0.0	0.0	2.0	0.0000000000	False
data in the test	0.0	0.0	0.0	2.0	0.0000000000	False
assume our accuracy increases	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm where the face	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm s accuracy increases	0.0	0.0	0.0	0.0	0.0000000000	False
components and just give	0.0	0.0	0.0	2.0	0.0000000000	False
out where the nose	0.0	0.0	0.0	2.0	0.0000000000	False
nt have to figure	0.0	0.0	0.0	0.0	0.0000000000	False
giving it the correct	0.0	0.0	0.0	2.0	0.0000000000	False
output label and end	0.0	0.0	0.0	2.0	0.0000000000	False
giving the ground-truth labels	0.0	0.0	0.0	2.0	0.0000000000	False
components could help boost	0.0	0.0	0.0	2.0	0.0000000000	False
boost your final performance	0.0	0.0	0.0	2.0	0.0000000000	False
added the face detection	0.0	0.0	0.0	2.0	0.0000000000	False
percent whereas in contrast	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to improve	0.0	0.0	0.0	2.0	0.0000000000	False
improve your background subtraction	0.0	0.0	0.0	2.0	0.0000000000	False
larger potential for gains	0.0	0.0	0.0	2.0	0.0000000000	False
easily choose to spend	0.0	0.0	0.0	2.0	0.0000000000	False
right ? and choosing	0.0	0.0	0.0	2.0	0.0000000000	False
choosing the right piece	0.0	0.0	0.0	2.0	0.0000000000	False
sort of diagnostic tells	0.0	0.0	0.0	2.0	0.0000000000	False
sort of another type	0.0	0.0	0.0	2.0	0.0000000000	False
analyses that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
talked about the error	0.0	0.0	0.0	2.0	0.0000000000	False
analysis i just talked	0.0	0.0	0.0	2.0	0.0000000000	False
current performance and perfect	0.0	0.0	0.0	2.0	0.0000000000	False
performance and perfect performance	0.0	0.0	0.0	2.0	0.0000000000	False
sort of ablative analysis	0.0	0.0	0.0	2.0	0.0000000000	False
analysis tries to explain	0.0	0.0	0.0	2.0	0.0000000000	False
difference between some baselines	0.0	0.0	0.0	2.0	0.0000000000	False
suppose you ve built	0.0	0.0	0.0	0.0	0.0000000000	False
anti-spam classifier for adding	0.0	0.0	0.0	2.0	0.0000000000	False
classifier for adding lots	0.0	0.0	0.0	2.0	0.0000000000	False
adding lots of clever	0.0	0.0	0.0	2.0	0.0000000000	False
lots of clever features	0.0	0.0	0.0	2.0	0.0000000000	False
logistic regression algorithm right	0.0	0.0	0.0	2.0	0.0000000000	False
added features for spam	0.0	0.0	0.0	2.0	0.0000000000	False
features for spam correction	0.0	0.0	0.0	2.0	0.0000000000	False
email text parser features	0.0	0.0	0.0	2.0	0.0000000000	False
features for embedded images	0.0	0.0	0.0	2.0	0.0000000000	False
research paper and claim	0.0	0.0	0.0	2.0	0.0000000000	False
made the big difference	0.0	0.0	0.0	2.0	0.0000000000	False
figure out what accounts	0.0	0.0	0.0	2.0	0.0000000000	False
accounts for your improvement	0.0	0.0	0.0	2.0	0.0000000000	False
ll instead remove components	0.0	0.0	0.0	2.0	0.0000000000	False
ll remove the sender	0.0	0.0	0.0	2.0	0.0000000000	False
remove the sender host	0.0	0.0	0.0	4.0	0.0000000000	False
occurred when you remove	0.0	0.0	0.0	2.0	0.0000000000	False
remove the text parser	0.0	0.0	0.0	2.0	0.0000000000	False
make a credible case	0.0	0.0	0.0	2.0	0.0000000000	False
made the biggest difference	0.0	0.0	0.0	2.0	0.0000000000	False
features on this line	0.0	0.0	0.0	2.0	0.0000000000	False
means that in case	0.0	0.0	0.0	2.0	0.0000000000	False
rid of the sender	0.0	0.0	0.0	2.0	0.0000000000	False
host features to speed	0.0	0.0	0.0	2.0	0.0000000000	False
good candidate for elimination	0.0	0.0	0.0	2.0	0.0000000000	False
shuffle around the order	0.0	0.0	0.0	2.0	0.0000000000	False
things ? the answer	0.0	0.0	0.0	2.0	0.0000000000	False
result so in practice	0.0	0.0	0.0	2.0	0.0000000000	False
fairly natural of ordering	0.0	0.0	0.0	2.0	0.0000000000	False
ordering for both types	0.0	0.0	0.0	2.0	0.0000000000	False
add things or remove	0.0	0.0	0.0	2.0	0.0000000000	False
things or remove things	0.0	0.0	0.0	2.0	0.0000000000	False
formulas that are constants	0.0	0.0	0.0	2.0	0.0000000000	False
feel free to invent	0.0	0.0	0.0	2.0	0.0000000000	False
system and just remove	0.0	0.0	0.0	2.0	0.0000000000	False
talk about is sort	0.0	0.0	0.0	2.0	0.0000000000	False
started on a learning	0.0	0.0	0.0	2.0	0.0000000000	False
broad to get started	0.0	0.0	0.0	2.0	0.0000000000	False
started on learning problem	0.0	0.0	0.0	2.0	0.0000000000	False
carefully design your system	0.0	0.0	0.0	2.0	0.0000000000	False
designing exactly the right	0.0	0.0	0.0	2.0	0.0000000000	False
collecting the right data	0.0	0.0	0.0	2.0	0.0000000000	False
implement it and hope	0.0	0.0	0.0	2.0	0.0000000000	False
right ? the benefit	0.0	0.0	0.0	2.0	0.0000000000	False
benefit of this sort	0.0	0.0	0.0	2.0	0.0000000000	False
contribute to basic research	0.0	0.0	0.0	2.0	0.0000000000	False
basic research in machine	0.0	0.0	0.0	2.0	0.0000000000	False
research in machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
invent new machine learning	0.0	0.0	0.0	4.0	0.0000000000	False
slowing down and thinking	0.0	0.0	0.0	2.0	0.0000000000	False
deeply about the problem	0.0	0.0	0.0	4.0	0.0000000000	False
sort of the right	0.0	0.0	0.0	2.0	0.0000000000	False
deeply about a problem	0.0	0.0	0.0	2.0	0.0000000000	False
error analyses and diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
wrong and you fix	0.0	0.0	0.0	2.0	0.0000000000	False
working much more quickly	0.0	0.0	0.0	2.0	0.0000000000	False
working in a company	0.0	0.0	0.0	4.0	0.0000000000	False
first product to market	0.0	0.0	0.0	2.0	0.0000000000	False
hack and then fixing	0.0	0.0	0.0	2.0	0.0000000000	False
quickly and the reason	0.0	0.0	0.0	2.0	0.0000000000	False
parts of a system	0.0	0.0	0.0	2.0	0.0000000000	False
lot of time focusing	0.0	0.0	0.0	2.0	0.0000000000	False
right ? for identifying	0.0	0.0	0.0	2.0	0.0000000000	False
big complicated learning system	0.0	0.0	0.0	2.0	0.0000000000	False
obvious at the outset	0.0	0.0	0.0	2.0	0.0000000000	False
components you should spend	0.0	0.0	0.0	2.0	0.0000000000	False
lots of time working	0.0	0.0	0.0	2.0	0.0000000000	False
nt know that preprocessing	0.0	0.0	0.0	0.0	0.0000000000	False
nt the right component	0.0	0.0	0.0	0.0	0.0000000000	False
spent three months working	0.0	0.0	0.0	2.0	0.0000000000	False
working on better background	0.0	0.0	0.0	2.0	0.0000000000	False
out what really works	0.0	0.0	0.0	2.0	0.0000000000	False
quickly and you find	0.0	0.0	0.0	2.0	0.0000000000	False
find out what parts	0.0	0.0	0.0	4.0	0.0000000000	False
hard parts to implement	0.0	0.0	0.0	2.0	0.0000000000	False
parts are hard parts	0.0	0.0	0.0	2.0	0.0000000000	False
parts that could make	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to build	0.0	0.0	0.0	4.0	0.0000000000	False
build a people recognition	0.0	0.0	0.0	2.0	0.0000000000	False
prototyped a few systems	0.0	0.0	0.0	2.0	0.0000000000	False
first system you re	0.0	0.0	0.0	0.0	0.0000000000	False
system you re designing	0.0	0.0	0.0	0.0	0.0000000000	False
concrete piece of advice	0.0	0.0	0.0	2.0	0.0000000000	False
applies to your projects	0.0	0.0	0.0	2.0	0.0000000000	False
build a working application	0.0	0.0	0.0	2.0	0.0000000000	False
system like this step	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to predict	0.0	0.0	0.0	2.0	0.0000000000	False
predict and just plot	0.0	0.0	0.0	2.0	0.0000000000	False
negative ? i thought	0.0	0.0	0.0	2.0	0.0000000000	False
wrong with this dataset	0.0	0.0	0.0	2.0	0.0000000000	False
wrong with your data	0.0	0.0	0.0	2.0	0.0000000000	False
out just by plotting	0.0	0.0	0.0	2.0	0.0000000000	False
find out be implementing	0.0	0.0	0.0	2.0	0.0000000000	False
big complicated learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms on it plotting	0.0	0.0	0.0	2.0	0.0000000000	False
plotting the data sounds	0.0	0.0	0.0	2.0	0.0000000000	False
lots of us give	0.0	0.0	0.0	2.0	0.0000000000	False
advice if your goal	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms all right	0.0	0.0	0.0	2.0	0.0000000000	False
lying around so give	0.0	0.0	0.0	2.0	0.0000000000	False
give me a learning	0.0	0.0	0.0	2.0	0.0000000000	False
complicated than logistic regression	0.0	0.0	0.0	2.0	0.0000000000	False
regression on it first	0.0	0.0	0.0	2.0	0.0000000000	False
nt want to hack	0.0	0.0	0.0	0.0	0.0000000000	False
follow this specifically shoot	0.0	0.0	0.0	2.0	0.0000000000	False
premature optimization of code	0.0	0.0	0.0	2.0	0.0000000000	False
people will prematurely optimize	0.0	0.0	0.0	2.0	0.0000000000	False
prematurely optimize one component	0.0	0.0	0.0	2.0	0.0000000000	False
big complicated machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
complicated machine learning system	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon that highly influenced	0.0	0.0	0.0	2.0	0.0000000000	False
influenced my own thinking	0.0	0.0	0.0	2.0	0.0000000000	False
thinking it was based	0.0	0.0	0.0	2.0	0.0000000000	False
based on a paper	0.0	0.0	0.0	2.0	0.0000000000	False
paper written by christos	0.0	0.0	0.0	2.0	0.0000000000	False
written by christos papadimitriou	0.0	0.0	0.0	2.0	0.0000000000	False
developmental progress of research	0.0	0.0	0.0	2.0	0.0000000000	False
build a mail delivery	0.0	0.0	0.0	2.0	0.0000000000	False
ve drawn a circle	0.0	0.0	0.0	2.0	0.0000000000	False
nt have to deliver	0.0	0.0	0.0	0.0	0.0000000000	False
wander around indoor environments	0.0	0.0	0.0	2.0	0.0000000000	False
robot to manipulate objects	0.0	0.0	0.0	2.0	0.0000000000	False
manipulate objects and pickup	0.0	0.0	0.0	2.0	0.0000000000	False
objects and pickup envelopes	0.0	0.0	0.0	2.0	0.0000000000	False
build those two components	0.0	0.0	0.0	2.0	0.0000000000	False
two components in order	0.0	0.0	0.0	2.0	0.0000000000	False
drawing those two components	0.0	0.0	0.0	2.0	0.0000000000	False
components and little arrows	0.0	0.0	0.0	2.0	0.0000000000	False
obstacle avoidance is needed	0.0	0.0	0.0	2.0	0.0000000000	False
build your mail delivery	0.0	0.0	0.0	2.0	0.0000000000	False
robot well for obstacle	0.0	0.0	0.0	2.0	0.0000000000	False
robot that can navigate	0.0	0.0	0.0	2.0	0.0000000000	False
obstacles now we re	0.0	0.0	0.0	0.0	0.0000000000	False
computer vision to detect	0.0	0.0	0.0	2.0	0.0000000000	False
evening this is lighting	0.0	0.0	0.0	2.0	0.0000000000	False
lighting causes the color	0.0	0.0	0.0	2.0	0.0000000000	False
colors of an object	0.0	0.0	0.0	2.0	0.0000000000	False
right ? because lighting	0.0	0.0	0.0	2.0	0.0000000000	False
represented by three-dimensional vectors	0.0	0.0	0.0	2.0	0.0000000000	False
learn when two colors	0.0	0.0	0.0	2.0	0.0000000000	False
appearance of two colors	0.0	0.0	0.0	2.0	0.0000000000	False
geometry of 3d manifolds	0.0	0.0	0.0	4.0	0.0000000000	False
manifolds because that helps	0.0	0.0	0.0	2.0	0.0000000000	False
build a sound theory	0.0	0.0	0.0	2.0	0.0000000000	False
develop our 3d similarity	0.0	0.0	0.0	2.0	0.0000000000	False
understand the fundamental aspects	0.0	0.0	0.0	2.0	0.0000000000	False
aspects of this problem	0.0	0.0	0.0	2.0	0.0000000000	False
complexity of non-riemannian geometries	0.0	0.0	0.0	2.0	0.0000000000	False
eventually you re proving	0.0	0.0	0.0	0.0	0.0000000000	False
re proving convergence bounds	0.0	0.0	0.0	2.0	0.0000000000	False
convergence bounds for sampled	0.0	0.0	5.99890789953	6.0	0.0000000000	False
sampled of non-monotonic logic	0.0	0.0	0.0	2.0	0.0000000000	False
chances are that link	0.0	0.0	0.0	2.0	0.0000000000	False
link is nt real	0.0	0.0	0.0	0.0	0.0000000000	False
nt real color variance	0.0	0.0	0.0	0.0	0.0000000000	False
variance just barely helped	0.0	0.0	0.0	2.0	0.0000000000	False
barely helped object recognition	0.0	0.0	0.0	2.0	0.0000000000	False
learning and that link	0.0	0.0	0.0	2.0	0.0000000000	False
thought in your head	0.0	0.0	0.0	2.0	0.0000000000	False
written on differential geometry	0.0	0.0	0.0	2.0	0.0000000000	False
written because some guy	0.0	0.0	0.0	2.0	0.0000000000	False
ll help 3d similarity	0.0	0.0	0.0	2.0	0.0000000000	False
friend of mine told	0.0	0.0	0.0	2.0	0.0000000000	False
told me that color	0.0	0.0	0.0	2.0	0.0000000000	False
working on color invariance	0.0	0.0	0.0	2.0	0.0000000000	False
mine that his thing	0.0	0.0	0.0	2.0	0.0000000000	False
ll tell a friend	0.0	0.0	0.0	2.0	0.0000000000	False
re working on convergence	0.0	0.0	0.0	2.0	0.0000000000	False
working on convergence bound	0.0	0.0	0.0	2.0	0.0000000000	False
day of your mail	0.0	0.0	0.0	2.0	0.0000000000	False
theory of vc dimension	0.0	0.0	0.0	2.0	0.0000000000	False
impact on many applications	0.0	0.0	0.0	2.0	0.0000000000	False
dramatically advanced data machine	0.0	0.0	0.0	2.0	0.0000000000	False
advanced data machine learning	0.0	0.0	0.0	2.0	0.0000000000	False
mind are you working	0.0	0.0	0.0	2.0	0.0000000000	False
relevance to some application	0.0	0.0	0.0	2.0	0.0000000000	False
work on an application	0.0	0.0	0.0	2.0	0.0000000000	False
link from the theory	0.0	0.0	0.0	2.0	0.0000000000	False
theory i m working	0.0	0.0	0.0	0.0	0.0000000000	False
back to an application	0.0	0.0	0.0	2.0	0.0000000000	False
working on will relate	0.0	0.0	0.0	2.0	0.0000000000	False
relate to an application	0.0	0.0	0.0	2.0	0.0000000000	False
personally see a link	0.0	0.0	0.0	2.0	0.0000000000	False
back just to summarize	0.0	0.0	0.0	2.0	0.0000000000	False
coming up with diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostics for learning algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
learning algorithms and making	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms and making progress	0.0	0.0	0.0	2.0	0.0000000000	False
tests on your learning	0.0	0.0	0.0	2.0	0.0000000000	False
spent implementing those tests	0.0	0.0	0.0	2.0	0.0000000000	False
out what to work	0.0	0.0	0.0	2.0	0.0000000000	False
talked about error analyses	0.0	0.0	0.0	2.0	0.0000000000	False
analyses and ablative analyses	0.0	0.0	2.99890789953	6.0	0.0000000000	False
approaches and the risks	0.0	0.0	0.0	2.0	0.0000000000	False
minutes for your questions	0.0	0.0	0.0	2.0	0.0000000000	False
quick announcement of sorts	0.0	0.0	0.0	2.0	0.0000000000	False
years ago that stanford	0.0	0.0	0.0	2.0	0.0000000000	False
ago that stanford submitted	0.0	0.0	0.0	2.0	0.0000000000	False
stanford submitted an entry	0.0	0.0	0.0	2.0	0.0000000000	False
entry to the darpa	0.0	0.0	0.0	2.0	0.0000000000	False
darpa grand challenge phase	0.0	0.0	0.0	2.0	0.0000000000	False
thrun has a team	0.0	0.0	0.0	2.0	0.0000000000	False
racing another autonomous car	0.0	0.0	0.0	2.0	0.0000000000	False
tools and ai machines	0.0	0.0	0.0	2.0	0.0000000000	False
ll try to drive	0.0	0.0	0.0	2.0	0.0000000000	False
drive itself in midst	0.0	0.0	0.0	2.0	0.0000000000	False
carry out the sort	0.0	0.0	0.0	2.0	0.0000000000	False
re free this weekend	0.0	0.0	0.0	2.0	0.0000000000	False
weekend if you re	0.0	0.0	0.0	0.0	0.0000000000	False
re free on saturday	0.0	0.0	0.0	2.0	0.0000000000	False
watch tv or search	0.0	0.0	0.0	2.0	0.0000000000	False
search online for urban	0.0	0.0	0.0	2.0	0.0000000000	False
online for urban challenge	0.0	0.0	0.0	2.0	0.0000000000	False
fun thing to watch	0.0	0.0	0.0	2.0	0.0000000000	False
cool demo or instance	0.0	0.0	0.0	2.0	0.0000000000	False
died a few seconds	0.0	0.0	0.0	2.0	0.0000000000	False
seconds before class started	0.0	0.0	0.0	2.0	0.0000000000	False
show you the things	0.0	0.0	0.0	2.0	0.0000000000	False
morning and welcome back	0.0	0.0	0.0	2.0	0.0000000000	False
today is actually begin	0.0	0.0	0.0	2.0	0.0000000000	False
begin a new chapter	0.0	0.0	0.0	2.0	0.0000000000	False
briefly talk about clustering	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm with a mixture	0.0	0.0	0.0	2.0	0.0000000000	False
describe something called jensen	0.0	0.0	0.0	4.0	0.0000000000	False
derive a general form	0.0	0.0	0.0	2.0	0.0000000000	False
place and different unsupervised	0.0	0.0	0.0	2.0	0.0000000000	False
machine or any application	0.0	0.0	0.0	2.0	0.0000000000	False
application so the cartoons	0.0	0.0	0.0	2.0	0.0000000000	False
draw for supervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
positive and negative crosses	0.0	0.0	0.0	2.0	0.0000000000	False
call it the supervised	0.0	0.0	0.0	2.0	0.0000000000	False
learning because you re	0.0	0.0	0.0	0.0	0.0000000000	False
re sort of told	0.0	0.0	0.0	2.0	0.0000000000	False
told what the right	0.0	0.0	0.0	2.0	0.0000000000	False
supervision in unsupervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
study a different problem	0.0	0.0	0.0	2.0	0.0000000000	False
re given a data	0.0	0.0	0.0	2.0	0.0000000000	False
set with no labels	0.0	0.0	0.0	2.0	0.0000000000	False
labels and no indication	0.0	0.0	0.0	2.0	0.0000000000	False
job of the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to discover structure	0.0	0.0	0.0	2.0	0.0000000000	False
structure in the data	0.0	0.0	0.0	2.0	0.0000000000	False
weeks we ll talk	0.0	0.0	0.0	0.0	0.0000000000	False
talk about a variety	0.0	0.0	0.0	2.0	0.0000000000	False
variety of unsupervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
first unsupervised learning algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
ll be an algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
automatically breaks the data	0.0	0.0	0.0	2.0	0.0000000000	False
breaks the data set	0.0	0.0	0.0	2.0	0.0000000000	False
set into different smaller	0.0	0.0	0.0	2.0	0.0000000000	False
applications just to rattle	0.0	0.0	0.0	2.0	0.0000000000	False
better-known ones i guess	0.0	0.0	0.0	2.0	0.0000000000	False
guess in biology application	0.0	0.0	0.0	2.0	0.0000000000	False
application you often cross	0.0	0.0	0.0	2.0	0.0000000000	False
cross the different things	0.0	0.0	0.0	2.0	0.0000000000	False
genes and they cluster	0.0	0.0	0.0	2.0	0.0000000000	False
cluster the different genes	0.0	0.0	0.0	2.0	0.0000000000	False
genes together in order	0.0	0.0	0.0	2.0	0.0000000000	False
examine them and understand	0.0	0.0	0.0	2.0	0.0000000000	False
understand the biological function	0.0	0.0	0.0	2.0	0.0000000000	False
common application of clustering	0.0	0.0	0.0	2.0	0.0000000000	False
clustering is market research	0.0	0.0	0.0	2.0	0.0000000000	False
market research so imagine	0.0	0.0	0.0	2.0	0.0000000000	False
common practice to apply	0.0	0.0	0.0	2.0	0.0000000000	False
practice to apply clustering	0.0	0.0	0.0	2.0	0.0000000000	False
clustering algorithms to break	0.0	0.0	0.0	2.0	0.0000000000	False
customers into different market	0.0	0.0	0.0	2.0	0.0000000000	False
products towards different market	0.0	0.0	0.0	2.0	0.0000000000	False
market segments and target	0.0	0.0	0.0	2.0	0.0000000000	False
target your sales pitches	0.0	0.0	0.0	2.0	0.0000000000	False
specifically to different market	0.0	0.0	0.0	2.0	0.0000000000	False
ll do later today	0.0	0.0	0.0	2.0	0.0000000000	False
clustering algorithm to everyday	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm to everyday group	0.0	0.0	0.0	2.0	0.0000000000	False
everyday group related news	0.0	0.0	0.0	2.0	0.0000000000	False
group related news articles	0.0	0.0	0.0	2.0	0.0000000000	False
articles together to display	0.0	0.0	0.0	2.0	0.0000000000	False
thousand news articles today	0.0	0.0	0.0	2.0	0.0000000000	False
top story of today	0.0	0.0	0.0	2.0	0.0000000000	False
websites on different story	0.0	0.0	0.0	2.0	0.0000000000	False
story of the day	0.0	0.0	0.0	2.0	0.0000000000	False
talks about image segmentation	0.0	0.0	0.0	2.0	0.0000000000	False
group together different subsets	0.0	0.0	0.0	2.0	0.0000000000	False
subsets of the picture	0.0	0.0	0.0	2.0	0.0000000000	False
picture into coherent pieces	0.0	0.0	0.0	2.0	0.0000000000	False
coherent pieces of pixels	0.0	0.0	0.0	2.0	0.0000000000	False
understand what s contained	0.0	0.0	0.0	0.0	0.0000000000	False
contained in the picture	0.0	0.0	0.0	2.0	0.0000000000	False
clustering the next idea	0.0	0.0	0.0	2.0	0.0000000000	False
automatically group the data	0.0	0.0	0.0	2.0	0.0000000000	False
group the data sets	0.0	0.0	0.0	2.0	0.0000000000	False
data sets into coherent	0.0	0.0	0.0	2.0	0.0000000000	False
sets into coherent clusters	0.0	0.0	0.0	2.0	0.0000000000	False
waiting for the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
nt i just start	0.0	0.0	0.0	0.0	0.0000000000	False
out the specific clustering	0.0	0.0	0.0	2.0	0.0000000000	False
show you the animation	0.0	0.0	0.0	4.0	0.0000000000	False
clustering algorithm for finding	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for finding clustering	0.0	0.0	0.0	2.0	0.0000000000	False
input to the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
set which i write	0.0	0.0	0.0	2.0	0.0000000000	False
talking about unsupervised learning	0.0	0.0	0.0	2.0	0.0000000000	False
sense when i show	0.0	0.0	0.0	2.0	0.0000000000	False
animation on my laptop	0.0	0.0	0.0	2.0	0.0000000000	False
re of training data	0.0	0.0	0.0	2.0	0.0000000000	False
steps so the cluster	0.0	0.0	0.0	2.0	0.0000000000	False
centroid j is closest	0.0	0.0	0.0	2.0	0.0000000000	False
step is called assigning	0.0	0.0	0.0	2.0	0.0000000000	False
point xi to cluster	0.0	0.0	0.0	2.0	0.0000000000	False
picking the cluster centroid	0.0	0.0	0.0	2.0	0.0000000000	False
step is you update	0.0	0.0	0.0	2.0	0.0000000000	False
update the cluster centroids	0.0	0.0	3.99789621318	6.0	0.0000000000	False
bring down the display	0.0	0.0	0.0	2.0	0.0000000000	False
display for the laptop	0.0	0.0	0.0	2.0	0.0000000000	False
k-means algorithm and hope	0.0	0.0	0.0	2.0	0.0000000000	False
michael jordan in berkley	0.0	0.0	0.0	2.0	0.0000000000	False
berkley so these points	0.0	0.0	0.0	2.0	0.0000000000	False
green are my data	0.0	0.0	0.0	2.0	0.0000000000	False
randomly initialize a pair	0.0	0.0	0.0	2.0	0.0000000000	False
pair of cluster centroids	0.0	0.0	0.0	2.0	0.0000000000	False
blue crosses to note	0.0	0.0	0.0	2.0	0.0000000000	False
clusters in this data	0.0	0.0	0.0	2.0	0.0000000000	False
data sets of k-means	0.0	0.0	0.0	2.0	0.0000000000	False
sets of k-means algorithms	0.0	0.0	0.0	2.0	0.0000000000	False
k-means algorithms as follow	0.0	0.0	0.0	2.0	0.0000000000	False
points in my data	0.0	0.0	0.0	2.0	0.0000000000	False
denote that by painting	0.0	0.0	0.0	2.0	0.0000000000	False
blue or red depending	0.0	0.0	0.0	2.0	0.0000000000	False
cross points are painted	0.0	0.0	0.0	2.0	0.0000000000	False
points that i ve	0.0	0.0	0.0	0.0	0.0000000000	False
red dots and compute	0.0	0.0	0.0	2.0	0.0000000000	False
move the cluster centroids	0.0	0.0	0.0	2.0	0.0000000000	False
repeat the same process	0.0	0.0	0.0	2.0	0.0000000000	False
assign all the points	0.0	0.0	0.0	2.0	0.0000000000	False
cross to the color	0.0	0.0	0.0	2.0	0.0000000000	False
blue and similarly red	0.0	0.0	0.0	2.0	0.0000000000	False
points to the cluster	0.0	0.0	0.0	2.0	0.0000000000	False
blue points and compute	0.0	0.0	0.0	2.0	0.0000000000	False
red points and update	0.0	0.0	0.0	2.0	0.0000000000	False
running these two sets	0.0	0.0	0.0	2.0	0.0000000000	False
two sets of k-means	0.0	0.0	0.0	2.0	0.0000000000	False
centroids and the assignment	0.0	0.0	0.0	2.0	0.0000000000	False
assignment of the points	0.0	0.0	0.0	2.0	0.0000000000	False
closest to the cluster	0.0	0.0	0.0	2.0	0.0000000000	False
centroids will actually remain	0.0	0.0	0.0	2.0	0.0000000000	False
make sure you understand	0.0	0.0	0.0	2.0	0.0000000000	False
understand how the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
maps onto the animation	0.0	0.0	0.0	2.0	0.0000000000	False
two steps this step	0.0	0.0	0.0	2.0	0.0000000000	False
shifting the cluster centroid	0.0	0.0	0.0	2.0	0.0000000000	False
assigned to that cluster	0.0	0.0	0.0	2.0	0.0000000000	False
centroid okay okay questions	0.0	0.0	0.0	2.0	0.0000000000	False
converge ? the answer	0.0	0.0	0.0	2.0	0.0000000000	False
define the distortion function	0.0	0.0	0.0	4.0	0.0000000000	False
squared you can define	0.0	0.0	0.0	2.0	0.0000000000	False
function of the cluster	0.0	0.0	0.0	2.0	0.0000000000	False
centroids and square distances	0.0	0.0	0.0	2.0	0.0000000000	False
points and the cluster	0.0	0.0	0.0	2.0	0.0000000000	False
centroids that they re	0.0	0.0	0.0	0.0	0.0000000000	False
sense as an authorization	0.0	0.0	0.0	2.0	0.0000000000	False
sense is the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
ll repeatedly with respect	0.0	0.0	0.0	2.0	0.0000000000	False
prove is that k-means	0.0	0.0	0.0	2.0	0.0000000000	False
k-means the two steps	0.0	0.0	0.0	2.0	0.0000000000	False
two steps of k-means	0.0	0.0	0.0	2.0	0.0000000000	False
respect a new alternately	0.0	0.0	0.0	2.0	0.0000000000	False
converge in the sense	0.0	0.0	0.0	2.0	0.0000000000	False
clustering s they give	0.0	0.0	0.0	0.0	0.0000000000	False
give the same value	0.0	0.0	0.0	2.0	0.0000000000	False
k-means may actually switch	0.0	0.0	0.0	2.0	0.0000000000	False
value for this objective	0.0	0.0	0.0	2.0	0.0000000000	False
ll just never happen	0.0	0.0	0.0	2.0	0.0000000000	False
randomly pick a number	0.0	0.0	0.0	2.0	0.0000000000	False
work best the number	0.0	0.0	0.0	2.0	0.0000000000	False
clusters in this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
hard to choose automatically	0.0	0.0	0.0	2.0	0.0000000000	False
automatic ways of choosing	0.0	0.0	0.0	2.0	0.0000000000	False
pick of the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of clusters randomly	0.0	0.0	0.0	2.0	0.0000000000	False
randomly and the reason	0.0	0.0	0.0	2.0	0.0000000000	False
problems the true number	0.0	0.0	0.0	2.0	0.0000000000	False
true number of clusters	0.0	0.0	0.0	2.0	0.0000000000	False
actual number of clusters	0.0	0.0	0.0	2.0	0.0000000000	False
right so yes k-means	0.0	0.0	0.0	2.0	0.0000000000	False
function and so k-means	0.0	0.0	0.0	2.0	0.0000000000	False
function is not guaranteed	0.0	0.0	0.0	2.0	0.0000000000	False
initializations and then run	0.0	0.0	0.0	2.0	0.0000000000	False
run clustering a bunch	0.0	0.0	0.0	2.0	0.0000000000	False
value for the distortion	0.0	0.0	0.0	2.0	0.0000000000	False
centroid has no points	0.0	0.0	0.0	2.0	0.0000000000	False
vast majority of applications	0.0	0.0	0.0	2.0	0.0000000000	False
ve seen for k-means	0.0	0.0	0.0	2.0	0.0000000000	False
norm and one norm	0.0	0.0	0.0	2.0	0.0000000000	False
variations on this algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
describe is actually talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about density estimation	0.0	0.0	0.0	2.0	0.0000000000	False
estimation as another k-means	0.0	0.0	0.0	2.0	0.0000000000	False
building off an assembly	0.0	0.0	0.0	2.0	0.0000000000	False
work for an aircraft	0.0	0.0	0.0	2.0	0.0000000000	False
re building aircraft engines	0.0	0.0	0.0	2.0	0.0000000000	False
engines off the assembly	0.0	0.0	0.0	2.0	0.0000000000	False
test these aircraft engines	0.0	0.0	0.0	2.0	0.0000000000	False
aircraft engines and measure	0.0	0.0	0.0	2.0	0.0000000000	False
measure various different properties	0.0	0.0	0.0	2.0	0.0000000000	False
heat and vibrations right	0.0	0.0	0.0	2.0	0.0000000000	False
vibrations right in reality	0.0	0.0	0.0	2.0	0.0000000000	False
amount of heat produced	0.0	0.0	0.0	4.0	0.0000000000	False
heat produced and vibrations	0.0	0.0	0.0	2.0	0.0000000000	False
produced and vibrations produced	0.0	0.0	0.0	2.0	0.0000000000	False
produced and the amount	0.0	0.0	0.0	2.0	0.0000000000	False
rolls off the assembly	0.0	0.0	0.0	4.0	0.0000000000	False
measure the same heat	0.0	0.0	0.0	2.0	0.0000000000	False
heat and vibration properties	0.0	0.0	0.0	2.0	0.0000000000	False
flaw in this aircraft	0.0	0.0	0.0	2.0	0.0000000000	False
typical distribution of features	0.0	0.0	0.0	2.0	0.0000000000	False
raise a red flag	0.0	0.0	0.0	2.0	0.0000000000	False
fly with the engine	0.0	0.0	0.0	2.0	0.0000000000	False
engine so this problem	0.0	0.0	0.0	2.0	0.0000000000	False
problem i just described	0.0	0.0	0.0	2.0	0.0000000000	False
described is an instance	0.0	0.0	0.0	2.0	0.0000000000	False
typical data you re	0.0	0.0	0.0	0.0	0.0000000000	False
unusual transactions to start	0.0	0.0	0.0	2.0	0.0000000000	False
stolen my credit card	0.0	0.0	0.0	2.0	0.0000000000	False
talk about specific algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
specific algorithm for density	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm for density estimation	0.0	0.0	0.0	2.0	0.0000000000	False
works with data sets	0.0	0.0	0.0	2.0	0.0000000000	False
standard text book distributions	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian or a explanation	0.0	0.0	0.0	2.0	0.0000000000	False
model to estimate densities	0.0	0.0	0.0	2.0	0.0000000000	False
imagine maybe a data	0.0	0.0	0.0	2.0	0.0000000000	False
axis and these dots	0.0	0.0	0.0	2.0	0.0000000000	False
dots represent the positions	0.0	0.0	0.0	2.0	0.0000000000	False
positions of the data	0.0	0.0	0.0	2.0	0.0000000000	False
coming from a density	0.0	0.0	0.0	2.0	0.0000000000	False
clear that the picture	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian s that generated	0.0	0.0	0.0	0.0	0.0000000000	False
generated this data set	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian to my crosses	0.0	0.0	0.0	2.0	0.0000000000	False
nt actually have access	0.0	0.0	0.0	0.0	0.0000000000	False
access to these labels	0.0	0.0	0.0	2.0	0.0000000000	False
idea in this model	0.0	0.0	0.0	2.0	0.0000000000	False
re gon na imagine	0.0	0.0	0.0	2.0	0.0000000000	False
distributed multinomial with parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameter are the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
distribution and the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution of xi conditioned	0.0	0.0	0.0	2.0	0.0000000000	False
equations that i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian discriminant analysis algorithm	0.0	0.0	0.0	2.0	0.0000000000	True
analysis with these latent	0.0	0.0	0.0	2.0	0.0000000000	False
explicit if we knew	0.0	0.0	0.0	2.0	0.0000000000	False
suppose for the sake	0.0	0.0	0.0	2.0	0.0000000000	False
estimation you can write	0.0	0.0	0.0	2.0	0.0000000000	False
write down the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
write down the law	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood and do maximum	0.0	0.0	0.0	2.0	0.0000000000	False
estimate all the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of your model	0.0	0.0	0.0	4.0	0.0000000000	False
model does this make	0.0	0.0	0.0	2.0	0.0000000000	False
make sense ? raise	0.0	0.0	0.0	2.0	0.0000000000	False
hand if this makes	0.0	0.0	2.99789621318	6.0	0.0000000000	False
nt raise your hands	0.0	0.0	0.0	0.0	0.0000000000	False
basically any other questions	0.0	0.0	0.0	2.0	0.0000000000	False
playing a similar role	0.0	0.0	0.0	2.0	0.0000000000	False
role to the cross	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian s discriminant analysis	0.0	0.0	0.0	0.0	0.0000000000	False
maximum likeliness estimation parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters but in reality	0.0	0.0	0.0	2.0	0.0000000000	False
guess what the values	0.0	0.0	0.0	2.0	0.0000000000	False
guess at the values	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the rest	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the model	0.0	0.0	0.0	4.0	0.0000000000	False
estimate for the parameters	0.0	0.0	5.99719495091	8.0	0.4233576642	False
parameters for the rest	0.0	0.0	0.0	2.0	0.0000000000	False
likeliness estimation to set	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of the model	0.0	0.0	0.0	2.0	0.0000000000	False
model so the algorithm	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm and it proceeds	0.0	0.0	0.0	2.0	0.0000000000	False
proceeds as follows repeat	0.0	0.0	0.0	2.0	0.0000000000	False
re going to guess	0.0	0.0	0.0	2.0	0.0000000000	False
values of the unknown	0.0	0.0	0.0	2.0	0.0000000000	False
rest of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters in my model	0.0	0.0	0.0	2.0	0.0000000000	False
sum from o equals	0.0	0.0	0.0	4.0	0.0000000000	False
essentially the same thing	0.0	0.0	0.0	2.0	0.0000000000	False
gaussian and the numerator	0.0	0.0	0.0	2.0	0.0000000000	False
numerator and the sum	0.0	0.0	0.0	2.0	0.0000000000	False
terms of the denominator	0.0	0.0	0.0	2.0	0.0000000000	False
estimates of the parameters	0.0	0.0	0.0	4.0	0.0000000000	False
lay down the formulas	0.0	0.0	0.0	2.0	0.0000000000	False
remember was the probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability that we computed	0.0	0.0	0.0	2.0	0.0000000000	False
nt want to call	0.0	0.0	0.0	0.0	0.0000000000	False
commonly use different covariant	0.0	0.0	0.0	2.0	0.0000000000	False
sort of by convention	0.0	0.0	0.0	2.0	0.0000000000	False
sigma i just wrote	0.0	0.0	0.0	2.0	0.0000000000	False
wrote down a lot	0.0	0.0	0.0	2.0	0.0000000000	False
values for the zis	0.0	0.0	0.0	2.0	0.0000000000	False
give you labeled data	0.0	0.0	0.0	2.0	0.0000000000	False
values of the zis	0.0	0.0	0.0	4.0	0.0000000000	False
giving you a data	0.0	0.0	0.0	2.0	0.0000000000	False
set that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
discriminant analysis we figured	0.0	0.0	0.0	2.0	0.0000000000	False
figured out the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
out the maximum likeliness	0.0	0.0	0.0	2.0	0.0000000000	False
estimation and the maximum	0.0	0.0	0.0	2.0	0.0000000000	False
probability that zi equals	0.0	0.0	0.0	4.0	0.0000000000	False
estimate that as sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum of i equals	0.0	0.0	0.0	2.0	0.0000000000	False
equals j and divide	0.0	0.0	0.0	2.0	0.0000000000	False
knew the cross labels	0.0	0.0	0.0	2.0	0.0000000000	False
estimate for the chance	0.0	0.0	0.0	2.0	0.0000000000	False
chance that the labels	0.0	0.0	0.0	2.0	0.0000000000	False
examples your maximum likeliness	0.0	0.0	0.0	2.0	0.0000000000	False
likeliness estimate for probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability of getting examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples in your training	0.0	0.0	0.0	2.0	0.0000000000	False
draw the same data	0.0	0.0	0.0	2.0	0.0000000000	False
cross label is unknown	0.0	0.0	0.0	2.0	0.0000000000	False
guess for the values	0.0	0.0	0.0	2.0	0.0000000000	False
step we computed wij	0.0	0.0	0.0	2.0	0.0000000000	False
guess for the probability	0.0	0.0	0.0	4.0	0.0000000000	False
probability that the point	0.0	0.0	0.0	2.0	0.0000000000	False
probability that this point	0.0	0.0	0.0	4.0	0.0000000000	False
point was a cross	0.0	0.0	0.0	2.0	0.0000000000	False
sum from i equals	0.0	0.0	0.0	2.0	0.0000000000	False
formula for the estimate	0.0	0.0	0.0	2.0	0.0000000000	False
back to the formula	0.0	0.0	0.0	2.0	0.0000000000	False
convey an intuitive sense	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm s make sense	0.0	0.0	0.0	0.0	0.0000000000	False
sense can you raise	0.0	0.0	0.0	2.0	0.0000000000	False
sense now ? cool	0.0	0.0	0.0	2.0	0.0000000000	False
present a broader view	0.0	0.0	0.0	2.0	0.0000000000	False
hour i have today	0.0	0.0	0.0	2.0	0.0000000000	False
describe a general description	0.0	0.0	0.0	2.0	0.0000000000	False
pre-cursor to actually deriving	0.0	0.0	0.0	2.0	0.0000000000	False
jensen s and equality	0.0	0.0	0.0	0.0	0.0000000000	False
function so a function	0.0	0.0	0.0	2.0	0.0000000000	False
function is a convex	0.0	0.0	0.0	2.0	0.0000000000	False
ve written f prime	0.0	0.0	0.0	2.0	0.0000000000	False
written f prime prime	0.0	0.0	0.0	2.0	0.0000000000	False
differentiatable to be convex	0.0	0.0	0.0	2.0	0.0000000000	False
prime should be creating	0.0	0.0	0.0	2.0	0.0000000000	False
applied to the expectation	0.0	0.0	0.0	2.0	0.0000000000	False
equal of 2d expectation	0.0	0.0	0.0	2.0	0.0000000000	False
remember i often drop	0.0	0.0	0.0	2.0	0.0000000000	False
drop the square back	0.0	0.0	0.0	2.0	0.0000000000	False
drop the square brackets	0.0	0.0	0.0	2.0	0.0000000000	False
picture that would explain	0.0	0.0	0.0	2.0	0.0000000000	False
equality is by drawing	0.0	0.0	0.0	2.0	0.0000000000	False
drawing the following picture	0.0	0.0	0.0	2.0	0.0000000000	False
ll illustrate this inequality	0.0	0.0	0.0	2.0	0.0000000000	False
vertical axis we re	0.0	0.0	0.0	0.0	0.0000000000	False
value in the middle	0.0	0.0	0.0	2.0	0.0000000000	False
prime of x makes	0.0	0.0	0.0	2.0	0.0000000000	False
makes than z row	0.0	0.0	0.0	2.0	0.0000000000	False
convex then the inequality	0.0	0.0	0.0	2.0	0.0000000000	False
inequality holds an equality	0.0	0.0	0.0	2.0	0.0000000000	False
variable x always takes	0.0	0.0	0.0	2.0	0.0000000000	False
value okay any questions	0.0	0.0	0.0	2.0	0.0000000000	False
definition for strictly convex	0.0	0.0	0.0	2.0	0.0000000000	False
part of this function	0.0	0.0	0.0	2.0	0.0000000000	False
strictly convexed just means	0.0	0.0	0.0	2.0	0.0000000000	False
nt have a convex	0.0	0.0	0.0	0.0	0.0000000000	False
nt any straight line	0.0	0.0	0.0	0.0	0.0000000000	False
goal is to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters of model right	0.0	0.0	0.0	2.0	0.0000000000	False
goal is to find	0.0	0.0	0.0	2.0	0.0000000000	False
find the maximum likeliness	0.0	0.0	0.0	2.0	0.0000000000	False
data where the likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
defined as something equals	0.0	0.0	0.0	2.0	0.0000000000	False
values of zi parameterized	0.0	0.0	0.0	2.0	0.0000000000	False
performing this maximum likeliness	0.0	0.0	0.0	2.0	0.0000000000	False
maximum likeliness estimation problem	0.0	0.0	0.0	2.0	0.0000000000	False
complicated by the fact	0.0	0.0	0.0	2.0	0.0000000000	False
zis in our model	0.0	0.0	0.0	2.0	0.0000000000	False
model that are unobserved	0.0	0.0	0.0	2.0	0.0000000000	False
axis in this cartoon	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon is the axis	0.0	0.0	0.0	2.0	0.0000000000	False
re trying to maximize	0.0	0.0	0.0	2.0	0.0000000000	False
construct a lower bound	0.0	0.0	5.99719495091	8.0	0.2974358974	False
bound for this law	0.0	0.0	0.0	2.0	0.0000000000	False
law of likelihood function	0.0	0.0	0.0	4.0	0.0000000000	False
bound will be tight	0.0	0.0	0.0	2.0	0.0000000000	False
equality after current guessing	0.0	0.0	0.0	2.0	0.0000000000	False
current guessing the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
parameters and they maximize	0.0	0.0	0.0	2.0	0.0000000000	False
maximize this lower boundary	0.0	0.0	0.0	2.0	0.0000000000	False
lower boundary with respect	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm look at theta	0.0	0.0	0.0	2.0	0.0000000000	False
construct a new lower	0.0	0.0	0.0	2.0	0.0000000000	False
lower bound of theta	0.0	0.0	0.0	2.0	0.0000000000	False
converge to local optimum	0.0	0.0	0.0	2.0	0.0000000000	False
local optimum on theta	0.0	0.0	0.0	2.0	0.0000000000	False
optimum on theta function	0.0	0.0	0.0	2.0	0.0000000000	False
respect to theta sum	0.0	0.0	0.0	2.0	0.0000000000	False
sum over all values	0.0	0.0	0.0	2.0	0.0000000000	False
construct the probability distribution	0.0	0.0	0.0	2.0	0.0000000000	False
ll later go describe	0.0	0.0	0.0	2.0	0.0000000000	False
describe the specific choice	0.0	0.0	0.0	2.0	0.0000000000	False
choice of this distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution over the random	0.0	0.0	0.0	2.0	0.0000000000	False
function so that tells	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the expected	0.0	0.0	0.0	4.0	0.0000000000	False
expected value of log	0.0	0.0	0.0	2.0	0.0000000000	False
function form of jensen	0.0	0.0	0.0	2.0	0.0000000000	False
equality and so continuing	0.0	0.0	0.0	2.0	0.0000000000	False
summary of a log	0.0	0.0	0.0	2.0	0.0000000000	False
log and an expectation	0.0	0.0	0.0	2.0	0.0000000000	False
value of the log	0.0	0.0	0.0	2.0	0.0000000000	False
lastly just to expand	0.0	0.0	0.0	2.0	0.0000000000	False
expand out this formula	0.0	0.0	0.0	2.0	0.0000000000	False
distribution let s denote	0.0	0.0	0.0	0.0	0.0000000000	False
probability of that value	0.0	0.0	0.0	2.0	0.0000000000	False
value of z times	0.0	0.0	0.0	2.0	0.0000000000	False
right that s sort	0.0	0.0	0.0	0.0	0.0000000000	False
sort of the definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition of a random	0.0	0.0	0.0	2.0	0.0000000000	False
step to this step	0.0	0.0	0.0	2.0	0.0000000000	False
ve been using distribution	0.0	0.0	0.0	2.0	0.0000000000	False
distribution qi to denote	0.0	0.0	0.0	2.0	0.0000000000	False
expected value with respect	0.0	0.0	0.0	2.0	0.0000000000	False
respect to a random	0.0	0.0	0.0	2.0	0.0000000000	False
random variable z joined	0.0	0.0	0.0	2.0	0.0000000000	False
joined from the distribution	0.0	0.0	0.0	2.0	0.0000000000	False
general when you re	0.0	0.0	0.0	0.0	0.0000000000	False
re doing maximum likelihood	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of the data	0.0	0.0	0.0	2.0	0.0000000000	False
previously we said probability	0.0	0.0	0.0	2.0	0.0000000000	False
probability of the data	0.0	0.0	0.0	4.0	0.0000000000	False
bound on the law	0.0	0.0	0.0	4.0	0.0000000000	False
theta are the parameters	0.0	0.0	0.0	2.0	0.0000000000	False
function of your parameters	0.0	0.0	0.0	2.0	0.0000000000	False
likelihood of your parameters	0.0	0.0	0.0	2.0	0.0000000000	False
theta is lower bounded	0.0	0.0	0.0	2.0	0.0000000000	False
bounded by this thing	0.0	0.0	0.0	2.0	0.0000000000	False
cartoon of repeatedly constructing	0.0	0.0	0.0	2.0	0.0000000000	False
lower bound and optimizing	0.0	0.0	0.0	2.0	0.0000000000	False
optimizing the lower bound	0.0	0.0	0.0	4.0	0.0000000000	False
bound for the law	0.0	0.0	0.0	2.0	0.0000000000	False
current value for theta	0.0	0.0	0.0	4.0	0.0000000000	False
theta so just refrain	0.0	0.0	0.0	2.0	0.0000000000	False
construct some lower bound	0.0	0.0	0.0	2.0	0.0000000000	False
bound to be tight	0.0	0.0	0.0	2.0	0.0000000000	False
equal to the law	0.0	0.0	0.0	2.0	0.0000000000	False
optimize my lower bound	0.0	0.0	0.0	2.0	0.0000000000	False
nt think i ve	0.0	0.0	0.0	0.0	0.0000000000	False
bound is a concave	0.0	0.0	0.0	2.0	0.0000000000	False
concave function of theta	0.0	0.0	0.0	4.0	0.0000000000	False
equality if the random	0.0	0.0	0.0	2.0	0.0000000000	False
inside is a constant	0.0	0.0	0.0	2.0	0.0000000000	False
right if you re	0.0	0.0	0.0	0.0	0.0000000000	False
re taking an expectation	0.0	0.0	0.0	2.0	0.0000000000	False
respect to constant valued	0.0	0.0	0.0	2.0	0.0000000000	False
theta and just normalize	0.0	0.0	0.0	2.0	0.0000000000	False
skipping here to show	0.0	0.0	0.0	2.0	0.0000000000	False
ll just be convinced	0.0	0.0	0.0	2.0	0.0000000000	False
convinced it s true	0.0	0.0	0.0	0.0	0.0000000000	False
steps that i skipped	0.0	0.0	0.0	2.0	0.0000000000	False
out in the lecture	0.0	0.0	0.0	2.0	0.0000000000	False
definition of conditional probability	0.0	0.0	0.0	2.0	0.0000000000	False
algorithm has two steps	0.0	0.0	0.0	2.0	0.0000000000	False
formula we just worked	0.0	0.0	0.0	2.0	0.0000000000	False
created a lower bound	0.0	0.0	0.0	2.0	0.0000000000	False
current value of theta	0.0	0.0	0.0	4.0	0.0000000000	False
optimize that lower bound	0.0	0.0	0.0	2.0	0.0000000000	False
lower bound with respect	0.0	0.0	0.0	4.0	0.0000000000	False
respect to our parameters	0.0	0.0	0.0	2.0	0.0000000000	False
step that i wrote	0.0	0.0	0.0	2.0	0.0000000000	False
constructs this lower bound	0.0	0.0	0.0	2.0	0.0000000000	False
lower bound and makes	0.0	0.0	0.0	2.0	0.0000000000	False
data okay so lots	0.0	0.0	0.0	2.0	0.0000000000	False
lecture let s check	0.0	0.0	0.0	0.0	0.0000000000	False
questions before we close	0.0	0.0	0.0	2.0	0.0000000000	False
close no okay cool	0.0	0.0	0.0	2.0	0.0000000000	False
wrap up for today	0.0	0.0	0.0	2.0	0.0000000000	False
today we shall discuss	0.0	0.0	0.0	2.0	0.0000000000	False
discuss what embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
covering in forty lectures	0.0	0.0	0.0	2.0	0.0000000000	False
lectures we shall cover	0.0	0.0	0.0	2.0	0.0000000000	False
processors a bus structures	0.0	0.0	0.0	2.0	0.0000000000	False
bus structures interfacing issues	0.0	0.0	0.0	2.0	0.0000000000	False
optimization of the program	0.0	0.0	0.0	2.0	0.0000000000	False
real time os issues	0.0	0.0	0.0	2.0	0.0000000000	False
aspects of network embedded	0.0	0.0	0.0	2.0	0.0000000000	False
start with the definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition of an embedded	0.0	0.0	0.0	2.0	0.0000000000	False
system micro any device	0.0	0.0	0.0	2.0	0.0000000000	False
computer it has hardware	0.0	0.0	0.0	2.0	0.0000000000	False
system and its expected	0.0	0.0	0.0	2.0	0.0000000000	False
function without human intervention	0.0	0.0	0.0	2.0	0.0000000000	False
intervention an embedded system	0.0	0.0	0.0	2.0	0.0000000000	False
expect to respond monitor	0.0	0.0	0.0	2.0	0.0000000000	False
external environment using sensors	0.0	0.0	0.0	2.0	0.0000000000	False
talking about is embedding	0.0	0.0	0.0	2.0	0.0000000000	False
embedding a computer embedding	0.0	0.0	0.0	2.0	0.0000000000	False
computer embedding a computer	0.0	0.0	0.0	2.0	0.0000000000	False
computer into an applies	0.0	0.0	0.0	2.0	0.0000000000	False
applies and that computer	0.0	0.0	0.0	2.0	0.0000000000	False
computer is not expected	0.0	0.0	0.0	2.0	0.0000000000	False
embedded into an plants	0.0	0.0	0.0	2.0	0.0000000000	False
interfaces and the model	0.0	0.0	0.0	2.0	0.0000000000	False
examples are personal digital	0.0	0.0	0.0	2.0	0.0000000000	False
personal digital assistance printers	0.0	0.0	0.0	2.0	0.0000000000	False
family are with automobiles	0.0	0.0	0.0	2.0	0.0000000000	False
automobiles in fact automobiles	0.0	0.0	0.0	2.0	0.0000000000	False
embedded networks computing system	0.0	0.0	0.0	2.0	0.0000000000	False
networks computing system television	0.0	0.0	0.0	2.0	0.0000000000	False
system television in television	0.0	0.0	0.0	2.0	0.0000000000	False
television from various purposes	0.0	0.0	0.0	2.0	0.0000000000	False
managing these microcontrollers designing	0.0	0.0	0.0	2.0	0.0000000000	False
microcontrollers designing there hardware	0.0	0.0	0.0	2.0	0.0000000000	False
designing there hardware designing	0.0	0.0	0.0	2.0	0.0000000000	False
hardware designing the software	0.0	0.0	0.0	2.0	0.0000000000	False
managing this a planes	0.0	0.0	0.0	2.0	0.0000000000	False
challenge than the adopt	0.0	0.0	0.0	2.0	0.0000000000	False
designing a general purpose	0.0	0.0	0.0	2.0	0.0000000000	False
purpose computer so lets	0.0	0.0	0.0	2.0	0.0000000000	False
surveillance system in fact	0.0	0.0	0.0	2.0	0.0000000000	False
system in fact surveillance	0.0	0.0	0.0	2.0	0.0000000000	False
fact surveillance system oblate	0.0	0.0	0.0	2.0	0.0000000000	False
reasons so your video	0.0	0.0	0.0	2.0	0.0000000000	False
part of embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
palm that is appeared	0.0	0.0	0.0	2.0	0.0000000000	False
cases the different types	0.0	0.0	0.0	2.0	0.0000000000	False
thirty two bit microcontroller	0.0	0.0	2.99793530626	6.0	0.0000000000	False
planes example front panel	0.0	0.0	0.0	2.0	0.0000000000	False
front panel of microwave	0.0	0.0	0.0	2.0	0.0000000000	False
panel of microwave oven	0.0	0.0	0.0	2.0	0.0000000000	False
examples because the functionality	0.0	0.0	0.0	2.0	0.0000000000	False
functionality that it handles	0.0	0.0	0.0	2.0	0.0000000000	False
fact that the camera	0.0	0.0	0.0	2.0	0.0000000000	False
thirty two bit processor	0.0	0.0	0.0	2.0	0.0000000000	False
processor because it handles	0.0	0.0	0.0	2.0	0.0000000000	False
similarly in an analog	0.0	0.0	0.0	2.0	0.0000000000	False
handles primarily the problem	0.0	0.0	0.0	2.0	0.0000000000	False
tuning and channel selection	0.0	0.0	0.0	2.0	0.0000000000	False
digital tv decompression disk	0.0	0.0	0.0	2.0	0.0000000000	False
box your microcontroller handles	0.0	0.0	0.0	2.0	0.0000000000	False
microcontroller handles a number	0.0	0.0	0.0	2.0	0.0000000000	False
number of complex functions	0.0	0.0	0.0	2.0	0.0000000000	False
microprocessors of four bit	0.0	0.0	0.0	2.0	0.0000000000	False
bit microcontroller can check	0.0	0.0	0.0	2.0	0.0000000000	False
tension of the seat	0.0	0.0	0.0	2.0	0.0000000000	False
belt microcontrollers can run	0.0	0.0	0.0	2.0	0.0000000000	False
run the display services	0.0	0.0	0.0	2.0	0.0000000000	False
services on the dashboard	0.0	0.0	0.0	2.0	0.0000000000	False
sees the engine controlling	0.0	0.0	0.0	2.0	0.0000000000	False
microcontroller that is sixteen	0.0	0.0	0.0	2.0	0.0000000000	False
two bit microcontroller lets	0.0	0.0	0.0	2.0	0.0000000000	False
lets look an architecture	0.0	0.0	0.0	2.0	0.0000000000	False
system of breaking system	0.0	0.0	0.0	2.0	0.0000000000	False
aspect of an automobile	0.0	0.0	0.0	2.0	0.0000000000	False
break which are control	0.0	0.0	0.0	2.0	0.0000000000	False
control by hydraulic pump	0.0	0.0	0.0	2.0	0.0000000000	False
pump and your embedded	0.0	0.0	0.0	2.0	0.0000000000	False
system is this automated	0.0	0.0	0.0	2.0	0.0000000000	False
breaking system which receives	0.0	0.0	0.0	2.0	0.0000000000	False
system which receives input	0.0	0.0	0.0	2.0	0.0000000000	False
input from the sensors	0.0	0.0	0.0	2.0	0.0000000000	False
sensors and then depending	0.0	0.0	0.0	2.0	0.0000000000	False
depending on the software	0.0	0.0	0.0	2.0	0.0000000000	False
software that is running	0.0	0.0	0.0	2.0	0.0000000000	False
running in the automated	0.0	0.0	0.0	2.0	0.0000000000	False
automated breaking system actuate	0.0	0.0	0.0	2.0	0.0000000000	False
actuate the hydraulic pump	0.0	0.0	0.0	2.0	0.0000000000	False
hydraulic pump to control	0.0	0.0	0.0	2.0	0.0000000000	False
control system begin implemented	0.0	0.0	0.0	2.0	0.0000000000	False
microcontrollers in an automobile	0.0	0.0	0.0	2.0	0.0000000000	False
characteristics of embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems first thing	0.0	0.0	0.0	2.0	0.0000000000	False
sophisticated functionality the degree	0.0	0.0	0.0	2.0	0.0000000000	False
vary from a plans	0.0	0.0	0.0	2.0	0.0000000000	False
plans this satisfy real	0.0	0.0	0.0	2.0	0.0000000000	False
satisfy real time operation	0.0	0.0	0.0	2.0	0.0000000000	False
operation we shall comeback	0.0	0.0	0.0	2.0	0.0000000000	False
comeback to this point	0.0	0.0	0.0	2.0	0.0000000000	False
cases low manufacturing cost	0.0	0.0	0.0	2.0	0.0000000000	False
manufacturing cost but cost	0.0	0.0	0.0	2.0	0.0000000000	False
request further closer examination	0.0	0.0	0.0	2.0	0.0000000000	False
examination in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
cases this a plans	0.0	0.0	0.0	2.0	0.0000000000	False
plans to uses application	0.0	0.0	0.0	2.0	0.0000000000	False
processors which we find	0.0	0.0	0.0	2.0	0.0000000000	False
work with restricted memory	0.0	0.0	0.0	2.0	0.0000000000	False
val mounter devices powered	0.0	0.0	0.0	2.0	0.0000000000	False
powered from direct power	0.0	0.0	0.0	2.0	0.0000000000	False
power supply then power	0.0	0.0	0.0	2.0	0.0000000000	False
supply then power consumption	0.0	0.0	0.0	2.0	0.0000000000	False
heat management heat dissipation	0.0	0.0	0.0	2.0	0.0000000000	False
management heat dissipation design	0.0	0.0	0.0	2.0	0.0000000000	False
design for this devices	0.0	0.0	0.0	2.0	0.0000000000	False
devices which can add	0.0	0.0	0.0	2.0	0.0000000000	False
cost of the embedded	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system so lets	0.0	0.0	0.0	2.0	0.0000000000	False
issue of manufacturing cost	0.0	0.0	0.0	2.0	0.0000000000	False
two aspects first aspect	0.0	0.0	0.0	2.0	0.0000000000	False
call non recurring engineering	0.0	0.0	0.0	2.0	0.0000000000	False
non recurring engineering cost	0.0	0.0	0.0	2.0	0.0000000000	False
cost into that system	0.0	0.0	0.0	2.0	0.0000000000	False
system the other aspect	0.0	0.0	0.0	2.0	0.0000000000	False
aspect of the cost	0.0	0.0	0.0	2.0	0.0000000000	False
targeting um mass market	0.0	0.0	0.0	2.0	0.0000000000	False
optimize is a production	0.0	0.0	0.0	2.0	0.0000000000	False
set for high production	0.0	0.0	0.0	2.0	0.0000000000	False
designing um an automated	0.0	0.0	0.0	2.0	0.0000000000	False
system for an aircraft	0.0	0.0	0.0	2.0	0.0000000000	False
aircraft i can invest	0.0	0.0	0.0	2.0	0.0000000000	False
money for its development	0.0	0.0	0.0	2.0	0.0000000000	False
designing as cell phone	0.0	0.0	0.0	2.0	0.0000000000	False
phone or low cost	0.0	0.0	0.0	2.0	0.0000000000	False
low cost cell phone	0.0	0.0	0.0	2.0	0.0000000000	False
cost cell phone aiming	0.0	0.0	0.0	2.0	0.0000000000	False
phone aiming to serve	0.0	0.0	0.0	2.0	0.0000000000	False
serve a mass market	0.0	0.0	0.0	2.0	0.0000000000	False
market so the based	0.0	0.0	0.0	2.0	0.0000000000	False
depend on the number	0.0	0.0	0.0	2.0	0.0000000000	False
back to this issue	0.0	0.0	0.0	2.0	0.0000000000	False
operation that will started	0.0	0.0	0.0	2.0	0.0000000000	False
operation the basic definition	0.0	0.0	0.0	2.0	0.0000000000	False
definition is then operations	0.0	0.0	0.0	2.0	0.0000000000	False
operations must be completed	0.0	0.0	0.0	4.0	0.0000000000	False
hard real time headlines	0.0	0.0	0.0	2.0	0.0000000000	False
soft real time headlines	0.0	0.0	0.0	2.0	0.0000000000	False
occluding also we classify	0.0	0.0	0.0	2.0	0.0000000000	False
classify real time systems	0.0	0.0	0.0	2.0	0.0000000000	False
hard real time systems	0.0	0.0	0.0	2.0	0.0000000000	False
control if i miss	0.0	0.0	0.0	2.0	0.0000000000	False
soft real time systems	0.0	0.0	0.0	2.0	0.0000000000	False
video on a laptop	0.0	0.0	0.0	2.0	0.0000000000	False
distance you viewing experience	0.0	0.0	0.0	2.0	0.0000000000	False
viewing experience many systems	0.0	0.0	0.0	2.0	0.0000000000	False
means this embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems are receiving	0.0	0.0	0.0	2.0	0.0000000000	False
systems are receiving inputs	0.0	0.0	0.0	2.0	0.0000000000	False
worlds and these inputs	0.0	0.0	0.0	2.0	0.0000000000	False
handle this different rate	0.0	0.0	0.0	2.0	0.0000000000	False
requirements in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
fault tolerance and reliability	0.0	0.0	0.0	4.0	0.0000000000	False
reliability further the systems	0.0	0.0	0.0	2.0	0.0000000000	False
safe systems um avoid	0.0	0.0	0.0	2.0	0.0000000000	False
physical or economic damage	0.0	0.0	0.0	2.0	0.0000000000	False
economic damage to person	0.0	0.0	0.0	2.0	0.0000000000	False
systems then the design	0.0	0.0	0.0	2.0	0.0000000000	False
programmability of this systems	0.0	0.0	0.0	2.0	0.0000000000	False
lifetime of the system	0.0	0.0	0.0	2.0	0.0000000000	False
system that means nonce	0.0	0.0	0.0	2.0	0.0000000000	False
nonce programmed this systems	0.0	0.0	0.0	2.0	0.0000000000	False
expected to be programmed	0.0	0.0	0.0	2.0	0.0000000000	False
design for specific task	0.0	0.0	0.0	2.0	0.0000000000	False
basically what we call	0.0	0.0	0.0	2.0	0.0000000000	False
important you can realize	0.0	0.0	0.0	2.0	0.0000000000	False
finally deliver the goals	0.0	0.0	0.0	2.0	0.0000000000	False
vending machine which users	0.0	0.0	0.0	2.0	0.0000000000	False
users eight bit motorola	0.0	0.0	0.0	2.0	0.0000000000	False
two thousand four introduction	0.0	0.0	0.0	2.0	0.0000000000	False
thousand four introduction product	0.0	0.0	0.0	2.0	0.0000000000	False
web enabled cashless vending	0.0	0.0	0.0	2.0	0.0000000000	False
enabled cashless vending machine	0.0	0.0	0.0	2.0	0.0000000000	False
simple task of delivering	0.0	0.0	0.0	2.0	0.0000000000	False
response to the cash	0.0	0.0	0.0	2.0	0.0000000000	False
change into an web	0.0	0.0	0.0	2.0	0.0000000000	False
web enabling the stock	0.0	0.0	0.0	2.0	0.0000000000	False
stock can be monitor	0.0	0.0	0.0	2.0	0.0000000000	False
remotely the whole cash	0.0	0.0	0.0	2.0	0.0000000000	False
credit cards or smart	0.0	0.0	0.0	2.0	0.0000000000	False
cards or smart cards	0.0	0.0	0.0	2.0	0.0000000000	False
monitored from a remote	0.0	0.0	0.0	2.0	0.0000000000	False
location this has happen	0.0	0.0	0.0	2.0	0.0000000000	False
brought in sophisticated processors	0.0	0.0	0.0	2.0	0.0000000000	False
sophisticated processors sophisticated functionalities	0.0	0.0	0.0	2.0	0.0000000000	False
robot and it users	0.0	0.0	0.0	2.0	0.0000000000	False
users an eight bit	0.0	0.0	0.0	2.0	0.0000000000	False
intel microprocessor in fact	0.0	0.0	0.0	2.0	0.0000000000	False
gps receiver global positioning	0.0	0.0	0.0	2.0	0.0000000000	False
receiver global positioning system	0.0	0.0	0.0	2.0	0.0000000000	False
system which actually enables	0.0	0.0	0.0	2.0	0.0000000000	False
enables any any transport	0.0	0.0	0.0	2.0	0.0000000000	False
transport vehicle to deprovement	0.0	0.0	0.0	2.0	0.0000000000	False
systems which provides automated	0.0	0.0	0.0	2.0	0.0000000000	False
navigational tool this gps	0.0	0.0	0.0	2.0	0.0000000000	False
tool this gps receivers	0.0	0.0	0.0	2.0	0.0000000000	False
component is the communication	0.0	0.0	0.0	2.0	0.0000000000	False
output regarding its positions	0.0	0.0	0.0	2.0	0.0000000000	False
mp3 player various versions	0.0	0.0	0.0	2.0	0.0000000000	False
versions of mp3 players	0.0	0.0	0.0	2.0	0.0000000000	False
compress form of audio	0.0	0.0	0.0	2.0	0.0000000000	False
decompress audio to play	0.0	0.0	0.0	2.0	0.0000000000	False
apprentic sophisticated computational task	0.0	0.0	0.0	2.0	0.0000000000	False
find that the microprocessor	0.0	0.0	0.0	2.0	0.0000000000	False
thirty two bit risk	0.0	0.0	0.0	2.0	0.0000000000	False
two bit risk microprocessor	0.0	0.0	0.0	2.0	0.0000000000	False
player the same issue	0.0	0.0	0.0	2.0	0.0000000000	False
dvd has got pdo	0.0	0.0	0.0	2.0	0.0000000000	False
rate at a video	0.0	0.0	0.0	2.0	0.0000000000	False
video rate video rate	0.0	0.0	0.0	2.0	0.0000000000	False
rate video rate means	0.0	0.0	0.0	2.0	0.0000000000	False
rate means what twenty	0.0	0.0	0.0	2.0	0.0000000000	False
effectively of about forty	0.0	0.0	0.0	2.0	0.0000000000	False
forty milliseconds to decompress	0.0	0.0	0.0	2.0	0.0000000000	False
decompress of video file	0.0	0.0	0.0	2.0	0.0000000000	False
case aprentic sophisticated microprocessor	0.0	0.0	0.0	2.0	0.0000000000	False
sophisticated microprocessor to work	0.0	0.0	0.0	2.0	0.0000000000	False
thirty two bit risc	0.0	0.0	0.0	2.0	0.0000000000	False
two bit risc microprocessor	0.0	0.0	0.0	2.0	0.0000000000	False
sony aibo robotic dog	0.0	0.0	0.0	4.0	0.0000000000	False
popular pet in japan	0.0	0.0	0.0	2.0	0.0000000000	False
japan and it users	0.0	0.0	0.0	2.0	0.0000000000	False
microprocessor or a microcontroller	0.0	0.0	0.0	4.0	0.0000000000	False
reduces sixty four bit	0.0	0.0	0.0	2.0	0.0000000000	False
sixty four bit neat	0.0	0.0	0.0	4.0	0.0000000000	False
four bit neat processor	0.0	0.0	0.0	4.0	0.0000000000	False
processor it uses sixty	0.0	0.0	0.0	2.0	0.0000000000	False
number of complex tasks	0.0	0.0	0.0	2.0	0.0000000000	False
familiar with this robo	0.0	0.0	0.0	2.0	0.0000000000	False
football between different robotics	0.0	0.0	0.0	2.0	0.0000000000	False
robotics teams in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact this sony aibo	0.0	0.0	0.0	2.0	0.0000000000	False
algorithms into to detect	0.0	0.0	0.0	2.0	0.0000000000	False
balls towards a goal	0.0	0.0	0.0	2.0	0.0000000000	False
functionalities have been built	0.0	0.0	0.0	2.0	0.0000000000	False
request pretty sophisticated processors	0.0	0.0	0.0	2.0	0.0000000000	False
sophisticated processors to handle	0.0	0.0	0.0	2.0	0.0000000000	False
sixty four bit mips	0.0	0.0	0.0	2.0	0.0000000000	False
four bit mips risc	0.0	0.0	0.0	2.0	0.0000000000	False
risc so now lets	0.0	0.0	0.0	2.0	0.0000000000	False
types of a embedded	0.0	0.0	0.0	2.0	0.0000000000	False
examples now lets classify	0.0	0.0	0.0	2.0	0.0000000000	False
lets classify the examples	0.0	0.0	0.0	2.0	0.0000000000	False
examples into different types	0.0	0.0	0.0	2.0	0.0000000000	False
similar to general computing	0.0	0.0	5.99793530626	6.0	0.0000000000	False
general computing like pda	0.0	0.0	0.0	2.0	0.0000000000	False
computing like pda video	0.0	0.0	0.0	2.0	0.0000000000	False
pda video games set	0.0	0.0	0.0	2.0	0.0000000000	False
games set of boxes	0.0	0.0	0.0	2.0	0.0000000000	False
boxes automatic teller machines	0.0	0.0	0.0	2.0	0.0000000000	False
maturity of the task	0.0	0.0	0.0	2.0	0.0000000000	False
form of the task	0.0	0.0	0.0	2.0	0.0000000000	False
thing is the video	0.0	0.0	0.0	2.0	0.0000000000	False
games okay you provide	0.0	0.0	0.0	2.0	0.0000000000	False
user provides a input	0.0	0.0	0.0	2.0	0.0000000000	False
input and it expect	0.0	0.0	0.0	2.0	0.0000000000	False
change the external world	0.0	0.0	0.0	2.0	0.0000000000	False
world so these devices	0.0	0.0	0.0	2.0	0.0000000000	False
general purpose computing machines	0.0	0.0	0.0	2.0	0.0000000000	False
computing machines they respond	0.0	0.0	0.0	2.0	0.0000000000	False
respond to users input	0.0	0.0	0.0	2.0	0.0000000000	False
side i got control	0.0	0.0	0.0	2.0	0.0000000000	False
system whose basic job	0.0	0.0	0.0	2.0	0.0000000000	False
actuating the feedback control	0.0	0.0	0.0	2.0	0.0000000000	False
actions i mean examples	0.0	0.0	0.0	2.0	0.0000000000	False
vehicles engine foal injection	0.0	0.0	0.0	2.0	0.0000000000	False
injection to be control	0.0	0.0	0.0	2.0	0.0000000000	False
control flight control nuclear	0.0	0.0	0.0	2.0	0.0000000000	False
flight control nuclear reactors	0.0	0.0	0.0	2.0	0.0000000000	False
examples of embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems which belongs	0.0	0.0	0.0	2.0	0.0000000000	False
belongs to the category	0.0	0.0	0.0	2.0	0.0000000000	False
category of control systems	0.0	0.0	0.0	2.0	0.0000000000	False
job or basic focus	0.0	0.0	0.0	2.0	0.0000000000	False
focus is signal processing	0.0	0.0	0.0	2.0	0.0000000000	False
signal processing your mp3	0.0	0.0	0.0	2.0	0.0000000000	False
processing your mp3 players	0.0	0.0	0.0	2.0	0.0000000000	False
mp3 players your dvd	0.0	0.0	0.0	2.0	0.0000000000	False
players your dvd players	0.0	0.0	0.0	2.0	0.0000000000	False
dvd players radar control	0.0	0.0	0.0	2.0	0.0000000000	False
players radar control system	0.0	0.0	0.0	2.0	0.0000000000	False
system the basic job	0.0	0.0	0.0	2.0	0.0000000000	False
cising of the deter	0.0	0.0	0.0	2.0	0.0000000000	False
deter similarly a sonar	0.0	0.0	0.0	2.0	0.0000000000	False
similarly a sonar system	0.0	0.0	0.0	2.0	0.0000000000	False
processing systems and communication	0.0	0.0	0.0	2.0	0.0000000000	False
networking is another category	0.0	0.0	0.0	2.0	0.0000000000	False
web enable vending machine	0.0	0.0	0.0	2.0	0.0000000000	False
kind of an internet	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system is expected	0.0	0.0	0.0	4.0	0.0000000000	False
actuation sensing an actuation	0.0	0.0	0.0	2.0	0.0000000000	False
task it must realize	0.0	0.0	0.0	2.0	0.0000000000	False
control second important issue	0.0	0.0	0.0	2.0	0.0000000000	False
sequencing logic this sequencing	0.0	0.0	0.0	2.0	0.0000000000	False
logic this sequencing logic	0.0	0.0	0.0	2.0	0.0000000000	False
logic is obviously task	0.0	0.0	0.0	2.0	0.0000000000	False
general purpose sequencing logic	0.0	0.0	0.0	2.0	0.0000000000	False
task specific sequencing logic	0.0	0.0	0.0	2.0	0.0000000000	False
specific sequencing logic implemented	0.0	0.0	0.0	2.0	0.0000000000	False
interfacing and embedded system	0.0	0.0	0.0	2.0	0.0000000000	False
system with external sense	0.0	0.0	0.0	2.0	0.0000000000	False
sense to be input	0.0	0.0	0.0	2.0	0.0000000000	False
processing ability to deal	0.0	0.0	0.0	2.0	0.0000000000	False
specific interfacing because application	0.0	0.0	0.0	2.0	0.0000000000	False
senses and what kind	0.0	0.0	0.0	2.0	0.0000000000	False
actuated to be interconnected	0.0	0.0	0.0	2.0	0.0000000000	False
interface this interfacing implies	0.0	0.0	0.0	2.0	0.0000000000	False
interfacing implies both hardware	0.0	0.0	0.0	2.0	0.0000000000	False
software next thing fault	0.0	0.0	0.0	2.0	0.0000000000	False
occurs the basic issue	0.0	0.0	0.0	2.0	0.0000000000	False
issue or basic design	0.0	0.0	0.0	2.0	0.0000000000	False
design philosophy for fault	0.0	0.0	0.0	2.0	0.0000000000	False
philosophy for fault response	0.0	0.0	0.0	2.0	0.0000000000	False
call graceful depredation cattest	0.0	0.0	0.0	2.0	0.0000000000	False
graceful depredation cattest traffic	0.0	0.0	0.0	2.0	0.0000000000	False
depredation cattest traffic failure	0.0	0.0	0.0	2.0	0.0000000000	False
failure should not happening	0.0	0.0	0.0	2.0	0.0000000000	False
system should tell users	0.0	0.0	0.0	2.0	0.0000000000	False
graceful it would decreed	0.0	0.0	0.0	2.0	0.0000000000	False
message to the user	0.0	0.0	0.0	2.0	0.0000000000	False
user saying that battery	0.0	0.0	0.0	2.0	0.0000000000	False
suddenly stop its activity	0.0	0.0	0.0	2.0	0.0000000000	False
sun so graceful degradation	0.0	0.0	0.0	2.0	0.0000000000	False
architecture of the embedded	0.0	0.0	0.0	2.0	0.0000000000	False
examples so now lets	0.0	0.0	0.0	2.0	0.0000000000	False
things which are involved	0.0	0.0	0.0	2.0	0.0000000000	False
expanded the basic block	0.0	0.0	0.0	4.0	0.0000000000	False
basic block have expanded	0.0	0.0	0.0	2.0	0.0000000000	False
block and have added	0.0	0.0	0.0	2.0	0.0000000000	False
earlier i adjust shown	0.0	0.0	0.0	2.0	0.0000000000	False
adjust shown the cpu	0.0	0.0	0.0	2.0	0.0000000000	False
showing obvious the memory	0.0	0.0	0.0	2.0	0.0000000000	False
analog to digital converters	0.0	0.0	0.0	2.0	0.0000000000	False
digital to analog converters	0.0	0.0	0.0	2.0	0.0000000000	False
converters this ad conversion	0.0	0.0	0.0	2.0	0.0000000000	False
interface to the sensor	0.0	0.0	0.0	2.0	0.0000000000	False
block actually provides interface	0.0	0.0	0.0	2.0	0.0000000000	False
interface to the actuators	0.0	0.0	0.0	2.0	0.0000000000	False
actuators because an embedded	0.0	0.0	0.0	2.0	0.0000000000	False
system which is situated	0.0	0.0	0.0	2.0	0.0000000000	False
situated in a environment	0.0	0.0	0.0	2.0	0.0000000000	False
expected to receive sensor	0.0	0.0	0.0	2.0	0.0000000000	False
sensor inputs and actuate	0.0	0.0	0.0	2.0	0.0000000000	False
change the external environment	0.0	0.0	0.0	2.0	0.0000000000	False
essential and integral component	0.0	0.0	0.0	2.0	0.0000000000	False
integral component in majority	0.0	0.0	0.0	2.0	0.0000000000	False
majority of your embedded	0.0	0.0	0.0	2.0	0.0000000000	False
systems here are shown	0.0	0.0	0.0	2.0	0.0000000000	False
fpga or acid block	0.0	0.0	0.0	2.0	0.0000000000	False
execute my software satisfying	0.0	0.0	0.0	2.0	0.0000000000	False
satisfying real time circumstance	0.0	0.0	0.0	2.0	0.0000000000	False
circumstance under those circumstance	0.0	0.0	0.0	2.0	0.0000000000	False
interface with my cpu	0.0	0.0	0.0	2.0	0.0000000000	False
implemented on an fpga	0.0	0.0	0.0	2.0	0.0000000000	False
alumeter cpu now lets	0.0	0.0	0.0	2.0	0.0000000000	False
lets look an order	0.0	0.0	0.0	2.0	0.0000000000	False
implement with the cpu	0.0	0.0	0.0	2.0	0.0000000000	False
cpu the human interface	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems any control	0.0	0.0	0.0	2.0	0.0000000000	False
systems any control functions	0.0	0.0	0.0	2.0	0.0000000000	False
functions to be altered	0.0	0.0	0.0	2.0	0.0000000000	False
interface so this interface	0.0	0.0	0.0	2.0	0.0000000000	False
find in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
simple um simple led	0.0	0.0	0.0	2.0	0.0000000000	False
based informative um color	0.0	0.0	0.0	2.0	0.0000000000	False
informative um color color	0.0	0.0	0.0	2.0	0.0000000000	False
user can be informed	0.0	0.0	0.0	2.0	0.0000000000	False
informed what is happening	0.0	0.0	0.0	2.0	0.0000000000	False
tools why diagnostic tools	0.0	0.0	0.0	2.0	0.0000000000	False
ability is a failure	0.0	0.0	0.0	2.0	0.0000000000	False
occurs how to trace	0.0	0.0	0.0	2.0	0.0000000000	False
diagnostic tools to interface	0.0	0.0	0.0	2.0	0.0000000000	False
working second important thing	0.0	0.0	0.0	2.0	0.0000000000	False
thing why diagnostic tools	0.0	0.0	0.0	2.0	0.0000000000	False
starting up or system	0.0	0.0	0.0	2.0	0.0000000000	False
parts are not functioning	0.0	0.0	0.0	2.0	0.0000000000	False
properly what can happen	0.0	0.0	0.0	2.0	0.0000000000	False
damage to the users	0.0	0.0	0.0	2.0	0.0000000000	False
users because of malfunctioning	0.0	0.0	0.0	2.0	0.0000000000	False
malfunctioning of some hardware	0.0	0.0	0.0	2.0	0.0000000000	False
check at regular basics	0.0	0.0	0.0	2.0	0.0000000000	False
regular basics so diagnostics	0.0	0.0	0.0	2.0	0.0000000000	False
basics so diagnostics tools	0.0	0.0	0.0	2.0	0.0000000000	False
design the cooling circuit	0.0	0.0	0.0	2.0	0.0000000000	False
aspect of it design	0.0	0.0	0.0	2.0	0.0000000000	False
important obviously the casing	0.0	0.0	0.0	2.0	0.0000000000	False
casing the whole system	0.0	0.0	0.0	2.0	0.0000000000	False
system should properly packaged	0.0	0.0	0.0	2.0	0.0000000000	False
exceptive to be pleased	0.0	0.0	0.0	2.0	0.0000000000	False
pleased so this packaging	0.0	0.0	0.0	2.0	0.0000000000	False
good well design system	0.0	0.0	0.0	2.0	0.0000000000	False
design system can fail	0.0	0.0	0.0	2.0	0.0000000000	False
bad packaging the system	0.0	0.0	0.0	2.0	0.0000000000	False
moister okay the moister	0.0	0.0	0.0	2.0	0.0000000000	False
effective electronics then heat	0.0	0.0	0.0	2.0	0.0000000000	False
heat can effective electronics	0.0	0.0	0.0	2.0	0.0000000000	False
aspect of the design	0.0	0.0	0.0	2.0	0.0000000000	False
design becomes extrument point	0.0	0.0	0.0	2.0	0.0000000000	False
discuss those mechanical aspects	0.0	0.0	0.0	2.0	0.0000000000	False
aspects of an embedded	0.0	0.0	0.0	2.0	0.0000000000	False
concuss about the fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact that these aspects	0.0	0.0	0.0	2.0	0.0000000000	False
important for any kind	0.0	0.0	0.0	2.0	0.0000000000	False
kind an appliances design	0.0	0.0	0.0	2.0	0.0000000000	False
appliances design and implementation	0.0	0.0	0.0	2.0	0.0000000000	False
implement an embedded system	0.0	0.0	0.0	4.0	0.0000000000	False
discuss obviously the processing	0.0	0.0	0.0	2.0	0.0000000000	False
processing elements the processing	0.0	0.0	0.0	2.0	0.0000000000	False
elements the processing elements	0.0	0.0	0.0	2.0	0.0000000000	False
peripheral devices because input	0.0	0.0	0.0	2.0	0.0000000000	False
input an output devices	0.0	0.0	0.0	2.0	0.0000000000	False
component in this context	0.0	0.0	0.0	2.0	0.0000000000	False
interface sensors and actuators	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of interfacing protocols	0.0	0.0	0.0	2.0	0.0000000000	False
protocols which can vary	0.0	0.0	0.0	2.0	0.0000000000	False
vary from one sensors	0.0	0.0	0.0	2.0	0.0000000000	False
sensors to another sensors	0.0	0.0	0.0	2.0	0.0000000000	False
aspects of an hardware	0.0	0.0	0.0	2.0	0.0000000000	False
hardware of an embedded	0.0	0.0	0.0	2.0	0.0000000000	False
aspects of the hardware	0.0	0.0	0.0	2.0	0.0000000000	False
computing the only issue	0.0	0.0	0.0	2.0	0.0000000000	False
important is these aspects	0.0	0.0	0.0	2.0	0.0000000000	False
purpose computer we tend	0.0	0.0	0.0	2.0	0.0000000000	False
talk about standard input	0.0	0.0	0.0	4.0	0.0000000000	False
standard input output devices	0.0	1.0	0.0	4.0	0.0000000000	False
devices although that set	0.0	0.0	0.0	2.0	0.0000000000	False
set is getting expanded	0.0	0.0	0.0	2.0	0.0000000000	False
expanded day by day	0.0	0.0	0.0	2.0	0.0000000000	False
day but we tend	0.0	0.0	0.0	2.0	0.0000000000	False
set of input output	0.0	0.0	0.0	2.0	0.0000000000	False
sensors and its sensor	0.0	0.0	0.0	2.0	0.0000000000	False
processors which are targeted	0.0	0.0	0.0	2.0	0.0000000000	False
targeted for embedded applications	0.0	0.0	0.0	2.0	0.0000000000	False
mechanisms in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
cases even simpler mechanisms	0.0	0.0	0.0	2.0	0.0000000000	False
complex mechanism to interface	0.0	0.0	0.0	2.0	0.0000000000	False
interface with external devices	0.0	0.0	0.0	2.0	0.0000000000	False
talked about system software	0.0	0.0	0.0	4.0	0.0000000000	False
system software and application	0.0	0.0	0.0	2.0	0.0000000000	False
software and application software	0.0	0.0	0.0	2.0	0.0000000000	False
software what s system	0.0	0.0	0.0	2.0	0.0000000000	False
software typically we talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about assemblers compilers	0.0	0.0	0.0	2.0	0.0000000000	False
compilers that is language	0.0	0.0	0.0	2.0	0.0000000000	False
translators i have clause	0.0	0.0	0.0	2.0	0.0000000000	False
clause of system software	0.0	0.0	0.0	4.0	0.0000000000	False
software the other clause	0.0	0.0	0.0	2.0	0.0000000000	False
system software or operating	0.0	0.0	0.0	2.0	0.0000000000	False
software or operating systems	0.0	0.0	0.0	2.0	0.0000000000	False
operating systems now majority	0.0	0.0	0.0	2.0	0.0000000000	False
majority of the cases	0.0	0.0	0.0	2.0	0.0000000000	False
systems will have specialized	0.0	0.0	0.0	2.0	0.0000000000	False
general purpose operating system	0.0	0.0	0.0	2.0	0.0000000000	False
system like your window	0.0	0.0	0.0	2.0	0.0000000000	False
window units order variance	0.0	0.0	0.0	2.0	0.0000000000	False
caricaturists of the embedded	0.0	0.0	0.0	2.0	0.0000000000	False
encounter in general purpose	0.0	0.0	0.0	2.0	0.0000000000	False
general purpose computing system	0.0	0.0	0.0	2.0	0.0000000000	False
satisfy the general purpose	0.0	0.0	0.0	2.0	0.0000000000	False
purpose need the requirements	0.0	0.0	0.0	2.0	0.0000000000	False
requirements of a programming	0.0	0.0	0.0	2.0	0.0000000000	False
case this embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems are dedicated	0.0	0.0	0.0	2.0	0.0000000000	False
tuned for that kind	0.0	0.0	0.0	2.0	0.0000000000	False
kind of a requirement	0.0	0.0	0.0	2.0	0.0000000000	False
real time scheduling features	0.0	0.0	0.0	2.0	0.0000000000	False
require real tome scheduling	0.0	0.0	0.0	2.0	0.0000000000	False
computer you also compilers	0.0	0.0	0.0	2.0	0.0000000000	False
compile your high level	0.0	0.0	0.0	2.0	0.0000000000	False
high level language code	0.0	0.0	0.0	2.0	0.0000000000	False
code to the target	0.0	0.0	0.0	2.0	0.0000000000	False
code to be executed	0.0	0.0	0.0	2.0	0.0000000000	False
executed on that system	0.0	0.0	0.0	2.0	0.0000000000	False
incase of an embedded	0.0	0.0	0.0	4.0	0.0000000000	False
systems you will find	0.0	0.0	0.0	2.0	0.0000000000	False
find what we call	0.0	0.0	0.0	2.0	0.0000000000	False
cross assemblers and cross	0.0	0.0	0.0	2.0	0.0000000000	False
assemblers and cross compilers	0.0	0.0	0.0	2.0	0.0000000000	False
compilers and various kinds	0.0	0.0	0.0	2.0	0.0000000000	False
kinds of other development	0.0	0.0	0.0	2.0	0.0000000000	False
high level language program	0.0	0.0	0.0	2.0	0.0000000000	False
software would take place	0.0	0.0	0.0	2.0	0.0000000000	False
software will be loaded	0.0	0.0	0.0	2.0	0.0000000000	False
loaded onto the target	0.0	0.0	0.0	2.0	0.0000000000	False
cross compilers and cross	0.0	0.0	3.99793530626	6.0	0.0000000000	False
compilers and cross assemblers	0.0	0.0	3.99793530626	6.0	0.0000000000	False
write a c program	0.0	0.0	0.0	2.0	0.0000000000	False
compile your c program	0.0	0.0	0.0	2.0	0.0000000000	False
generate it will generate	0.0	0.0	0.0	2.0	0.0000000000	False
code for big microcontroller	0.0	0.0	0.0	2.0	0.0000000000	False
microcontroller and that code	0.0	0.0	0.0	2.0	0.0000000000	False
executed on that target	0.0	0.0	0.0	2.0	0.0000000000	False
compilers for a family	0.0	0.0	0.0	2.0	0.0000000000	False
variance of this processor	0.0	0.0	0.0	2.0	0.0000000000	False
processor because this processor	0.0	0.0	0.0	2.0	0.0000000000	False
differences in the number	0.0	0.0	0.0	2.0	0.0000000000	False
number of registers etcetera	0.0	0.0	0.0	2.0	0.0000000000	False
part of system software	0.0	0.0	0.0	2.0	0.0000000000	False
software which a call	0.0	0.0	0.0	2.0	0.0000000000	False
emulators what are emulators	0.0	0.0	0.0	2.0	0.0000000000	False
instructions to the emulators	0.0	0.0	0.0	2.0	0.0000000000	False
emulators this instruction set	0.0	0.0	0.0	2.0	0.0000000000	False
set emulators actually emulates	0.0	0.0	0.0	2.0	0.0000000000	False
processors on another target	0.0	0.0	0.0	2.0	0.0000000000	False
emulates the um behavior	0.0	0.0	0.0	2.0	0.0000000000	False
behavior of the target	0.0	0.0	0.0	2.0	0.0000000000	False
simple implement the instruction	0.0	0.0	0.0	2.0	0.0000000000	False
implement the instruction set	0.0	0.0	0.0	2.0	0.0000000000	False
set of the target	0.0	0.0	0.0	2.0	0.0000000000	False
processor in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
analysis of your code	0.0	0.0	0.0	2.0	0.0000000000	False
code on a host	0.0	0.0	0.0	2.0	0.0000000000	False
tools system software tools	0.0	0.0	0.0	2.0	0.0000000000	False
typically targeted for embedded	0.0	0.0	0.0	2.0	0.0000000000	False
targeted for embedded system	0.0	0.0	0.0	4.0	0.0000000000	False
communication of the tool	0.0	0.0	0.0	2.0	0.0000000000	False
simple software to execute	0.0	0.0	0.0	2.0	0.0000000000	False
connected to the target	0.0	0.0	0.0	2.0	0.0000000000	False
board through a hardware	0.0	0.0	0.0	2.0	0.0000000000	False
hardware connector that code	0.0	0.0	0.0	2.0	0.0000000000	False
developed on the target	0.0	0.0	0.0	2.0	0.0000000000	False
board can be loaded	0.0	0.0	0.0	2.0	0.0000000000	False
loaded via the connector	0.0	0.0	0.0	2.0	0.0000000000	False
happen you can monitor	0.0	0.0	0.0	2.0	0.0000000000	False
execution of the code	0.0	0.0	0.0	2.0	0.0000000000	False
tools so to summarize	0.0	0.0	0.0	2.0	0.0000000000	False
software we are talking	0.0	0.0	0.0	2.0	0.0000000000	False
talking about what compilers	0.0	0.0	0.0	2.0	0.0000000000	False
compilers in particular cross	0.0	0.0	0.0	2.0	0.0000000000	False
assemblers when you talk	0.0	0.0	0.0	2.0	0.0000000000	False
simulators and we talk	0.0	0.0	0.0	2.0	0.0000000000	False
talk about debugging tools	0.0	0.0	0.0	2.0	0.0000000000	False
tools so this system	0.0	0.0	0.0	2.0	0.0000000000	False
expect for general purpose	0.0	0.0	0.0	2.0	0.0000000000	False
systems which are targeted	0.0	0.0	0.0	2.0	0.0000000000	False
targeted for dedicated appliances	0.0	0.0	0.0	2.0	0.0000000000	False
appliances and many times	0.0	0.0	0.0	2.0	0.0000000000	False
times they do support	0.0	0.0	0.0	2.0	0.0000000000	False
support real time scheduling	0.0	0.0	0.0	2.0	0.0000000000	False
real time scheduling capabilities	0.0	0.0	0.0	2.0	0.0000000000	False
capabilities the next thing	0.0	0.0	0.0	2.0	0.0000000000	False
thing is application software	0.0	0.0	0.0	2.0	0.0000000000	False
application software obviously software	0.0	0.0	0.0	2.0	0.0000000000	False
support the same system	0.0	0.0	0.0	2.0	0.0000000000	False
operating system weak works	0.0	0.0	0.0	2.0	0.0000000000	False
system weak works running	0.0	0.0	0.0	2.0	0.0000000000	False
running on your laser	0.0	0.0	0.0	2.0	0.0000000000	False
plants but application software	0.0	0.0	0.0	2.0	0.0000000000	False
software in the laser	0.0	0.0	0.0	2.0	0.0000000000	False
laser printer is targeted	0.0	0.0	0.0	2.0	0.0000000000	False
multiple just a appliances	0.0	0.0	0.0	2.0	0.0000000000	False
appliances but your application	0.0	0.0	0.0	2.0	0.0000000000	False
application software would things	0.0	0.0	0.0	2.0	0.0000000000	False
things which the functionality	0.0	0.0	0.0	2.0	0.0000000000	False
functionality of this appliances	0.0	0.0	0.0	2.0	0.0000000000	False
history of hardware evolution	0.0	0.0	0.0	2.0	0.0000000000	False
status to the status	0.0	0.0	0.0	2.0	0.0000000000	False
status of embedded system	0.0	0.0	0.0	2.0	0.0000000000	False
purpose microprocessor and microcontroller	0.0	0.0	0.0	2.0	0.0000000000	False
microcontroller and in fact	0.0	0.0	0.0	2.0	0.0000000000	False
higher degree of integration	0.0	0.0	0.0	4.0	0.0000000000	False
peripherals have got integrated	0.0	0.0	0.0	2.0	0.0000000000	False
integrated into the chip	0.0	0.0	0.0	2.0	0.0000000000	False
general purpose microprocessor microcontroller	0.0	0.0	0.0	2.0	0.0000000000	False
system so unary cost	0.0	0.0	0.0	2.0	0.0000000000	False
unary cost towards development	0.0	0.0	0.0	2.0	0.0000000000	False
development of the processor	0.0	0.0	0.0	2.0	0.0000000000	False
processor is to minimized	0.0	0.0	0.0	2.0	0.0000000000	False
purpose processors and microcontrollers	0.0	0.0	0.0	2.0	0.0000000000	False
dsp that digital signal	0.0	0.0	0.0	2.0	0.0000000000	False
talking about digital signal	0.0	0.0	0.0	2.0	0.0000000000	False
processing i am talking	0.0	0.0	0.0	2.0	0.0000000000	False
variety of signal processor	0.0	0.0	0.0	2.0	0.0000000000	False
processor with different architecture	0.0	0.0	0.0	2.0	0.0000000000	False
architecture which are today	0.0	0.0	0.0	2.0	0.0000000000	False
microcontroller in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
permit this additional unary	0.0	0.0	0.0	2.0	0.0000000000	False
unary cost the kind	0.0	0.0	0.0	2.0	0.0000000000	False
top i have put	0.0	0.0	0.0	2.0	0.0000000000	False
put system on chip	0.0	0.0	0.0	2.0	0.0000000000	False
system on chip soc	0.0	0.0	0.0	2.0	0.0000000000	False
chip soc and soc	0.0	0.0	0.0	2.0	0.0000000000	False
find in an soc	0.0	0.0	0.0	2.0	0.0000000000	False
processor code but multiple	0.0	0.0	0.0	2.0	0.0000000000	False
code but multiple processor	0.0	0.0	0.0	2.0	0.0000000000	False
multiple processor codes alu	0.0	0.0	0.0	2.0	0.0000000000	False
processor codes alu peripherals	0.0	0.0	0.0	2.0	0.0000000000	False
integrated why because today	0.0	0.0	0.0	2.0	0.0000000000	False
textes instrument omap processor	0.0	0.0	0.0	2.0	0.0000000000	False
sitting inside the chip	0.0	0.0	0.0	2.0	0.0000000000	False
find there are variety	0.0	0.0	0.0	2.0	0.0000000000	False
talking about the system	0.0	0.0	0.0	2.0	0.0000000000	False
processor and its peripherals	0.0	0.0	0.0	2.0	0.0000000000	False
large number of peripherals	0.0	0.0	0.0	2.0	0.0000000000	False
coprocessors and even multiple	0.0	0.0	0.0	2.0	0.0000000000	False
multiple processors being integrated	0.0	0.0	0.0	2.0	0.0000000000	False
single piece of silicon	0.0	0.0	0.0	2.0	0.0000000000	False
sophisticated functionality being implemented	0.0	0.0	0.0	2.0	0.0000000000	False
system as single silicon	0.0	0.0	0.0	2.0	0.0000000000	False
design in a power	0.0	0.0	0.0	2.0	0.0000000000	False
power of the man	0.0	0.0	0.0	2.0	0.0000000000	False
canjancem of power software	0.0	0.0	0.0	2.0	0.0000000000	False
characteristics of the application	0.0	0.0	0.0	2.0	0.0000000000	False
software and the operating	0.0	0.0	0.0	2.0	0.0000000000	False
operating systems that supports	0.0	0.0	0.0	2.0	0.0000000000	False
means in an execution	0.0	0.0	0.0	2.0	0.0000000000	False
correct logically correct means	0.0	0.0	0.0	2.0	0.0000000000	False
means you all understand	0.0	0.0	0.0	2.0	0.0000000000	False
thing is temporal correctness	0.0	0.0	0.0	2.0	0.0000000000	False
correctness in this case	0.0	0.0	0.0	2.0	0.0000000000	False
purpose computer we talk	0.0	0.0	0.0	2.0	0.0000000000	False
multiple users multiple processors	0.0	0.0	0.0	2.0	0.0000000000	False
users multiple processors running	0.0	0.0	0.0	2.0	0.0000000000	False
reliability and fault tolerance	0.0	0.0	0.0	2.0	0.0000000000	False
tolerance obviously critical issues	0.0	0.0	0.0	2.0	0.0000000000	False
effort to this fault	0.0	0.0	0.0	2.0	0.0000000000	False
software has to application	0.0	0.0	0.0	2.0	0.0000000000	False
specific and single purpose	0.0	0.0	0.0	2.0	0.0000000000	False
familiar with this definition	0.0	0.0	0.0	2.0	0.0000000000	False
review so by multitasking	0.0	0.0	0.0	2.0	0.0000000000	False
important for embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
systems need to deal	0.0	0.0	0.0	2.0	0.0000000000	False
deal with several inputs	0.0	0.0	0.0	2.0	0.0000000000	False
outputs and multiple events	0.0	0.0	0.0	2.0	0.0000000000	False
multiple events can occur	0.0	0.0	0.0	2.0	0.0000000000	False
system in many cases	0.0	0.0	0.0	2.0	0.0000000000	False
exceptive to be multitasking	0.0	0.0	0.0	2.0	0.0000000000	False
multitasking and separating task	0.0	0.0	0.0	2.0	0.0000000000	False
task can other issue	0.0	0.0	0.0	2.0	0.0000000000	False
issue is separating task	0.0	0.0	0.0	2.0	0.0000000000	False
task simplifies your programming	0.0	0.0	0.0	2.0	0.0000000000	False
simplifies your programming complexity	0.0	0.0	0.0	2.0	0.0000000000	False
kernel which would support	0.0	0.0	0.0	2.0	0.0000000000	False
switching of the processor	0.0	0.0	0.0	2.0	0.0000000000	False
processor between different tasks	0.0	0.0	0.0	2.0	0.0000000000	False
concurrency is basically appearance	0.0	0.0	0.0	2.0	0.0000000000	False
appearance of simultaneous execution	0.0	0.0	0.0	2.0	0.0000000000	False
simultaneous execution of multiple	0.0	0.0	0.0	2.0	0.0000000000	False
execution of multiple tasks	0.0	0.0	0.0	2.0	0.0000000000	False
multiple tasks so lets	0.0	0.0	0.0	2.0	0.0000000000	False
concurrency in temperature controller	0.0	0.0	0.0	2.0	0.0000000000	False
supposed to just control	0.0	0.0	0.0	2.0	0.0000000000	False
request to handle concurrency	0.0	0.0	0.0	2.0	0.0000000000	False
monitoring temperature and depending	0.0	0.0	0.0	2.0	0.0000000000	False
depending on the temperature	0.0	0.0	0.0	2.0	0.0000000000	False
day the different temperature	0.0	0.0	0.0	2.0	0.0000000000	False
modification in the setting	0.0	0.0	0.0	2.0	0.0000000000	False
setting from the keypad	0.0	0.0	0.0	2.0	0.0000000000	False
evens that can occur	0.0	0.0	0.0	2.0	0.0000000000	False
concurrent processors or task	0.0	0.0	0.0	2.0	0.0000000000	False
task and being handled	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system also request	0.0	0.0	0.0	2.0	0.0000000000	False
system also request concurrency	0.0	0.0	0.0	2.0	0.0000000000	False
interact to the system	0.0	0.0	0.0	2.0	0.0000000000	False
system in a concurrent	0.0	0.0	0.0	2.0	0.0000000000	False
issue in this contest	0.0	0.0	0.0	2.0	0.0000000000	False
processors from multiple users	0.0	0.0	0.0	2.0	0.0000000000	False
multiple users being run	0.0	0.0	0.0	2.0	0.0000000000	False
designing an embedded system	0.0	0.0	0.0	2.0	0.0000000000	False
size of the cpu	0.0	0.0	0.0	2.0	0.0000000000	False
deadline on project deadlines	0.0	0.0	0.0	2.0	0.0000000000	False
project deadlines but deadlines	0.0	0.0	0.0	2.0	0.0000000000	False
deadlines to be met	0.0	0.0	0.0	2.0	0.0000000000	False
system okay faster hardware	0.0	0.0	0.0	2.0	0.0000000000	False
faster hardware or cleverer	0.0	0.0	0.0	2.0	0.0000000000	False
hardware or cleverer software	0.0	0.0	0.0	2.0	0.0000000000	False
software and in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact there maybe cases	0.0	0.0	0.0	2.0	0.0000000000	False
cases i might write	0.0	0.0	0.0	2.0	0.0000000000	False
write a clever software	0.0	0.0	0.0	2.0	0.0000000000	False
deadline on the cpu	0.0	0.0	0.0	2.0	0.0000000000	False
cpu as incase executed	0.0	0.0	0.0	2.0	0.0000000000	False
executed i might require	0.0	0.0	0.0	2.0	0.0000000000	False
require a first cpu	0.0	0.0	0.0	2.0	0.0000000000	False
cpu but first cpu	0.0	0.0	0.0	2.0	0.0000000000	False
design on an fpga	0.0	0.0	0.0	2.0	0.0000000000	False
fpga at dedicated function	0.0	0.0	0.0	2.0	0.0000000000	False
cpu but that function	0.0	0.0	0.0	2.0	0.0000000000	False
deadline using the software	0.0	0.0	0.0	2.0	0.0000000000	False
design at dedicated logic	0.0	0.0	0.0	2.0	0.0000000000	False
logic on an fpga	0.0	0.0	0.0	2.0	0.0000000000	False
turn of unnecessary logic	0.0	0.0	0.0	2.0	0.0000000000	False
unnecessary logic reduce memory	0.0	0.0	0.0	2.0	0.0000000000	False
logic reduce memory access	0.0	0.0	0.0	2.0	0.0000000000	False
reduce memory access reducing	0.0	0.0	0.0	2.0	0.0000000000	False
memory access reducing memory	0.0	0.0	0.0	2.0	0.0000000000	False
access reducing memory access	0.0	0.0	0.0	2.0	0.0000000000	False
access why each memory	0.0	0.0	0.0	2.0	0.0000000000	False
power when we discuss	0.0	0.0	0.0	2.0	0.0000000000	False
discuss is power management	0.0	0.0	0.0	2.0	0.0000000000	False
important point to deal	0.0	0.0	0.0	2.0	0.0000000000	False
objectives dependability affordability safety	0.0	0.0	0.0	2.0	0.0000000000	False
dependability affordability safety security	0.0	0.0	0.0	2.0	0.0000000000	False
affordability safety security scalability	0.0	0.0	0.0	2.0	0.0000000000	False
safety security scalability timeliness	0.0	0.0	0.0	2.0	0.0000000000	False
timeliness as an issue	0.0	0.0	0.0	2.0	0.0000000000	False
kind of fault tolerance	0.0	0.0	0.0	2.0	0.0000000000	False
tolerance and graceful degradation	0.0	0.0	0.0	2.0	0.0000000000	False
hum to the users	0.0	0.0	0.0	2.0	0.0000000000	False
users i am depending	0.0	0.0	0.0	2.0	0.0000000000	False
kind of a multi	0.0	0.0	0.0	2.0	0.0000000000	False
approach why one aspect	0.0	0.0	0.0	2.0	0.0000000000	False
aspect is electronic hardware	0.0	0.0	0.0	2.0	0.0000000000	False
hardware the other aspect	0.0	0.0	0.0	2.0	0.0000000000	False
aspect is mechanical hardware	0.0	0.0	0.0	2.0	0.0000000000	False
talked about the control	0.0	0.0	0.0	2.0	0.0000000000	False
important the other thing	0.0	0.0	0.0	2.0	0.0000000000	False
institutions the sociological aspect	0.0	0.0	0.0	2.0	0.0000000000	False
sociological aspect about accepting	0.0	0.0	0.0	2.0	0.0000000000	False
product you can make	0.0	0.0	0.0	2.0	0.0000000000	False
people may not accepting	0.0	0.0	0.0	2.0	0.0000000000	False
acceptable depending on norms	0.0	0.0	0.0	2.0	0.0000000000	False
norms of the society	0.0	0.0	0.0	2.0	0.0000000000	False
sociological perspective for introducing	0.0	0.0	0.0	2.0	0.0000000000	False
introducing an a plans	0.0	0.0	0.0	2.0	0.0000000000	False
cycle how the embedded	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system gets developed	0.0	0.0	0.0	2.0	0.0000000000	False
design look into manufacturing	0.0	0.0	0.0	2.0	0.0000000000	False
system and then requirement	0.0	0.0	0.0	2.0	0.0000000000	False
means how to draw	0.0	0.0	0.0	2.0	0.0000000000	False
product because after introducing	0.0	0.0	0.0	2.0	0.0000000000	False
wont support the product	0.0	0.0	0.0	2.0	0.0000000000	False
consumer you are invested	0.0	0.0	0.0	2.0	0.0000000000	False
commitment to that product	0.0	0.0	0.0	2.0	0.0000000000	False
product so the retirement	0.0	0.0	0.0	2.0	0.0000000000	False
plan of the product	0.0	0.0	0.0	2.0	0.0000000000	False
important so the design	0.0	0.0	0.0	2.0	0.0000000000	False
design goal in terms	0.0	0.0	0.0	2.0	0.0000000000	False
performance the overall speed	0.0	0.0	0.0	2.0	0.0000000000	False
functionality and user interface	0.0	0.0	0.0	2.0	0.0000000000	False
user interface manufacturing cost	0.0	0.0	0.0	2.0	0.0000000000	False
interface manufacturing cost power	0.0	0.0	0.0	2.0	0.0000000000	False
manufacturing cost power consumption	0.0	0.0	0.0	2.0	0.0000000000	False
power consumption physical size	0.0	0.0	0.0	2.0	0.0000000000	False
give you the performance	0.0	0.0	0.0	2.0	0.0000000000	False
performance as a criteria	0.0	0.0	0.0	2.0	0.0000000000	False
satisfied otherwise your product	0.0	0.0	0.0	2.0	0.0000000000	False
acceptability in the market	0.0	0.0	0.0	2.0	0.0000000000	False
kbs nobody will buy	0.0	0.0	0.0	2.0	0.0000000000	False
output as a function	0.0	0.0	0.0	2.0	0.0000000000	False
non-functional requirements non-functional requirements	0.0	0.0	0.0	2.0	0.0000000000	False
size power consumption reliability	0.0	0.0	0.0	2.0	0.0000000000	False
power consumption reliability etcetera	0.0	0.0	0.0	2.0	0.0000000000	False
part of your design	0.0	0.0	0.0	2.0	0.0000000000	False
design goal and design	0.0	0.0	0.0	2.0	0.0000000000	False
goal and design objective	0.0	0.0	0.0	2.0	0.0000000000	False
non-functional requirement at times	0.0	0.0	0.0	2.0	0.0000000000	False
appliances are a embedded	0.0	0.0	0.0	2.0	0.0000000000	False
system so this design	0.0	0.0	0.0	2.0	0.0000000000	False
architecture that is design	0.0	0.0	0.0	2.0	0.0000000000	False
architecture is a block	0.0	0.0	0.0	2.0	0.0000000000	False
shown is basically testing	0.0	0.0	0.0	2.0	0.0000000000	False
testing face because testing	0.0	0.0	0.0	2.0	0.0000000000	False
rectify that um bug	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system that flexibility	0.0	0.0	0.0	2.0	0.0000000000	False
product to the user	0.0	0.0	0.0	2.0	0.0000000000	False
connect to the internet	0.0	0.0	0.0	2.0	0.0000000000	False
patch so these systems	0.0	0.0	0.0	2.0	0.0000000000	False
tested and the bug	0.0	0.0	0.0	2.0	0.0000000000	False
software faults the design	0.0	0.0	0.0	2.0	0.0000000000	False
faults the design approaches	0.0	0.0	0.0	2.0	0.0000000000	False
top down or bottom	0.0	0.0	0.0	2.0	0.0000000000	False
abstract description and work	0.0	0.0	0.0	2.0	0.0000000000	False
detailed level the bottom	0.0	0.0	0.0	2.0	0.0000000000	False
terms of embedded system	0.0	0.0	0.0	2.0	0.0000000000	False
system design um strategies	0.0	0.0	0.0	2.0	0.0000000000	False
work from small component	0.0	0.0	0.0	2.0	0.0000000000	False
component to big system	0.0	0.0	0.0	2.0	0.0000000000	False
cases when some bodies	0.0	0.0	0.0	2.0	0.0000000000	False
bodies developing a product	0.0	0.0	0.0	2.0	0.0000000000	False
component from previous system	0.0	0.0	0.0	2.0	0.0000000000	False
system because its availability	0.0	0.0	0.0	2.0	0.0000000000	False
process and in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact any real design	0.0	0.0	0.0	2.0	0.0000000000	False
real design actually involves	0.0	0.0	0.0	2.0	0.0000000000	False
talking about software development	0.0	0.0	0.0	2.0	0.0000000000	False
talking about both hardware	0.0	0.0	0.0	2.0	0.0000000000	False
development and you realize	0.0	0.0	0.0	2.0	0.0000000000	False
hardware and software development	0.0	0.0	0.0	2.0	0.0000000000	False
embedded system go handing	0.0	0.0	0.0	2.0	0.0000000000	False
system go handing hand	0.0	0.0	0.0	2.0	0.0000000000	False
design a special purpose	0.0	0.0	0.0	2.0	0.0000000000	False
hardware and in fact	0.0	0.0	0.0	2.0	0.0000000000	False
hardware um who designing	0.0	0.0	0.0	2.0	0.0000000000	False
designing software hardware partitioning	0.0	0.0	0.0	2.0	0.0000000000	False
partitioning and those approaches	0.0	0.0	0.0	2.0	0.0000000000	False
approaches so the step	0.0	0.0	0.0	2.0	0.0000000000	False
talking about is stepwise	0.0	0.0	0.0	2.0	0.0000000000	False
refinement of both hardware	0.0	0.0	0.0	2.0	0.0000000000	False
sort of the software	0.0	0.0	0.0	2.0	0.0000000000	False
requirement of the system	0.0	0.0	0.0	2.0	0.0000000000	False
system as a hole	0.0	0.0	0.0	2.0	0.0000000000	False
abroad overview um introductory	0.0	0.0	0.0	2.0	0.0000000000	False
overview um introductory overview	0.0	0.0	0.0	2.0	0.0000000000	False
appliances which are embedded	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems in fact	0.0	0.0	0.0	2.0	0.0000000000	False
fact some body made	0.0	0.0	0.0	2.0	0.0000000000	False
body made this statement	0.0	0.0	0.0	2.0	0.0000000000	False
viewer microwave your cell	0.0	0.0	0.0	2.0	0.0000000000	False
microwave your cell phone	0.0	0.0	0.0	2.0	0.0000000000	False
cell phone a viewer	0.0	0.0	0.0	2.0	0.0000000000	False
embedded computers and embedded	0.0	0.0	0.0	2.0	0.0000000000	False
computers and embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
move how to design	0.0	0.0	0.0	2.0	0.0000000000	False
issue and therefore embedded	0.0	0.0	0.0	2.0	0.0000000000	False
system goes mainly design	0.0	0.0	0.0	2.0	0.0000000000	False
challenges design time deadlines	0.0	0.0	0.0	2.0	0.0000000000	False
realize that embedded systems	0.0	0.0	0.0	2.0	0.0000000000	False
form a general purpose	0.0	0.0	0.0	2.0	0.0000000000	False
methodology and the principles	0.0	0.0	0.0	2.0	0.0000000000	False
embedded systems are expected	0.0	0.0	0.0	2.0	0.0000000000	False
methodologies help us mange	0.0	0.0	0.0	2.0	0.0000000000	False
mange the design process	0.0	0.0	0.0	2.0	0.0000000000	False
questions if you don	0.0	0.0	0.0	2.0	0.0000000000	False
class we shall start	0.0	0.0	0.0	2.0	0.0000000000	False
discussion on embedded hardware	0.0	0.0	0.0	2.0	0.0000000000	False
