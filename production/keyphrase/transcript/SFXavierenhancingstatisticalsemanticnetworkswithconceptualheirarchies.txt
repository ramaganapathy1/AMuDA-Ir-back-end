Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/282787968

EnhancingStatisticalSemanticNetworkswith ConceptHierarchies
ConferencePaper·August2015
DOI:10.1109/ICACCI.2015.7275792

READS

48

3authors,including: VidhyaBalasubramanian AmritaVishwaVidyapeetham
20PUBLICATIONS100CITATIONS
SEEPROFILE

Allin-textreferencesunderlinedinbluearelinkedtopublicationsonResearchGate, lettingyouaccessandreadthemimmediately.

Availablefrom:VidhyaBalasubramanian Retrievedon:27May2016

Enhancing Statistical Semantic Networks with Concept Hierarchies
Sofia Francis Xavier, Lakshmi Priyanka Selvaraj and Vidhya Balasubramanian Department of Computer Science and Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham(University).

Abstract--With the emergence of the semantic web, effective knowledge representation has gained importance. Statistically generated semantic networks are simple representations whose semantic power is yet to be completely explored. Though, these semantic networks are created with simple statistical measures without much overhead, they have the potential to express the semantic relationship between concepts. In this paper, we explore the capability of such networks and enhance them with concept hierarchies to serve as better knowledge representations. The concept hierarchies are built based on the level of importance of concepts. The level of importance/coverage of a concept within the given set of documents has to be taken into account to build an effective knowledge representation. In this paper, we provide a domain-independent, graph based approach for identifying the level of importance of each concept from the statistically generated semantic network which represents the entire document set. Insights about the depth of every concept is obtained by analysing the graph theoretical properties of the statistically generated semantic network. A generic concept hierarchy is created using a greedy strategy, and the original semantic network is reinforced with this concept hierarchy. Experiments over different data sets demonstrate that our approach works effectively in classifying concepts and generating taxonomies based on it, thereby effectively enhancing the semantic network.

I. I NTRODUCTION Representation of knowledge in the form of concepts and their inter-relationships is important for various applications like automated processing of information content, information retrieval and document summarization. Constant efforts have been made to improve the efficiency of knowledge representations. Information content, in most domains is vast, and is constantly changing i.e, new concepts and relationships between concepts are continuously being updated. Therefore generating effective knowledge representation is a challenging process. Currently, knowledge representations use simple bagof-words or statistical models like vector space models [21] or graph based models like ontologies [12]. While the former models use statistics to identify important concepts, they do not explicitly define the relationships between them. The latter on the other hand use graph representations to explicitly relate concepts in a domain. Such graph based representations can range from simple semantic networks that use word co-occurrence to correlate terms to complex concept graph representations like ontologies. While ontologies are the most expressive representations, and hence desirable, they are very complex to generate, requires domain knowledge and involve lot of manual intervention for generation.

Since graphs are naturally suitable to represent knowledge, it would be desirable to extract more semantic information from simple semantic networks to make them more expressive. Therefore, our work aims to enhance the basic semantic networks generated using co-occurrence and word distance metrics [29][31], by extracting and superimposing concept hierarchies over such representations. The advantage of the such graphs is that they are easy to generate and do not require prior knowledge about the domain or Natural Language Processing(NLP) to establish relationships. Our work builds on the premise that the graphs generated using co-occurrence and word distance (distance between concepts in a document) naturally capture interesting patterns reflecting different semantic aspects of the domain like concept hierarchy, topic based clusters etc. In order to achieve the above firstly, we generate a semantic network for a corpus of documents representing a single domain or multiple domains. In our work, concept identification is not the focus, and hence existing techniques are used to extract the key concepts from the corpus. To enhance the representation we address two major aspects: 1. While key concepts of a domain may be available, the level of importance of a concept to a domain, and the level of coverage in a domain is rarely captured. This information is essential to categorize concepts into topics, subtopics, and supporting concepts. Each of these can further be categorized into multiple levels. In this work we extract the level of importance/coverage of a concept with respect to a domain, using the properties of the initially generated semantic network. 2. Once the level of a concept is identified, a formal concept hierarchy in the form of a tree structure is generated, which connects the right concepts across levels. This step also uses the properties of the original semantic network along with the extracted concept levels. This enhanced semantic network can help in different information retrieval problems like knowledge representation, taxonomy extraction, finding document similarity etc. We will motivate the need for this work in the next section, and explain our methodology in the consequent sections. II. R ELATED WORK Knowledge Representation(KR) focuses on extracting and representing knowledge from the given documents in a meaningful way, to enable intelligent systems to perform better. The available methods for representing knowledge include statistical approaches, graph theoretic approaches, semantic approaches and a hybrid of these approaches. The statistical

978-1-4799-8792-4/15/$31.00 c 2015 IEEE

1298

approaches to KR include bag of words model, vector space model and scalar value decomposition [21]. Although, these techniques capture the important concepts discussed in the given text, they do not capture the exact nature of relationships between them. Currently the most commonly used KRs are graph based [3], the most common of these being ontologies [11]. Graph based techniques are capable of expressing relationships between concepts and this power has been used for various applications. Such graph representations range from semantic networks generated using statistical approaches, [31][25][15] which implicitly relate concepts to concept graphs, which have both concept nodes and relationship nodes [9][27]. Concept graphs like ontologies have been widely used for information retrieval, topic labelling, recommendation systems etc [16][17]. There are various automatic [32] and semi automatic methods [13] for ontology construction. For building ontologies accurately, domain expertise is mandatory, and extensive NLP techniques are needed. Though ontologies are comprehensive, ontologies are domain specific and the effort required to construct ontologies for a broader domain is enormous and highly complex. The other type of graph based KRs include semantic networks, which have also been widely used to model semantic knowledge [31][30]. These networks represent concepts as nodes, but the relationships between them are not explicitly defined. Semantic networks that are statistically generated represent relationships using statistical weights, and are also commonly used [31]. Statistical measures such as word-distance, absolute frequency, relative frequency, and cooccurrence have been widely used to create semantic networks [29][31][8]. These types of networks have been used in a wide range of information retrieval applications [28][23], and keyphrase extraction systems [19]. Amongst these statistical measures, it has been observed that the networks created with co-occurrence [23][25] and word-distance [15][8] are capable of quantifying the association between concepts to an appreciable extent [2]. Though these are simple statistical measures, they are able to express the relationship between concepts with considerable accuracy. However they suffer from the lack of expressivity and there is scope for enhancing these representations to improve their semantic expressivity. For any knowledge representation, concept identification is an important task. Concept identification has been widely studied and there are several approaches for the same including statistical [22], machine learning [6][4] and graph theoretic approaches [20]. However most of these approaches do not classify these concepts based on their dominance or coverage within a corpus. The classification of the concepts based on their level of coverage/importance in the given documents can greatly influence the effectiveness of KR. In [14], phrases have been classified into two types i.e. topic and theme phrases based on their frequency of occurrence. However more work is needed to classify the level of importance of a term to a document, and in this paper we classify concepts based on the level of importance using both statistical and graph theoretic techniques. To improve the semantic expressivity of the statistical se-

mantic networks, it is essential to augment them with the concept hierarchy established by classifying the concepts as mentioned above. Concept hierarchies have generally been generated in the form of taxonomies which relate terms using is-A, type-of, kind-of, part-of, hypernymy-hyponymy hierarchies [33][26]. These taxonomies restrict the connections to certain relationships, and are not general in nature. For instance, while a 'stack' is-A 'data structure', 'push' and 'pop' which are in the next level of hierarchy do not exhibit this relationship. It is therefore desirable to connect concepts at different levels even if they do not fall under a specific category of relationship. This can help in identifying topic-subtopic relationships, and the relationships between major topics and background topics. Once a generic hierarchy is generated, the exact nature of relationships can be found more easily. Our goal is to create a generic hierarchy of concepts within a subject domain. Finding the exact relationships is beyond scope of this work. The next section will outline our semantic network generation. III. S TATISTICAL S EMANTIC N ETWORK : C ONSTRUCTION

As mentioned in the previous section, our goal is to represent knowledge using statistical semantic networks and analyze the semantic properties of the same. In this section, we discuss the construction of this graph and analyze its basic graph theoretic properties. A semantic network is a graph structure that provides intuitive and useful representations for modelling semantic knowledge [31]. In our work, we aim to construct a semantic network using statistical approaches like word-distance and co-occurrence. This semantic network is constructed from a corpus of documents representing a topic of interest. Here a topic can be a very broad topic like computer science or some specific topic like binary trees. The corpus is assumed to represent all aspects of the topic. Given this corpus, a statistical network is defined as a graph of concepts within this topic, where nodes are concepts, and the edges represent a mapping between the concepts representing the relationship between the concepts. This network is a weighted graph where each edge has a numeric value associated with it indicating the strength of the relationship between the incident concepts. In statistical semantic networks, the strength of a relationship between the concepts is statistically derived. Measures like co-occurrence, word distance, n-distance, absolute frequency and relative frequency have been used to characterize this relationship, the most common of them being cooccurrence and word distance. For instance, two concepts are assumed to be closely related if they co-occur together in a document [23]. If the number of documents they cooccur in is higher, the strength is proportionally higher, and the correspondingly generated graphs become co-occurrence graphs. The number of terms separating two concepts in a document is word-distance. The closer the two terms, the stronger the relationship between them. Such graphs are worddistance graphs. There has been lot of attempts at using both these types of graphs for information retrieval tasks like keyword extraction, text summarization etc. From the related

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1299

work, we can see that these two measures are simple to compute, and intuitively connect conceptually similar terms. Hence we use them both as a basis for our semantic network. In most works, both these measures are combined as follows, the strength is defined based on the number of times two terms co-occur within a specific window size (in word distance). But many a time, this method misses important co-occurrences or includes spurious ones. Therefore the edge weight should factor in the frequency of co-occurrence and word distance of the incident nodes (terms) irrespective of the window size. The next paragraphs will elaborate the steps involved in our technique for graph construction that combine these metrics. The input for the graph is a set of technical documents of a particular topic. As mentioned earlier, the topic can be as broad as 'Computer science' or as narrow as 'Binary search tree'. The first step in the construction of statistical semantic network is to extract the key-phrases from the given technical documents. N-gram keyphrases have been extracted with the keyphrase extraction method discussed in [4]. These keyphrases are taken as the concepts and they form the nodes of the graph. The edges are constructed with the help of co-occurrence and worddistance metrics. We first apply the word distance metric, where the distance between these two concepts is given by the average of the number of words between the two concepts, over all the occurrences present in the documents. Next we apply co-occurrence over this. Co-occurrence is used to eliminate spurious or weak relationships. For instance, two terms might be closely occurring in a document but may not be conceptually related, for e.g. words in the index of a book. To account for this possibility, we calculate the number of times they co-occur within a document and across the corpus. This co-occurrence value is combined with the worddistance measure. Hence two terms are closely related if they co-occur frequently and have low word distance. The final edge weight is given by a normalized value of both the word distance and the co-occurrence values. Let G=(V, E ) be the graph generated over the document collection D, where V =(v1 , v2 ,..., vn ) represents the vertices and E is the set of edges. e(i, j ) represents the edge connecting concepts (nodes) vi and vj . Let w(i, j ) represent the word distance between concepts vi and vj and c(i, j ) be the number of times concepts vi and vj co-occur within the corpus. Let h(i, j ) be the heuristic i.e., the weight assigned to each edge e(i, j ) considering both word distance and co-occurrence measures as given by the following equation. Two related concepts have high co-occurrence value and a low worddistance value. There is a necessity to balance the influence of both these metrics so as to obtain the final relationship measure h(i, j ). This is derived as follows. h(i, j ) = w(i, j ) × (cm - c(i, j )) (cm ) (1)

TABLE I: Comparison on Word distance, Co-occurrence and Heuristic measures Vertex 1 Complete binary tree Self balancing tree Array Vertex 2 Height Worst case Pointer Word distance 60 60 60 CoHeuristic occurrence value 4 5.868 2 1 5.934 5.967

though they have the same word distance. This completely depends on the data-set under consideration. While all three pairs have semantic relationships, the pair with least heuristic value exhibits the closest relationship. The graph G=(V, E ) so obtained using the heuristic values H is our semantic network. Fig.1 shows a sample semantic network generated over three topics, which has been visualized using the Gephi tool [5]. As we can see from the graph, such semantic networks generated by using word-distance and co-occurrence metrics have interesting semantic properties. Also, the nodes under a similar topic were found to cluster together. The following section elaborates how a concept hierarchy is generated using this statistical semantic network. IV. C ONCEPT HIERARCHY GENERATION USING THE
PROPERTIES OF THE SEMANTIC NETWORK

We have so far explained in detail, how the statistical semantic network is generated for a given set of documents. The concept hierarchy generation involves two steps 1) Concept classification 2) Creating concept hierarchy A. Concept classification Concept classification involves identifying the level of importance of each concept. The level of importance of a concept determines how influential or dominant it is in the given set of documents. Not all concepts are equally important. Consider a document about 'Stacks'. The main topic/concept is the data structure 'stack', concepts such as 'push', 'pop' are its subtopics and then concepts like 'stack empty exception', which occur sparingly can be placed under 'pop'. We classify concepts into the following classes based on the level of discussion and level of importance to the domain. 1) Topics: These contain the concepts that outline the major topic of discussion. 2) Sub topics: For each major topic discussed, the subtopics that are covered are categorized here. Subtopics can be further classified into multiple levels based on their level of discussion. 3) Background topics: These topics outline the allied subjects that can help in understanding of the main topics and subtopics. However the document focus is not these topics.

where cm is the maximum value of the co-occurrence measure over all the edges in the graph. The lower the heuristic value the higher the strength of the relationship. The Table I shows an example corpus where the heuristic value of the edges change depending on the co-occurrence values even

1300

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

Fig. 1.

Statistical semantic network

To classify the concepts, it is necessary to understand the properties of the concepts under consideration and for this we analyse the properties of the generated graph. Since our goal in this work is to classify concepts based on their importance and level of contribution to the domain or corpus, we primarily focus on the graph based measures that provide insights into the importance of concepts. To obtain a rough idea of the role played by each concept, we use centrality measures as our basic features [19]. Specifically we choose the nine centrality measures that are defined in [19]. The measures are · Degree: is the number of edges that are connected to the node. · Closeness Centrality: is the reciprocal of the sum of the distances from the given node to all the other nodes in the graph. · Betweenness Centrality: measures how often the node gets traversed in the shortest path between two other nodes. · Eigenvector Centrality: assesses how well an individual node is connected to other important nodes of the network. · Strength: is sum of the weights of all the edges that are connected to the node. · HITS: A popular ranking algorithm by Kleinberg [18] gives rise to two measures namely, 1) Authority: measures authoritativeness of the node's content on a specific topic.

Hub: measures how well a node is connected to many authoritative nodes. · PageRank: measures how often a user performing a random walk from one node to another in a network would visit that node. · Clustering Coefficient: indicates how the node is embedded in its neighborhood. It has been observed that in general the concepts in the topic cluster have high values for all the centrality measures but this need not be the case always. For example, consider the two topic nodes 'binary tree' and 'stack'. 'Binary tree' has a high value for all the centrality measures whereas 'stack' has high value for degree, betweenness, authority, hub and pagerank but has low value for strength, clustering coefficient, closeness and eigenvector centrality. Similarly the subtopic 'load factor' has a high strength, betweenness and clustering coefficient but low degree, authority, hub, pagerank, eigenvector and closeness centrality measures. Considering just one or two of these centrality measures will result in spurious identification of the topics. Hence there is a necessity to consider all the centrality measures to identify the topics, subtopics and the background topics accurately. Therefore, we first cluster the nodes using these centrality measures and then label the clusters as topics, subtopics and background topics using a filtering approach. For concept clustering, the most suitable algorithms are K-means and Expectation Maximization(EM), since both the algorithms are found to work well for large domains [1]. 2)

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1301

Expectation Maximization(EM) is a distance-based algorithm that uses statistical models with latent variables and computes the distribution parameters which maximizes a model's loglikelihood measure [1]. K-means clustering algorithm is a simple partitioning algorithm which partitions m nodes into k clusters where each node belongs to the cluster with the nearest centroid [1]. In order to analyse which clustering algorithm produces better results, both K-means and EM are applied to the nodes in our statistical semantic network. Taking into account the level of coverage of topics, we have considered 4 classes for classification, i.e. two classes for subtopics, one for topics and the other for background topics. Each concept is represented by its feature vectors which is a measure of all the nine centrality metrics. The clustering algorithm is applied to the feature vectors using the Weka tool [7] and the clusters are obtained. Once the concepts are grouped into four clusters, we need to identify which cluster represents topics, subtopics and background topics. The nodes with a high degree tend to be topics, and those with lower degrees are generally background topics. Hence by sorting the clusters based on the average degree of each cluster we will be able to identify the topic, subtopic and the background topic clusters. The clusters are then named from level 0 to level 3, with level 0 being the topic cluster with the highest average degree and level 3 being the background topic cluster with the lowest average degree. B. Creating Concept Hierarchy Once the different levels of importance are identified, a greedy hierarchy generation algorithm is applied over the semantic network and the identified clusters. Let l l l V l =(v1 , v2 ,..., vp ) be the list of vertices at each level l and N be the list of edges corresponding to the concept hierarchy generated. L is the number of levels of importance (level 0 corresponds to major topics and level L - 1 corresponds to the background topic). Then  v l  V l there exists an edge n(x, y ) l l -1 connecting the nodes vx from level l and vy from level l - 1 l l -1 such that the distance between vx and vy is minimum in the network. The algorithm to create the concept hierarchy can be defined formally as given below. Algorithm 1 Greedy Hierarchy Generation Algorithm l1 for l <number of levels of importance do for all vx V l do temp  N one minimumDistance   for all vy V l-1 do if shortestP ath(vx , vy ) < minimumDistance then minimumDistance  shortestP ath(vx , vy ) tempvy end if end for n(vx , temp)  minimumDistance end for end for

The concept hierarchies obtained using the above mentioned algorithm consist of different connected components, with each component representing a hierarchy of the major topics of discussion. In the subsequent sections we will evaluate both the concept classification and the hierarchy generation algorithms. C. Evaluation of concept classification and concept hierarchies Now that we have classified concepts and generated their hierarchies, we need to asses the algorithms' performance. There are two methods of assessing the classification, the first of which, is to evaluate it based on a ground truth. Ground truth can be obtained from Wordnet [24], or generated manually. Wordnet provides a shallow hierarchy of the concepts in a technical domain, and so most of the nodes present in the semantic network are not present in Wordnet. Hence ground truth has to be generated manually, which is a difficult task for large datasets. Hence for this type of evaluation we use expert generated classifications for small datasets on topics like 'stacks', 'binary trees' etc. The ground truth consists of four clusters with topics, subtopics, sub-subtopics and background topics represented as G0 , G1 , G2 and G3 respectively. Similarly, the clusters obtained using our algorithm over the three data-sets are represented as M0 , M1 , M2 and M3 . Given the groundtruth, we use the following metrics of evaluation. 1) Precision, Recall and F-measure: Precision is defined as the measure of the number of concepts that have been correctly identified in each cluster out of the total number of concepts present in the cluster. Using the ground truth the precision can be mathematically calculated using the equation Pi = n ( G i  Mi ) n ( Mi ) (2)

where Pi is the precision obtained for cluster i, n(Gi  Mi ) is the number of concepts of Gi in Mi and n(Mi ) is the number of concepts in Mi . Recall is a measure of the number of concepts that have been correctly identified in each cluster out of the number of concepts present in the ground truth and is mathematically calculated as follows Ri = n( G i  Mi ) n( Gi ) (3)

where Ri is the recall obtained for cluster i, n(Gi ) is the number of concepts in Gi . F-measure is the harmonic mean of precision and recall and can be calculated as 2 × Pi × Ri (4) Fi = Pi + R i where Fi is the F-measure for cluster i. To calculate the overall performance of the classification we take the weighted average of the F-measures calculated for each cluster and can be given by n( Gi ) × Fi F = (5) n i

1302

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

TABLE II: Precision values for the data subsets Data-set Stack Binary Tree Hashing K-means 0.6572 0.5666 0.4755 Precision EM 0.7656 0.6067 0.7040

TABLE III: Recall values for the data subsets Data-set Stack Binary Tree Hashing K-means 0.5666 0.5263 0.4431 Recall EM 0.6923 0.5714 0.5735

where n is the total number of concepts in the ground truth. Similarly the overall precision and recall of the classification is calculated by taking the weighted average of their corresponding measures. Since generation of ground truth is difficult in many cases, user rating is often a commonly chosen method. User evaluation of the concept hierarchies based on five parameters are used to evaluate the concept hierarchies. The five parameters as defined in [10] are 1) Cohesiveness: Judge whether the concepts in the hierarchy are semantically similar. 2) Isolation: Judge whether the concepts at the same level in the hierarchy are distinguishable and do not subsume one another. 3) Hierarchy: Judge whether the hierarchies are traversed from broader concepts to narrower concepts. 4) Navigation Balance: Judge whether the concepts in the hierarchy fan-out appropriately at each level. 5) Readability: Judge whether it is easy to locate the concepts of hierarchies at all levels with the composed hierarchies and instances. The concept hierarchies that have been generated using our greedy hierarchy generation algorithm over the three datasets are given to a set of 5 users. The users rate the concept hierarchies generated on a scale of 0 to 5 where 0 means very poor. Given these two evaluation techniques, the former will be applied for concept classification, and the latter for the hierarchy generated. The next subsections deal with the performance of our techniques. 2) Evaluation of Proposed Concept Classification: To analyse the performance of the concept classification, we have used three data-sets on stacks, binary tree and hashing which is TABLE IV: F-measure values for the data subsets Data-set Stack Binary Tree Hashing K-means 0.5747 0.5284 0.4346 F-measure EM 0.6990 0.5702 0.5998

Fig. 2.

Concept hierarchy for Stacks generated using K-means

TABLE V: User evaluation of concept hierarchies for 'stacks' Parameters Cohesion Isolation Hierarchy Navigation Balance Readability K-Means 3.6398 3.7225 3.5 3.7225 3.7225 EM 4.167 3.75 4.167 4.583 4.33

a subset of the overall data-set that is based on data structures. Each data-set consists of approximately 8 documents. Using these data-sets, classification of the concepts have been performed using the proposed algorithm. Metrics such as Fmeasure, precision and recall were then calculated. Since such a classification has not been attempted to the best of our knowledge, we do not have previous techniques to compare with. Hence we evaluate the two classification algorithms, EM and K-Means to see which performs better. The precision, recall and F-measures for the data subsets, as stated before, using the two classification algorithms EM and K-Means can be seen in Tables II, III and IV. The results demonstrate that our method of using graph properties for classification results in good accuracies using both techniques for different topics. Amongst these two techniques (both work well in text classification), the EM classifier performs better than the KMeans classifier consistently. Since EM classification gives more importance to the probability of concepts occurring together, in such a concept classification scheme, it works better. 3) Evaluation of Proposed Concept Hierarchy Generation: Once the concepts are classified, the concept hierarchy is generated using our greedy approach. A sample hierarchy has been generated using both the K-means and EM clustering algorithms over the 'stack' data-set and can be seen in Fig.2 and Fig.3. As observed from the figures we can see the levels of hierarchy and how the concepts like push and pop are related to stack, and stack full exception

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1303

TABLE VI: User evaluation of concept hierarchies for 'hash tables' Parameters Cohesion Isolation Hierarchy Navigation Balance Readability K-Means 3.33 4 3.334 3.5 3.417 EM 4 4.33 4 4.167 4.167

The enhancement is done by enhancing properties of both the nodes and edges of the semantic network. Node enhancement: The nodes in the semantic network are weighted based on its level of importance in the concept classification. The concepts with the highest level of importance i.e. the topics are assigned with a weight of 1. The nontopic concepts are assigned with weights based on the level it occupies in the hierarchy. The weighting of the nodes can be mathematically represented as L - li (6) L where ui is the assigned weight of the vertex vi in the semantic network G = (V, E ), L is the number of levels of importance considered in concept classification, and li is the level occupied by vertex vi in the hierarchy. Here 'Level 0' represents the topics. The ui value assigned for concepts in different levels of importance are shown in Table VIII. ui = TABLE VIII: ui values for four levels of hierarchy

TABLE VII: User evaluation of concept hierarchies for 'binary trees' Parameters Cohesion Isolation Hierarchy Navigation Balance Readability K-Means 3.833 4 4.0833 4.166 3.8333 EM 4.5 4.1666 4.5 4.333 4.5

For the evaluation of the generated hierarchies the latter method of user ratings is used. The hierarchies are generated over results from both classifiers and given to 5 users for rating on different aspects. The average rating has been tabulated and shown in Tables V,VI and VII. The results show that our approach generates accurate hierarchies, and are acceptable to users. On average, users have given a rating of about 4.0 for the hierarchies generated from different datasets. Here too, the EM classification results in better hierarchies and is preferred by the users. We can see that the dataset binary tree has higher accuracy. This is due to the variety of documents available covering different aspects of binary trees. V. E NHANCEMENT OF THE S EMANTIC N ETWORK

Levels of importance Topics Subtopics Sub-subtopics Background topics

ui values assigned 1 0.75 0.50 0.25

The semantic network that has been generated from the previous sections is enhanced using the concept hierarchies.

Edge enhancement: Every edge in the semantic network is enhanced by a factor which depends on the type of link it has in the concept hierarchy. An edge in the semantic network falls under one of the following three cases 1) For an edge in the semantic network, the incident vertices belong to the same component in the hierarchy and have a direct hierarchical edge. 2) For an edge in the semantic network, the incident vertices belong to the same component in the hierarchy but do not have a direct hierarchical edge. 3) For an edge in the semantic network, the incident vertices belong to different components in the concept hierarchy. For the first two cases the edge enhancement is done by reducing the edge weight by a factor . The factor  is given by the fraction of the path length between the two vertices in the concept hierarchy to the maximum path length between any two concepts in the hierarchy.  can be mathematically defined as p(i, j ) = (7) pm where p(i, j ) is the path length between the vertices vi and vj in the concept hierarchy and pm is the maximum path length between any two vertices in the concept hierarchy. pm given by 2 × (L - 1), where L is the number of levels of importance considered. In the third case the edge weight remains unaltered. Hence  is taken to be 1 in this case. The enhancement of the edge weight for three sample edges is shown in Table IX. The edge weight of each edge in the semantic network is then enhanced as h(i, j ) =  × h(i, j ) (8)

Fig. 3.

Concept hierarchy for Stacks generated using EM

1304

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

REFERENCES

Fig. 4.

Enhanced semantic network

where h(i,j) is the heuristic edge weight between the vertices vi and vj . In addition these edges are also specially labelled as hierarchical edges. TABLE IX: Enhanced edge weights for different types of edges in hierarchy Vertex 1 Stack Stack Stack Vertex 2 Pop Linked stack Array based stack Edge weight 6.8067 10.8658 1.9512  Value 0.1667 0.3333 0.5 Enhanced edge weight 1.1344 3.6219 0.6504

The semantic network thus enhanced is shown in the Fig.4. This graph consists of concepts from 'stacks', 'binary trees' and 'hash tables'. We can observe that the clusters are more pronounced and the important nodes are enhanced, as are the edges, as compared to the original graph in Figure 1. The hierarchies are also clearly visible within each cluster. VI. C ONCLUSION

the semantic network based on their level of importance. Our novel approach of using the graph theoretic properties of these semantic networks as features in an EM Clustering algorithm is effective in classifying concepts. We have demonstrated how this approach can effectively classify concepts as topics, subtopics and background topics. To enhance the expressivity of these semantic networks, a hierarchy created over the classified concepts is superimposed over it. The enhanced semantic network consists of weighted nodes indicating the importance of the node in the semantic network. The edge weight between two concepts that are more semantically related have been further reduced thereby bringing the two concepts more closer in the network. This enhanced semantic network serves as a better knowledge representation when compared to the initially generated semantic network and hence can be used for efficient information retrieval. Since the concept hierarchies have been generated without any prior knowledge about the importance of the concept in the domain, it can be used to identify the thematic structure in a constantly changing document collection. Future work involves deeper analysis into the properties of these graphs for information retrieval tasks. R EFERENCES Osama Abu Abbas et al. "Comparisons Between Data Clustering Algorithms." In: Int. Arab J. Inf. Technol. 5.3 (2008), pp. 320­325.

In this paper, we proposed an approach to generate semantic networks using statistical properties such as co-occurrence and word distance. We have observed that these semantic networks have interesting properties which can be exploited for different purposes. We use these properties to classify the concepts of

[1]

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1305

REFERENCES

[2] [3]

[4]

[5]

[6]

[7] [8]

[9] [10]

[11] [12]

[13]

[14]

[15]

[16]

[17]

Asad et al. "Automated generation of concept graphs". In: (2013). Antonio Badia and Mehmed Kantardzic. "Graph building as a mining activity: finding links in the small". In: Proceedings of the 3rd international workshop on Link discovery. ACM. 2005, pp. 17­24. Arun Balagopalan et al. "Automatic keyphrase extraction and segmentation of video lectures". In: Technology Enhanced Education (ICTEE), 2012 IEEE International Conference on. IEEE. 2012, pp. 1­10. Mathieu Bastian, Sebastien Heymann, Mathieu Jacomy, et al. "Gephi: an open source software for exploring and manipulating networks." In: ICWSM 8 (2009), pp. 361­ 362. Vishwanath Bijalwan et al. "KNN based Machine Learning Approach for Text and Document Mining". In: International Journal of Database Theory and Application 7.1 (2014), pp. 61­70. Remco R Bouckaert et al. "WEKA--Experiences with a Java Open-Source Project". In: The Journal of Machine Learning Research 11 (2010), pp. 2533­2541. Ramon Ferrer i Cancho and Richard V Sol´ e. "The small world of human language". In: Proceedings of the Royal Society of London B: Biological Sciences 268.1482 (2001), pp. 2261­2265. Michel Chein and Marie-Laure Mugnier. "Conceptual graphs are also graphs". In: Graph-Based Representation and Reasoning. Springer, 2014, pp. 1­18. Shui-Lung Chuang and Lee-Feng Chien. "Taxonomy generation for text segments: A practical web-based approach". In: ACM Transactions on Information Systems (TOIS) 23.4 (2005), pp. 363­396. Li Ding et al. "Using ontologies in the semantic web: A survey". In: Ontologies. Springer, 2007, pp. 79­113. David Faure and Claire N´ edellec. "A corpus-based conceptual clustering method for verb frames and ontology acquisition". In: LREC workshop on adapting lexical and corpus resources to sublanguages and applications. Vol. 707. 728. 1998, p. 30. Blaz Fortun and Mladeni Dunja. "Semi-automatic ontology construction". PhD thesis. Doctoral Dissertation, Polavtomatska gradnja ontologij: doktorska disertacija.(Ljubljana, Slovenia, 2011. Alexander Haubold. "Analysis and visualization of index words from audio transcripts of instructional videos". In: Multimedia Software Engineering, 2004. Proceedings. IEEE Sixth International Symposium on. IEEE. 2004, pp. 570­573. David Hawking, Paul Thistlewaite, et al. "Relevance weighting using distance between term occurrences". In: Computer Science Technical Report TR-CS-96-08, Australian National University (1996). Ioana Hulpus et al. "Unsupervised graph-based topic labelling using dbpedia". In: Proceedings of the sixth ACM international conference on Web search and data mining. ACM. 2013, pp. 465­474. Wei Jin and Rohini K Srihari. "Graph-based text representation and knowledge discovery". In: Proceedings of

[18] [19]

[20]

[21] [22]

[23]

[24] [25]

[26]

[27]

[28] [29] [30] [31]

[32]

the 2007 ACM symposium on Applied computing. ACM. 2007, pp. 807­811. Jon M Kleinberg. "Authoritative sources in a hyperlinked environment". In: Journal of the ACM (JACM) 46.5 (1999), pp. 604­632. Shibamouli Lahiri, Sagnik Ray Choudhury, and Cornelia Caragea. "Keyword and keyphrase extraction using centrality measures on collocation networks". In: arXiv preprint arXiv:1401.6571 (2014). Marina Litvak and Mark Last. "Graph-based keyword extraction for single-document summarization". In: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization. Association for Computational Linguistics. 2008, pp. 17­24. Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨ utze. Introduction to information retrieval. Vol. 1. Cambridge university press Cambridge, 2008. Yutaka Matsuo and Mitsuru Ishizuka. "Keyword extraction from a single document using word co-occurrence statistical information". In: International Journal on Artificial Intelligence Tools 13.01 (2004), pp. 157­169. Alexander Mehler. "Large text networks as an object of corpus linguistic studies". In: Corpus linguistics. An international handbook of the science of language and society (2008), pp. 328­382. Roberto Navigli, Paola Velardi, and Stefano Faralli. "A graph-based algorithm for inducing lexical taxonomies from scratch". In: IJCAI. 2011, pp. 1872­1877. Rogelio Nazar, Jorge Vivaldi, and Leo Wanner. "Cooccurrence graphs applied to taxonomy extraction in scientific and technical corpora". In: Procesamiento del lenguaje natural 49 (2012), pp. 67­74. Simone Paolo Ponzetto and Michael Strube. "Taxonomy induction based on a collaboratively built knowledge repository". In: Artificial Intelligence 175.9-10 (2011), pp. 1737­1756. ´ Miguel Riesco, Mari´ an D Fond´ on, and Dar´ io Alvarez. "Designing Degrees: Generating Concept Maps for the Description of Relationships between Subjects". In: Concept Mapping: Connecting Educators. Proc. of the Third Int. Conference on Concept Mapping. Tallinn, Estonia & Helsinki, Finland: Tallinn University. 2008. Cornelis Joost van Rijsbergen. "A theoretical basis for the use of co-occurrence data in information retrieval". In: Journal of documentation 33.2 (1977), pp. 106­119. Adam Schenker. "Graph-theoretic techniques for web content mining". In: (2003). John F Sowa. "Principles of semantic networks". In: (1991). Mark Steyvers and Joshua B Tenenbaum. "The Largescale structure of semantic networks: Statistical analyses and a model of semantic growth". In: Cognitive science 29.1 (2005), pp. 41­78. Wilson Yiksen Wong. Learning lightweight ontologies from text across different domains using the web as background knowledge. University of Western Australia, 2009.

1306

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

[33]

T Yildiz and S Yildirim. "Association rule based acquisition of hyponym and hypernym relation from a Turkish corpus". In: Innovations in Intelligent Systems and Applications (INISTA), 2012 International Symposium on. IEEE. 2012, pp. 1­5.

2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1307

