1 spire  efficient data inference and compression over rfid streams yanming nie  richard cocci  zhao cao  yanlei diao  and prashant shenoy department of computer science  university of massachusetts amherst  u.s.a school of computer science  northwestern polytechnical university  china abstractdespite its promise  rfid technology presents numerous challenges  including incomplete data  lack of location and containment information  and very high volumes  in this work  we present a novel data inference and compression substrate over rfid streams to address these challenges  our substrate employs a time-varying graph model to efficiently capture possible object locations and inter-object relationships such as containment from raw rfid streams  it then employs a probabilistic algorithm to estimate the most likely location and containment for each object  by performing such online inference  it enables online compression that recognizes and removes redundant information from the output stream of this substrate  we have implemented a prototype of our inference and compression substrate and evaluated it using both real traces from a laboratory warehouse setup and synthetic traces emulating enterprise supply chains  results of a detailed performance study show that our data inference techniques provide high accuracy while retaining efficiency over rfid data streams  and our compression algorithm yields significant reduction in output data volume  index termsrfid  data streams  data cleaning  compression  supply-chain management i i ntroduction r fid is a promising electronic identification technology that enables a real-time information infrastructure to provide timely  high-value content to monitoring and tracking applications  an rfid-enabled information infrastructure is likely to revolutionize areas such as supply chain management  healthcare  pharmaceuticals  1   postal services and surveillance in the coming decade  data stream management is central to the realization of such a monitoring and tracking infrastructure  while data stream management has been extensively studied for environments such as sensor networks  2    3    4  existing research has mostly focused on sensor data that captures continuous environmental phenomena  rfid dataa triplet < tag id  reader id  timestamp > in its most basic formraises new challenges since it may be insufficient  incomplete  and voluminous  insufficient information  since rfid is inherently an identification technology designed to identify individual objects  a stream of rfid readings does not capture inter-object relationships such as co-location and containment  for instance  an rfid stream does not directly reveal whether flammable objects are secured in a fire-proof container  or foods with and a preliminary version of this work appeared as a 3-page poster paper at icde 2008 without peanuts are not packaged in the same container  even though all items and containers are affixed with rfid tags  incomplete data  despite technological advances  rfid readings are inherently noisy with observed read rates significantly below 100 % in actual deployments  5    6   this is largely due to the intrinsic sensitivity of radio frequencies  rfs  to environmental factors such as occluding metal objects  7  and contention among tags  8   missed readings result in lack of information about an objects location  significantly complicating the tasks of determining object location and containment and detecting anomalies such as missing objects  high volume streams  rfid readers are often configured to read frequently when they are deployed in wired  powered environments  large deployments of such readers can create excessively large volumes of data  e.g  over terabytes of data in a single day  9   the resulting data  however  may encode significant amounts of redundant information such as an unchanged object location  hence  it is crucial that data be filtered and compressed close to the hardware while preserving all useful information  recent research on rfid data cleaning  10    6    11  has employed smoothing techniques to clean individual tag streams and estimate tag counts in a given location in the presence of missed readings  these techniques  however  do not capture inter-object relationships such as containment or identify anomalies such as missing objects  recent research on probabilistic query processing  12    13  has not focused on the derivation of information mentioned above  such as containment or missing objects  but its query processing can be enriched once such information is made available as input  furthermore  none of the above work has addressed the data compression problem  compression techniques for rfid warehouses use expensive disk-based operations such as sorting and summarization  14  or employ applicationspecific logic  15   hence  they are unsuitable for fast online compression of rfid streams close to the hardware  in this paper  we present spire  a system that addresses the above three challenges by building an inference and compression substrate over rfid data streams  this substrate enables accurate inference of observed data  even though the raw data is incomplete  further  it infers inter-object relationships such as co-location and containment as well as anomalies such as missing objects  finally  by performing online inference  it enables online compression that discards redundant data such as an unchanged object location or an unchanged containment between objects  online compression significantly reduces 2 data volume  thereby expediting processing and reducing data transfer costs  the spire system employs three key techniques  which are also the main contributions of this paper  we propose a time-varying graph model that captures possible object locations and containment relationships with its efficient construction from raw rfid streams  we further develop an online probabilistic algorithm that estimates the most-likely locations of objects and containment relationships among objects  which subsume co-location relationships  from the information captured in the graph model  we finally devise an online compression algorithm that transforms an input raw rfid stream into a compressed yet richer output event stream with both location and containment information  we have implemented our inference and compression substrate in a prototype system and evaluated it using both real traces from a laboratory warehouse setup and synthetic traces emulating enterprise supply chains  our results show that our data inference techniques achieve error rates below 15 % for location estimates for a wide range of rfid read rates  and within 20 % for containment estimates when the read rate reaches 80 %  in addition  these techniques can be performed efficiently on high-volume rfid streams  furthermore  our compression techniques can encode rich location and containment information using only 20 % or less of the raw input data size when the read rate reaches 80 %  finally  we compare our system with smurf  11   a state of the art system for rfid data cleaning  that can be used to produce object location information but not containment information  for object location updates  our system outperforms smurf in both the error rate and the resulting compression ratio  the rest of the paper is organized as follows  section ii formulates the problem  sections iii  iv  and v describe the three key techniques of our system  section vi presents results of a detailed performance study  finally  section vii presents related work  and section viii concludes the paper  ii  p roblem s tatement before defining the problem  we present the notion of the physical world  a physical world covers a specific geographical area comprising a set of objects o  a set of pre-defined  fixed locations l  and an ordered discrete time domain t  the set of locations can be either pre-defined logical areas such as aisle 1 in warehouse a  or  x  y  z  coordinates generated by a positioning system  at time instant t  the state of the world includes  1  the set of objects present in each location  encoded by the boolean function resides  o i o  lk l  t   which is true iff object oi is present at location l k ; and 2  the containment relationship between objects  encoded by the boolean function contained  o i o  oj o  lk l  t   which is true iff objects o i and oj are both in location lk and oi is contained in oj  in this work  we refer to the functions resides and contained as the ground truth  the state of the world changes whenever an object enters the world  exits the world through a designated channel  e.g  an exit door   or changes its location or containment relationship with other objects  the set of locations l also contains a special location called unknown  in particular  an object can be in the unknown location if it is not present in any pre-defined location  e.g  if it is in transit between two locations  or if it exited the physical world improperly  e.g  was stolen   rfid readers provide a means to observe the physical world  the readings produced at time t are collectively called an observation of the world  in this work  we focus on readers mounted at fixed locationsa common configuration in todays rfid deployments  for such fixed readers  a reading captures the location of the object  which is the same as the location of the reader  such readings  however  are inadequate for capturing the containment between objects  furthermore  the observation of the world may be incomplete since some objects may not respond to reader queries due to technological limitations  as a result  both the location and containment of an unobserved object becomes unclear  the data inference problem is to construct an approximate yet accurate estimate of the state of the world based on the observations thus far  we define an approximation using functions resides and contained that for given arguments  return probabilistic values representing the likelihood of the function being true  then the data inference problem can be formulated as  given the time now and an object o i  report 1  the most likely location of the object  denoted by arg maxk resides  oi  lk  now   and 2  the most likely container of the object  denoted by arg maxj  k contained  oi  oj  lk  now   the data compression problem is to transform the input stream into an output stream with a reduced data volume but with no loss of information  such compression requires the knowledge of what data is redundant and thus can be safely discarded ; in this work  we use inference to obtain such knowledge  the combination of inference and compression yields an output stream that  i  augments the input stream with additional  likely information about objects  and  ii  has a significantly reduced volume of data  a running example  a warehouse scenario is depicted in fig  1  where rfid readers are installed above the loading dock  the conveyor belt  and two shelves  each arriving pallet is scanned at the loading lock  together with the cases on the pallet and items in the cases  an rfid tag is attached to every pallet  case  and item  in this example  at time t=1  the reader at the loading dock reports objects 1 to 6  denoted by the black nodes  these nodes are arranged according to the packaging levels that the reported tag ids indicate  16   object 7 is also present but was missed by the reader  denoted by a white node  i.e  a missed reading  containment between objects  depicted by the dashed edges  is not reported by the readings and often uncertain  examples of ambiguous containment are the containers of items 4  5  6  which can be either case 2 or case 3 based on the readings received  then the pallet stays at the locking dock while the cases are further scanned in other places  at time t=2  case 2 is scanned individually on the conveyor belt  it is possible to confirm the 3 time 1 level 2  case level 3  item locations t=4 t=3 t=2 t=1 level 1  pallet 2 4 3 5 6 3 2 7 a  loading dock 4 5 b  belt 6 7 b  belt 9 10 s1  shelf 1 9 s2  shelf 2 2 8 8 8 6 b  belt 9 10 4 s1  shelf 1 3 5 3 7 s2  shelf 2 fig  1 example readings of rfid-tagged objects in a warehouse  nodes in black  gray  and white represent objects read in ithe true location  objects read by a nearby reader  and unobserved objects  respectively  a dashed edge denotes a containment relationship between objects  which can not be directly observed  event db complex event processor network event output  compression interpretation & interpretation  probabilistic inference compression data capture  graph construction rfid devices fig  2 architecture of the spire system  containment between the case and its item  s  if we know that the belt reader scans cases one at a time  at t=3  case 3 is scanned on the belt with its contained items  a new case 8 with items 9 and 10 are read at shelf 1  which is their true location  in addition  case 8 and item 9 are also read at shelf 2 by a nearby reader  which are called duplicate readings  at t=4  item 6 is read at the belt again  it fell off its case at t=3 and stayed here   case 2 was placed onto shelf 1 however  case 2 and item 5 are missed by the reader at shelf 1 case 3 was placed onto shelf 2 it then receives both a reading from shelf 2 and a duplicate one from shelf 1 finally  case 8 is read at shelf 1 again but the reading of item 9 is missed  system architecture  the spire system employs a data inference and compression substrate to address the above issues  the substrate  epicted in fig  2  consists of  i  a data capture module that implements a stream-driven construction of a time-varying graph model to encode possible object locations and containments   ii  an inference module that employs a probabilistic algorithm to estimate the most likely location and containment for an object  and  iii  a compression module that outputs stream data in an compressed format  the next three sections describe these techniques in detail  iii  data c apture this section describes our data capture technique to construct a time-varying graph model from the raw rfid stream  a a time-varying colored graph model our graph model g =  v  e  encodes the current view of the objects in the physical world  including their reported locations and  unreported  possible containment relationships  in addition  the model incorporates statistical history about co-occurrences between objects  example graphs for the observations in fig  1 are shown in fig  3 the node set v denotes all rfid-tagged objects in a physical world  in a supply-chain environment  the rfid standard  16  requires that an object have a packaging level of an item  case  or a pallet  and this information be encoded in the objects tag id  given such information  our graph is arranged into layers  with one layer for each packaging level  in addition  each node has a set of colors that denote its observed locations  with one color for each observed location  the node colors are updated using the stream of readings in each epoch  each color assigned to a node represents the location where it is observed by an rfid reader   if an object is not read by any reader in a given epoch  its node becomes uncolored  however  every node retains memory of its recent colors in the last k observations  k=1 as depicted in fig  3  but k=3 actually used in this work   denoted by a list   recent color  seen at    if the node obtained the same color several times in the last k observations  the most recent observation is used to set the seen at attribute  the directed edge set e encodes possible containment relationships between objects  a directed edge o i oj denotes that oi contains object oj  e.g  a case i contains item j   we allow multiple outgoing and incoming edges to and from each node  indicating an object such as a case may contain multiple items  and conversely  an item may be contained in multiple possible cases  our inference method will subsequently choose only one of these possibilities   more generally  edges can exist between different combinations of colored and uncolored nodes  except that an edge can not connect two colored nodes for which all available colors indicate a distance between the two objects beyond the read range of an rfid reader  e.g  over 40 feet  in such cases  there can not exist a containment relationship between the two objects  as such  our colored graph can capture a wide variety of containment relationships  to enable probabilistic analysis  our graph also encodes rich statistics  each edge maintains a bit-vector recent co-locations to record recent positive and negative evidence for the co-location of the two objects  a bit is set every time the two nodes connected by an edge are assigned the same color  i.e  the two objects are both observed in the same location  furthermore  each node records the confirmed parent  i.e  the most recent confirmed container as a result of a highly likely estimate  the time of confirmation  and the number of conflicting observations obtained thus far  among all incoming edges to a node  at most one edge can be chosen as the confirmed edge  which we detail in section 4   4 time level 2  a,1  2 locations  a,1  3 5 4 6  a,1   a,1   a,1  a  loading dock 4  b,2   d  t = 4 1  a,1   a,1  3  b,2  2  b,2  2  b  3  3 1  a,1   s1,3  8 8  s2,3   b,2  2  s1,4  3 3  s2,4  8  s1,4  10 5 4 6 7 9 9 4 5 10 6 7 9  b,2   a,1   a,1   b,3   s1,3   s2,3   s1,3   s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  5 6  a,1   a,1  b  belt b  belt s1  shelf 1 s2  shelf 2 b  belt s1  shelf 1 s2  shelf 2 evolution of the time-varying colored graph model as rfid readings arrive in each epoch   b  at t=4  step 2  add edges   a  at t=4  step 1  create and color nodes  1  a,1  1  a,1   s1,4  3 3  s2,4   b,2  2 8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt fig  4   c  t = 3 1  a,1  1  a,1  level 3 fig  3   b  t = 2  a  t = 1 level 1 s1  shelf 1 s2  shelf 2  b,2  2  s1,4  3 3  s2,4  8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt s1  shelf 1 s2  shelf 2 intermediate steps of the graph update procedure  b stream-driven graph construction we assume that time is divided into epochs and the graph is updated using stream data from each epoch  our construction algorithm takes the graph g from the previous epoch and a set of readings rk from each reader k in the current epoch  and produces a new graph g  the graph update procedure proceeds in four steps  the detailed pseudo-code is provided in our technical report  17    step 1 create and color nodes  if a new object is observed for the first time  a new node is created in the graph  for each observed object  the color of the location in which it was observed is added to the color set of the corresponding node  fig  4  a  shows the result of this step when it is applied at time t=4 to the previous graph  fig  3  c    using the readings from the conveyor belt  shelf 1  and shelf 2 step 2 add edges  next  if two nodes in adjacent layers share a common color  e.g  nodes 3 and 4 share the color for shelf 1  an edge is added between them if it does not already exist  doing so enumerates all possible containment relationships  e.g  an item observed at shelf 1 can be contained in any of the cases that are also observed at shelf 1   this step may require each node in a layer to be compared with all the nodes in the adjacent layers  an optimization for this step is to restrict such comparisons with adjacent layers only to the nodes that have just been assigned a new color  this is because if neither node of an edge is assigned a new color  then both objects are either in original locations or unobserved  offering no information for establishing a new containment relationship  fig  4  b  illustrates the result of this step of adding edges to the graph in fig  4  a   the bold circles represent the nodes that have changed their colors at t=4  hence  new edges are created only for these nodes  e.g  between nodes 3 and 4  3 and 10  8 and 4  and 8 and 5 step 3 remove edges  while the previous step adds new edges to the graph  in this step we remove outdated edges from the graph  an edge is removed if both nodes of the edge are colored and every possible pair of colors of these nodes indicates a large distance between the locations of the corresponding objects  i.e  beyond the read range of a single reader   in our example  assume that the readers for the belt and shelf 1 are far away from each other  then the edge between nodes 3 and 6 in fig  4  b  is removed because the colors of these nodes indicate the belt and shelf 1  respectively  the resulting graph is in fig  3  d   additional edges can be pruned if we can confirm the parent edge  container  of a node  hence eliminating other possibilities  we discuss the inference method that enables such edge pruning in the next section  step 4 update edge statistics  this step updates statistics of the edges that have at least one node colored in step 1 given an edge e  if the two linked nodes share a common color  recent co-locations of e is updated by setting the most recent bit to true  if one of the linked nodes is uncolored  the most recent bit is set to false  in this case  we also check if e was set before as the confirmed parent edge of the child node  and if so count the current epoch as a conflicting observation of the confirmation  these statistics play a key role in containment inference as we shall show shortly  complexity analysis  we finally analyze the complexity of the graph update procedure  the total cost of updating a graph g  v  e  using reading sets r 1      rk is the sum of the individual update costs for r k  1 k k for each reading set rk  only the colored nodes and their incident edges are processed  the cost analysis is as follows  step 1  given the reading set r k  the cost of coloring nodes is simply |rk |  steps 2  3  and 4  after processing all reading sets  r1   rk  the next three steps share the process of examining every edge linked to a colored node  two observations hold  first  consider those edges for which the two linked nodes share a common color  given a reading set rk  the maximum number of edges that may have both nodes colored by r k is the size of the largest bipartite graph covered by r k  that is   |rk |/2  2  other edges either have different colors assigned to the two linked nodes or have one node colored but the other uncolored  these edges must have already existed in the input graph g such edges can be bounded by the projection of the input graph onto the subset of edges that are linked to at least one node colored in the current epoch  denoted by r1    rk  g   given that each edge is visited at most twice  one from  each linked colored node  the cost of steps 2 to 4 is at most k |rk |2 /2 + 2|r1    rk  g  |  so   the total cost of graph update for all readers in an epoch 2 is o  k |rk | + |r1    rk  g  |   this upper bound includes a cost no more than the input graph size and some local 5 costs quadratic in the size of the subgraph colored by each reader  it is important to note that the local quadratic costs are rare because there are usually more objects than containers  hence in practice  we see close to a linear cost in r k  these costs are also bounded because the anti-collision protocol of an rfid reader limits the number of tags that it can read in an epoch  e.g  up to 400 using latest rfid readers such as thingmagic mercury 6e  the actual numbers of tags placed in real deployments are significantly less in order to ensure stable performance  iv  data i nference the graph constructed from the data capture step can result in nodes that are uncolored or multicolored and possess multiple parent nodes  the data inference step estimates the most likely location of an object if it is unreported  uncolored  or is reported in multiple locations  multicolored   it also estimates the most likely container  parent  of each object  we present probabilistic techniques that include edge inference to address ambiguous containment  node inference to address uncertain locations  and an iterative procedure that applies both to the entire graph in an alternating fashion  a edge inference edge inference is applied to all incoming edges of a node v  i.e  edges from the parent nodes of v  regardless of whether the node is colored  it assigns a probability value p ei to each edge ; the edge with the highest probability value is then chosen as the most likely container of this object  edge confirmation algorithm  our first technique aims to find a sequence of readings  called the critical region  that distinguish the true container from others  an example would be when a reader at the conveyor belt reads a case with its contained items  with no other case being observed at the same time  moreover  the belt reader is far away from other readers so the case and its items can not be observed elsewhere  if this trend continues for a few epochs  one shall be able to confirm the  only  case at the belt to be the container of the observed tags  this period may be short in general as the case can be soon put on a shelf together with many other cases  we next present an information-theoretic approach to detect short critical regions when the received readings distinguish the true container from others  given a node v  our detection algorithm works as follows  step 1 assign weights  the first step computes a weight wei for each incoming edge as follows  w recent co-locations  i  wei = i=0   1  w i=0 1 where recent co-locations  i  indexes the i th bit of colocation bit vector and w is the size of the window used in critical region detection  we consider a small window  w =4 used in our work  since larger w values could add noise and hence make these edge weights less useful  step 2 compute probabilities  this step computes a probability pei for each edge by normalizing the weight  we pei = m i j=0 wej  2  step 3 compute entropy  this step computes the normalized entropy of the probability distribution of all incoming edges of a node  h= m  j=1 pej log pej / m  1 1 log m m j=1  3  as is known in information theory  the entropy is a measure of uncertainty associated with a random variablethe true parent of a node in this work  a low value of the entropy indicates less uncertainty about the true parent  hence  whenever the entropy is significantly low  we can consider the current window as a critical region and return the edge with the highest probability as the confirmed parent  however  it is also known that a random variable with 10 equally possible values has a higher degree of uncertainty than a random variable with 2 equally possible values ; that is  the entropy is sensitive to the number of possible values  to mitigate this effect  we normalize the entropy of a node with m parent edges with the maximum entropy of a m-valued random variable  i.e  when each value has probability 1/m  then we compare the normalized entropy  h  with a threshold  ; if h <  the edge with the highest probability is chosen to be the confirmed parent  as our evaluation in section vi-b shows  using the normalized entropy makes it easy to choose a reasonably low threshold that offers stable performance  step 4 edge pruning  once an edge is confirmed to be the parent of the node v  all other incoming edges pointing to v can be dropped  in the example in fig  3  b   assume that the readings from the belt reader constitute a critical region and the edge from node 2 to node 4 is confirmed to be the parent of node 4 then  the edge from node 3 to node 4 can be safely removed  if we further know  using domain knowledge  that a case is the top level container on a belt and indeed the belt reader only observes a case but not a pallet  we can further drop all parent edges of a confirmed container  e.g  the edge from node 1 to node 2 in fig  3  b   edge inference algorithm  if the edge confirmation did not succeed  we perform edge inference for node v to select the most likely parent edge as the estimated container  performing edge inference requires the use of history that includes  i  the recent history of co-locations  stored in the bit-vector recent co-locations  and  ii  the last confirmed parent of v  captured in confirmed parent  such use of history makes edge inference less sensitive to missed readings at present time  edge inference at a node consists of two steps  as illustrated in fig  5  a   step 1 assign weights  the first step computes a weight wei for each incoming edge as follows  s recent co-locations  i  i wei = i=0   4  s 1 i=0 i where s is the full size of the co-location bit vector  this entire co-location history is weighted using the parameter and them normalized  essentially implements a zipf distribution  6 8 2 step 1 e2 e1  b  2  2 e1  weight w1 e 1 e4 e5 4 5 6  s1  4   s1  4   b  4  e2  weight w2 4 step 2 confirmed in the past e1  pe 1 z 1  belief from past confirmation e2  pe 1 color `` unknown ''  p color `` b ''  pb  missing from b   still at b  2 w1 w2 z z  1   1  belief from recent history  a  edge inference fig  5  1  now  seen_at+1    1   now  seen_at+1  1  belief gained from the node itself color `` s1 ''  ps1 pe 5 pe i  moved to s1  pe pe 1 4 pe pe i i  belief gained via containment  b  node inference examples of edge inference and node inference  where > 0 assigns a higher weight to recent history  while = 0 weighs all prior co-location information equally  step 2 compute probabilities  the next step builds a probability distribution across all incoming edges of node v it computes a probability p ei for each edge by balancing the relative weight on this edge against the last confirmation of this edge as the parent of v a parameter is used to weigh these two factors  the probability p ei of the edge e i is   1  m  ei  + wei  5  z the memory function m  e i  takes the value 1 if e i is the last confirmed edge and 0 otherwise  since at most one parent edge of a node can be a confirmed edge  such an edge gains an extra weight and is favored over other possibilities until other edges gain sufficient history to outweigh it  z is a normalization factor for yielding the final distribution  which is the sum of the probabilities of all incoming edges of node v fig  5  a  shows a distribution across two parent edges  e 1 and e2  of node 4  with e 1 assigned the additional weight 1 due to its past confirmation  edge inference involves three parameters   1  s  the size of the co-location history   2   the zipf parameter for weighting the history  and  3   the partition of beliefs between the recent history and past confirmation  section vi quantifies the sensitivity of edge inference to these parameters  in particular  we will show that the choices for s and are quite constant but that for can be variable  furthermore  the value of can be dynamically determined using an adaptive method that sets to be the ratio of the instances that only one of the object and its confirmed container is observed against the instances that any of them is observed  this method is shown to perform well in section vi  pei = b node inference node inference is applied to all the nodes in the colored graph  if a node is multicolored  node inference chooses the most likely color as the estimated location of the corresponding object  if a node is uncolored instead  node inference attempts to infer the most likely location of the object or confirm its absence from any known location  a key challenge in node inference arises from a three-way tradeoff among continued stay  movement to a new location  and absence from any known location  these situations are depicted in fig  5  b  for node 2 at time t=4  this object was last seen in location b at time t=2 and has a few possibilities for its current location  it is still in location b but the reading in this location was missed  continued stay  ; it moved to location s 1 with its contained objects and its reading was missed in s 1  movement to a new location  ; it disappeared from b and its current location is unclear  absence from any known location   node inference algorithm  to account for all these possibilities  the node inference builds a probabilistic distribution over all possible colors of a node v  including  1  the recent colors of the node   2  the colors of its neighboring nodes that can be propagated through the edgesedges are considered bidirectional in node inference  and  3  a special color unknown  among all  the color with the highest probability represents the most likely estimate of this objects location  formally  the probability of the node v having color c i is   pe   v  ci  i pci  v  =  1  +  6   now seen at + 1  z2 ei ci   v  ci  1  ci is a recent color of v  v  ci  =    v  ci  =  0  otherwise j  v  cj  here  v  ci  is an indicator function that takes the value 1 if ci is one of the recent colors for the node v  and 0 otherwise ;   v  ci  is the value normalized across all colors  the parameter controls the rate of fading of a recent color  we further consider the influence of the colors of the neighboring nodes  hence taking advantage of the containment relationship  if a node v has acquired multiple colors  we consider these colors observed at the neighboring nodes and include them in the node inference at v if the node is not colored in the current epoch  we take into account both the observed and inferred colors at the neighboring nodes  in the above formula  e i ci means that the edge e i propagates the color ci to v  and z2 is the normalization factor across all edges of v that propagate colors to v of particular interest is the parameter that weighs the colors that originate from the node against the colors that propagate through the edges  finally  the probability of the special color unknown is     v  cj   punknown  v  =  1   1  now seen at + 1  j  1  m   7  7  a  d = 0  l = 3  item level   b  d = 0  l = 2  case level   b,2  2  s1,4  3 3  s2,4  8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt fig  6 s1  shelf 1  c  d = 1  l = 3  item level  1  a,1  1  a,1  s2  shelf 2  b,2  2 3  s1  s2,4  8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt  d  d =  l = 1  pallet level  1  a,1  s1  shelf 1 s2  shelf 2  b,2  2 1  a,1  3  s1  s2,4  8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt s1  shelf 1 s2  shelf 2  b,2  2 3  s1  s2,4  8  s1,4  4 5 10 6 7 9  s1,4   s1,4   b,4   s2,4   s1  s2,3   s1,4  b  belt s1  shelf 1 s2  shelf 2 illustration of iterative inference across the graph in increasing distance from the colored nodes  where m denotes the number of parent edges of v as can be seen  a colored node always has the unknown probability 0 because now equals seen_at and the sum of   v  cj  is 1 it is evident that the quantities computed for all colors using eq  6 and eq  7 sum up to 1  hence forming a probability distribution  fig  5  b  shows the resulting probability distribution over three colors  b  s 1  and unknown  in summary  node inference is influenced by two parameters   1  weighs the node colors assigned based on the assumption that the object is independent of other objects  against the colors that are propagated from edges based on the containment relationships ; and  2  for the former set of colors   the exponent of the function  nowseen at+1   further adjusts the distribution of the probability mass between the fading color and the unknown color  we quantify the sensitivity of node inference to and in section vi  c iterative inference iterative inference combines node and edge inference to iterate over the entire graph g  v  e  and derives the most likely location and containment for each object  traditional graph traversal algorithms such as breadth-first and depth-first search can not be applied here due to the dependency between edge and node inference  specifically  node inference involves the colors of its neighboring nodes and the probabilities of the edges between those nodes  so it can not begin until these dependencies are first resolved  the key idea of our iterative algorithm is to start inference from the colored nodesthe nodes with observed locations and run it iteratively across the graph  through the edges linked to the colored nodes  to the uncolored nodes incident to these edges  to the edges linked to these nodes  and so on  in this way  inference sweeps through regions of the graph in increasing distance from the colored nodes ; the colors and edge probabilities determined at nodes in a shorter distance can contribute to the inference at nodes in a larger distance  to run iterative inference  we classify nodes based on their closest distance  d  from a colored node in the graph  and visit the nodes in increasing value of d for the nodes of the same distance d  we visit them in decreasing value of the packaging level  e.g  from the items to the cases and then to the pallets   for each node visited  we first perform edge inference among all incoming edges of the node  and then node inference involving all neighboring nodes at distance d1 or less  except that if a node is colored  i.e  at d = 0  we consider all colored neighboring nodes  which are also at d = 0 fig  6 illustrates this process for the graph in fig  3  d    the pseudocode is left to  17  due to space constraints   the algorithm first considers the nodes with the distance d = 0  colored  and the packaging level = 3  at the item level   including nodes 4  5  6  7  and 10 as shown in fig  6  a   edge inference is performed for these nodes to estimate their most likely parents  where a gray bar marks the edges considered in the edge inference at a node  since each of the nodes has only one color  the node inference is trivial  then the algorithm considers the nodes with d = 0 and = 2  at the case level   including nodes 3 and 8 as shown in fig  6  b   since these nodes do not have incoming edges  edge inference is simply skipped  during node inference  node 3 is assigned the color for shelf 2 this is because the edge from node 3 to node 7 was confirmed as a parent edge of node 7 when they were scanned on the belt  and now the high probability of this edge transfers from node 7 significant evidence for the color for shelf 2  outweighing the color for shelf 1 next  the algorithm considers the nodes labeled with d = 1  uncolored  and = 3  at the item level   including only the node 9 as shown in fig  6  c   the node inference chooses the color for shelf 1 due to the containment relationship with node 8  which is observed at shelf 1 finally  the algorithm considers node 1 with d =  disconnected from any colored node  and = 1 it has not been observed since t=1 and hence gains the highest probability for the unknown location  i.e  reported missing from the physical world observable by the existing readers  complexity and optimizations  the complexity of the iterative algorithm is bounded by the number of edges examined in the graph g  v  e   given that each edge is visited at most twice  once from each linked node  the complexity is o  |e|   to improve time and space efficiency  we can further use a graph pruning routine in the iterative inference procedure  first  if an object exits the physical world through a proper channel  e.g  through an exit door of a warehouse  and is detected by the reader at that place  after inference at the node representing this object  our system removes the node and any associated edges from the graph  second  after edge inference at a node  we can also use the edge weights to prune edges that are unlikely to be the true containment  to do so  we use a threshold  with a default value 0.25  to remove edges whose weights are below the threshold  partial inference  a practical issue to consider is that rfid readers may read at different frequencies  in a warehouse  for instance  belt readers may read once every second while shelf readers may read once every 10 seconds  if we run inference for the objects whose closest readers are inactive for a while  the inferred locations are likely to be the unknown location  which are different from the true locations  to address the 8 issue  our system performs partial inference in the epochs when not all the readers are active  by  1  restricting inference to the subset of the graph up to 1 hop away from the colored nodes  and  2  withholding the inference result of the unknown location until a later time when all readers are active to run complete inference and output accurate results  conflicts resolution  a final issue with the iterative inference algorithm is that it may result in different colors inferred for the two nodes of an edge  this is because the colors of the two nodes were inferred individually in steps d and d + 1 if the edge between the two nodes is also chosen to represent their containment relationship  then the inference is yielding conflicting information  the container and the contained object are reported in different locations  in the example in fig 3  d   node 7 is observed at shelf 2 suppose that the edge from node 3 to node 7 is inferred to be node 7s parent but the location of node 3 is inferred to be shelf 1  which is unlikely but used for the sake of an example   now we have a conflict between location and containment inference  in our system  we preserve the graph model with all the statistics as is  and resolve conflicts in a post-processing step after inference  our guideline on conflict resolution is to give priority to a containment relationship  this is because the containment is often based on the confirmed parent that was derived with high probability before  as described in section iv-a  hence  given a reported containment relationship  if the parent and child nodes have different inferred colors  we use the parents location to override the childs locationhere we favor the parents location because its inference has taken into account the locations of all its children  in particular  those that have confirmed containment relationships with this parent node  v s tream o utput with c ompression the output module of the spire system takes the results of data inference and transforms them into a compressed event stream for output  compared to the raw rfid stream  the output based on inference results adds location information for unobserved objects and containment information not available in the input stream  the key idea behind our compression methods is that only those readings that indicate a state change  such as the change of an objects location or containment relationships with others  need to be included in the output stream  in the absence of a state change  all readings merely confirm the current state of the world and hence are redundant ; these readings can be safely discarded  hence  the compressed output stream contains richer information yet with a reduced data volume  below  we describe the data format of a compressed stream and two compression techniques  a data format of a compressed event stream a compressed output stream contains location and containment events that occur in a time interval  called the events validity interval  18   the validity interval is represented by two timestamps  vs for the start time and v e for the end time  our compressed output format represents these events using the following five messages  startlocation  object  location  v s  ve =  endlocation  object  location  v s  ve  startcontainment  object  container  v s  ve =  endcontainment  object  container  v s  ve  missing  object  locationmissingfrom  v s  ve = vs  start and end location messages always occur in pairs and encapsulate the time period when an object is inferred to be present at a particular location  the difference is that the start location message of an event sets only the v s timestamp  leaving ve with the default value  while the end location message later sets ve  similarly  start and end containment messages encapsulate the time period of a containment relationship  missing messages are singletons always output after an endlocation event for the objects previous location  in this work  we call a compressed stream well-formed if for a given object  every start location  containment  message has a matching end location  containment  message and a missing message appears outside any start-end location pair  our system guarantees well-formed output  and at the same time  allows location and containment update events to be nested in the most flexible way  for an object  a start-end containment pair can span multiple start-end location pairs  representing an unchanged containment as the two objects move together through various locations  in addition  when an object is reported missing  the existing containment is not ended  that is  a start-end containment pair can also enclose the missing events  on the other hand  it is also possible that a start-end location pair covers multiple start-end containment pairs  capturing the containment changes in the same location  b range compression our first compression method  which we refer to as range compression or level-1 compression  leverages the fact that if an object is stationaryresident at the same location for a period of timeits entire stay at this location can be represented by a single ranged location event  likewise  if an object has a stable containmentcontained in the same case or pallet for a period of timethis containment relationship can also be represented by a single ranged containment event  the method is implemented by simply comparing an objects newly inferred state  either location or containment  to its previously reported state  for an object that is inferred to be missing  we output an endlocation message to complete the previous location event and then a singleton missing message  the output stream of range compression has two properties  first  location compression and containment compression are performed separately  hence  it is possible to split the output into separate location and containment update streams  and suppress the output of one stream if not needed  second  the result stream of range compression includes complete location and containment information for each object  and hence is directly queriable by event systems such as  18    19   c location compression using containment our second compression method  referred to as level-2 compression  uses the additional knowledge that under stable containment  location readings of child objects can be further suppressed because they are identical to that of the parent  9 time startcontainment  c1  p  t1    startcontainment  c2  p  t1    startlocation  p  l1  t1    p t1 c2 c1 p t2 endlocation  p  l1  t1  t2  startlocation  p  l2  t2    c2 c1 t4 locations fig  7 c1 endcontainment  c2  p  t1  t3  endlocation  p  l2  t2  t3  startlocation  p  l3  t3    startlocation  c2  l2  t3    c2 p c1 endloction  c2  l2  t3  t4  startlocation  c2  l4  t4    c2 l1 parameter l2 l3 l4 an example of level-2 compression for a group of objects  the benefit of dosing so is to minimize the location output to only the location of top-level containers  this compression is lossless because the location of a contained object can be recovered from its containment relationship and the location of its top-level container  an example for level-2 compression is shown in fig  7 at time t1  a pallet p and two cases c1 and c2 are observed at location l1  for simplicity of presentation  we omit items in this example   a startcontainment is output for each of the contained cases  given the containment relationships  only a startlocation is output for p  the single top-level container  at time t2  the three objects move as a group to l 2  only the location of p is updated due to level 2 compression  at time t3  the three objects are split to two groups  p and c 1 move to location l3 while c2 stays at l2  as a result  the containment between c2 and p is broken  signaled by the endcontainment for c2  then location updates for c 2 are soon output since c2 is no longer contained  in contrast  c 1 is still contained in p so only location updates are sent for p  this compression method has different properties from the range compression method  first  the location and containment output streams are no longer independent  in particular  a reported containment and the related location updates of the container need to be correlated to recover the locations of the contained objects  second  the output stream is not directly queriable by event processors due to the lack of location information of some objects  to facilitate query processing  our system offers a decompression routine that transforms a level-2 compressed stream to a level-1 compressed stream  this routine can be plugged into the front end of a query processor to decompress the input stream on demand  e.g  to retrieve locations of objects in a certain period of time as requested by the queries  in the interest of space  the reader is referred to our technical report  17  for details  vi  p erformance e valuation we have implemented a prototype of our inference and compression substrate in java  in this section  we evaluate the value  s  used duration of simulation 3 24 hours rate of pallet injection 1 per 4 600 seconds cases per pallet 8 items per case 20 read rate  rr  of readers p t3 table i parameters used for generating rfid streams  output of level 2 compression current graph 0.5 1  default 0.85  overlap rate  or  for shelf readers 0 0.8  default 0.25  non-shelf reader frequency  fixed  1  interrogation  per sec shelf reader frequency  variable  1 per sec 1 per min  default  accuracy and efficiency of our inference techniques using both synthetic traces emulating enterprise supply chains and real traces from a laboratory warehouse setup  we also explore the benefits of compression based on results of inference  a simulation design we first developed a simulator that emulates deployments of rfid readers in a large warehouse  pallets arrive at the warehouse at a certain rate  they are first read at the entry door  using reader group 1   they then become unpacked  the contained cases are scanned on the receiving belt  using reader group 2   placed onto shelves for a period of stay  scanned by reader group 3   and then repackaged  scanned by reader group 4   the newly assembled pallets are rescanned on the belt  using reader group 5  and finally read at the exit of the warehouse  using reader group 6   the parameters for the simulation are shown in table i the read rate  rr  parameter specifies the probability that a tag is read by its closest reader  the overlap rate  or  parameter is applied to shelf readers and specifies the probability that a tag is read by a nearby reader to its left or right  the read frequencies of shelf readers and non-shelf readers are controlled separately  this design allows flexible settings of the simulation where items may stay on shelves for hours and shelf readers may read less frequently than other readers  data inference is performed every epoch  which is 1 second   b accuracy of data inference we first evaluate the accuracy of our inference techniques  we created data streams with 6 pallets injected per hour  an average shelving period of 1 hour  and a total simulation time of 3 hours  an inference result is marked as an error if it is inconsistent with the ground truth  containment inference  we first study the effects of the edge inference parameters  s   and  shown in eq  4 and eq  5   on containment inference  our results show that the two parameters  s and  on the recent history of co-locations can be tuned easily  the size of the history  s  limits the inference accuracy when it is small  e.g  4  8  but offers no additional benefit after the point of 32 the zipf parameter   yields best accuracy when set to 0  indicating that recent co-location instances are equally important to inference  hence  we use s = 32 and = 0 in the rest of experiments  the parameter governs the beliefs between the recent history  which can be noisy  and the past edge confirmation  10 80 60 40 20 100 0 0.6 0.7 0.8 0.9 20 0 0 1 0.1 0.2 0.3 0.4 0.5 read rate  a  60 0.8 0.9 40 30 20 10 0 40 30 20 0 0.6 0.7 60 40 20 0 0 0.5 1 0.8 0.9 1 0 read rate 0.1 0.2 0.3 1.5 0.4  e  2.5 3 0.5 0.6 0.7 0.8 100 shelf reader frequency 1 sec 10 sec 30 sec 1 min 80 60 40 20 0 0 0.5 1 1.5 overlapping rate  d  2  c  10 0.5 or0_rr0.5 or0_rr0.6 or0_rr0.7 or0_rr0.8 or0_rr0.85 or0_rr0.9 or0_rr0.95 or0_rr1 80 1 containment inference error  0.7 containment inference error  0.85 location inference error  0.7 location inference error  0.85 50 error rate  %  error rate  %  0.7 100  b  60 containment inference error location inference error 50 0.6 location inference error rate  %  0.5 or0_rr0.5 or0_rr0.6 80 or0_rr0.7 or0_rr0.8 or0_rr0.85 60 or0_rr0.9 or0_rr0.95 or0_rr1 40 location inference error rate  %  = 1.0 = 0.9 = 0.8 = 0.4 =0 adaptive location inference error rate  %  containment inference error rate  %  100 2 2.5 3  f  containment inference error rate  %  fig  8 containment and location inference results   a  containment inference error for   b  location inference error for   c  location inference error for   d  inference error for varied read rates   e  inference error for varied overlap rates   f  inference error  1 anomaly/100 sec   60 0.2 0.4 0.6 0.8 0.9 0.93 0.95 50 40 30 20 10 0 2 3 4 5 6 7 8 9 10 top k edges of highest probabilities fig  9 evaluation results of the edge confirmation algorithm  which may be obsolete  = 1 gives all the weight to recent history and = 0 does the opposite  we compare the use of fixed values and an adaptive method that sets based on the number of conflicting observations  see section iv-a   fig  8  a  shows the results as the read rate  rr  is varied  the adaptive method performs the best for different rr values tested  among the fixed values  large values tend to produce more errors in this workload  this is because many cases can stay on the same shelf for an extended period of time and their shelf readings are a main source of noise in containment inference  putting too much weight on such noisy recent history causes many errors  in contrast  the edge confirmation algorithm confirms the true parent from the readings from the belt reader  the adaptive method can automatically apportion its belief on such past confirmation based on the number of conflicting observations received  we have also demonstrated the effectiveness of the adaptive method when various overlap rates  read rates  and read frequencies are used  the details are left to our technical report  17  due to space constraints  we next evaluate our technique for edge confirmation  as described in section iv-a  at each node we compute the normalized entropy h over all parent edges and check if h is lower than a threshold  supposed to be relatively small   to choose a universal for all nodes  it is helpful to model each node using a k-valued random variable  if a node has more than k parent edges  we consider the top k edges with the highest probabilities ; if a node has less than k edges  we add a few virtual edges and assign them the smallest non-zero probability  in this experiment  we vary k and values  as fig  9 shows  all values under 0.9 offer similar accuracy around 10 %  irrespective of the choice of k this is because the critical information for parent edge confirmation is often the sharp difference in probability between the top two edges  however  when is set too high  over 0.9 here  more false positives of edge confirmation occur  and the accuracy becomes sensitive to the choice of k similar observations hold for various read rates and overlap rates  see  17  for more details   hence  we use =0.75 and k=2 in the rest of study  location inference  location inference uses the node inference method defined in eq  6 and eq  7 we now study the effects of two parameters on location inference  the parameter weighs the belief of an objects recent locations  favored by low values  against the belief of its location inferred via containment  favored by high values   fig  8  b  shows the results for varied values  very low values place most emphasis on the recent locations  which function as fading colors   as such  if an object has experienced several missed readings  it is likely to be inferred to be in the unknown location even if its container has been observed  high values place too much weight on the containment information of an object  which can be unreliable when containment is uncertain  overall  we observe values in  0.2  0.4  to be favorable from this plot and many others which cover a wide range of read rates  overlap rates  and read frequencies  for more see  17    values in this range offer 11 a balance between an objects recent colors and containment relationships  with some more weight on the former  the parameter is the dampening factor on the belief of the continued existence of an unobserved object in a recently reported location  high values cause more quickly reduced belief  making it more likely to refer the object to be in the unknown location  e.g  in transit or missing   fig  8  c  shows that as increases  the error rate quickly declines from over 90 %  flattens in the mid-range from 1 to 2  and then degrades slightly with higher values  the initial decline occurs because  with very low values  the inference takes long to reduce its belief of the continued presence of an object even if the object left a while ago  the deterioration with high values occurs because the inference now drops its belief of continued existence too quickly and identifies an object as being away after just a few missed readings  similar trends are observed for different overlap rates and read frequencies  based on the above results  we use the adaptive algorithm to set the value but fixed values = 0.4 and = 1.25 in the rest of the study  we demonstrate the validity of these values for the real traces from our lab deployment in the next section  sensitivity to read rate  the next experiment reports the effect of the read rate  rr  parameter on the accuracy of both location and containment inference  we varied the read rate uniformly for all readers  as fig  8  d  shows  when the read rate is above 0.8  observed in our lab deployment when the environment is clear of metal objects   our location error rate is less than 10 % and the containment error rate is less than 20 %  as the read rate decreases  the accuracy of containment inference degrades faster than that of location inference  this is because the location inference relies more on the recent locations given our parameter setting  while the containment inference suffers both the loss of parent edge confirmation due to poorer readings from the belt readers and lack of consistent observations in the recent history  sensitivity to overlap rate  we next vary the overlap rate  or  in a wide range  0  0.8  with the read rate  rr  set to 0.85 and 0.7  respectively  we note that in practice one would expect or to be significantly less than 0.5  or otherwise would consider improving the rfid setup to avoid excessive overlap between readers  fig  8  e  shows the inference error rates  first  for common setups where rr is in  0.8  1  and or is in  0  0.3   our system offers both location and containment error rates within 10 %  when stress testing our system with higher or values  we see that the location inference error rates degrade modestly to 9 %  rr=0.85  and 17 %  rr=0.7  when or becomes 0.5 the main effect of or on location inference is to add more possible locations for each object and in certain cases the readings from a nearby reader may outnumber those from the closest reader  hence causing more errors  in comparison  the containment error rates are higher because containment inference heavily relies on co-location information and significant overlap between readers makes this information noisy  the reduced read rate  from 0.85 to 0.7  makes the co-location information even noisier and allows fewer true containment relationships to be confirmed from the belt readings  hence the worst performance in the top line  anomaly detection  the traces used so far have not captured any abnormal behaviors  which are expected to be rare but of significant interest to the application  we next simulated unexpected removals of objects from the warehouse  representing theft or misplacement  at a rate of 1 removal every 100 seconds with random selection from all objects  we report on the inference error rate in fig  8  f  as the most relevant parameter   is varied  this figure exhibits similar trends as fig  8  c  and confirms that the values between 1 and 2 also work well for anomaly detection  we also measured the delay of anomaly detection as varies  the graph is available  17    in general  large values yield short delay whereas small values cause high delay  we observe that the region of 1 and 2 gives the delay as good as larger values  thus   1,2  is shown to offer both accuracy and short detection delay  c accuracy results using a lab deployment to evaluate our system in real-world settings  we developed an rfid lab with 7 readers based on thingmagics mercury5 rfid system  we used 20 cases containing 5 items each  and attached alien squiggle tags to all cases and items  we used the 7 readers to implement 1 entry door reader  1 belt reader  4 shelf readers  and 1 exit reader  cases with contained items transitioned through the readers in that order  receiving around 5 interrogations from each nonshelf reader and dozens from a shelf reader  the shelf readers had overlapping read ranges as they were placed close to each other  we created a collection of traces with distinct characteristics regarding the environmental noise and overlap between readers  t1 represents the combination of good read rates  an average of 0.85 across all readers  and limited overlap rates  an average of 0.25 for shelf readers  which we obtained by using low power of readers  t2 represents the combination of good read rates  an average of 0.85  and significant overlap rates  an average of 0.5  which we obtained using high power of readers  t3 differs from t 1 by including severe noise in the sensing environment  more specifically  we placed a metal bar on each shelf that is 1/3 the length of the shelf  causing the average read rate to drop to 0.7 t4 differs from t 2 with an average read rate of 0.7 t5 to t8 extend t 1 to t4  respectively  with added anomalies  with 20 cases placed on shelves  we randomly selected 4 cases and removed one item from each case to generate missing objects in the trace  each trace is 15 minutes long with 15,000 to 22,000 readings  we ran inference every 3 minutes and manually collected ground truth  including object locations and containment relationships  at those time points  table ii reports the inference results  we make several observations  first  despite the added environmental noise and anomalies of missing objects  our inference techniques still offer the location and conference inference error rates within 20 %  second  under the normal conditions of rr=0.85  without the metal bars placed on shelves   both error rates are within or around 15 %  third  for the traces without anomalies  t1 to t4   our inference results are similar to our simulation results shown in fig  8  d  and fig  8  e   hence demonstrating 12 table ii i nference error rates using lab traces  rr  or  = loc  cont  loc  cont   0.85  0.25  t1 7.0 % 9.0 % t5 11.7 % 14.0 %  0.85  0.5  t2 8.4 % 14.2 % t6 15.4 % 15.5 %  0.7  0.25  t3 13.3 % 14.3 % t7 16.1 % 16.8 % table iii c osts of u pdate and i nference o perations  sec   0.7  0.5  t4 16.5 % 17.3 % t8 18.4 % 19.7 % num  objects update inference total 25068 0.0055 0.0638 0.0693 55118 0.0079 0.1518 0.1597 75066 0.0096 0.2295 0.2392 115144 0.0141 0.4249 0.4389 155044 0.0199 0.6313 0.6512 172550 0.0229 0.7345 0.7573 the validity of our simulation results  in particular  we observe that the containment inference error rates are somewhat better than those in 8  e   this is because the belt reader in our lab deployment produced more readings  5-6 readings on the average  for each tag than in our simulation  3-4 per tag   these extra readings allowed our edge confirmation algorithm to confirm more containment relationships  in some cases  the location inference error rates are slightly higher than the simulation results  the main reason  as we observed  is that for some tags more readings were returned from a nearby reader than from the closest reader in the lab traces  such irregular data makes location inference more difficult  we next discuss two limitations of our lab data set and the methods that we employ to overcome them in our evaluation  first  our data set with 100 items and 20 cases is not large enough for evaluating the efficiency and scalability of inference  in the next section we will extend our evaluation using high-volume synthetic streams that contain hundreds of thousands objects  second  our data set covers several  but not all  combinations of average read rates and overlap rates across readers  due to limited resources  it is hard to produce all possible combinations through manual configuration  fortunately  our results using the lab traces match the simulation results presented above  hence  our simulation results obtained for a broad range of parameter settings can serve as an indicator of the performance expected to be seen in real deployments  memory usage  the memory usage in our system is dominated by the size of the graph  in this experiment  we measured the memory usage with varied graph sizes  note that when an object leaves a warehouse  we remove its node and all of the associated edges to keep the graph small  we also observe that the graph size can be reduced by pruning edges for which the containment inference yields low confidence  the confidence value here is the value in eq  5 but before normalization and thus is insensitive to the presence of other edges  to explore this factor  we applied a threshold for pruning edges and varied it from 0 to 0.75 fig  10  a  shows the rate of memory usage increase as the graph size grows  until our system runs out of memory   first  we see that the memory usage increases fast without edge pruning but less so with increased thresholds for pruning  with a threshold of 0.75  pruning is able to keep the size of the graph under 1.2gb  even with 400,000 objects present in the system  in addition  the memory growth of using the 0.5 and 0.75 thresholds is shown to be close to linear  rather than the worse-case quadratic expansion in the number of nodes  finally  we note that the pruned edges have little effect on the location inference error rate  less than 1 % difference between no pruning and pruning with the threshold 0.75   but may cause up to 8.2 % increase in error rate for containment inference  which is a small cost to pay if memory is scarce  d efficiency of data capture and inference e accuracy and data reduction of the output event stream we next evaluate the efficiency of our system in both memory usage and processing speed  to do so  we used a high pallet injection rate of 1 every 4 seconds  900 pallets per hour   the tests were performed on a linux server with intel 2.33ghz xeon cpu and 8gb memory running jvm 1.6.0 the maximum java allocation pool size was set to 5.5gb  processing speed  table iii reports the processing time for graphs of different sizes  with the increasing graph size shown in the first column of the table  the cost of graph update for all active readers and the cost of inference on the graph in each epoch  which is a second  are reported in the second and third columns  as can be seen  both the update and inference costs are less than a second  with the inference being the dominant cost  the total cost is reported in the last column  as is shown  the total cost is 0.76 second for the largest graph size  which uses an already high injection rate of one pallet every 4 seconds  these results show that our data capture and inference techniques can keep up with high-volume rfid streams  a key requirement of rfid monitoring and tracking  after data inference  our system translates inference results into output events using level 1 or level 2 compression  section v   we next compare the final output of our system against smurf  11   a state-of-the-art rfid data cleaning system  smurf applies smoothing with an adaptive window to mitigate the missed reading problem  to enable a comparison to our output  we extend smurf as follows   i  we use the static reader locations to estimate object locations as readings are smoothed in   ii  if an object is observed by multiple readers  we compute the strength of each reader as the percentage of readings in its current window and report the location of the reader with the highest strength as the objects location   iii  we finally apply level 1 compression to produce a compressed event stream  smurf does not support containment inference or level 2 compression  which is unique to our system  for these reasons  we only consider object location events in the output when compared to smurf  accuracy of output events  our accuracy metric for the output stream is event-based  for each event in the output  we 13 1200 1000 800 600 400 100 100 90 80 80 70 smurf inference  100 % cont  inference  50 % cont  inference  0 % cont  60 200 0 50 0 50 100 150 200 250 node count  k   a  fig  10  300 350 400 % of original file size graph size  mb  1400 f-measure  %  threshold 0 0.25 0.5 0.75 1600 0.5 0.6 0.7 0.8 0.9 1 inference+l2  both  inference+l1  both  inference+l2  loc only  inference+l1  loc only  60 40 20 0 0.5 0.6 0.7 0.8 read rate read rate  b   c  0.9 1 memory test and evaluation of the output event stream   a  memory usage   b  event error rate   c  compressed location and containment output  determine if it is present in a compressed event stream of the ground truth  we use precision to capture the percentage of returned events that exist in the ground truth stream  and recall to capture the percentage of events in the ground truth stream that are returned in our output  we combine them into the metric f-measure = 2*precision*recall/  precision+recall   fig  10  b  compares smurf and our system in f-measure for location inference  since the compression technique does not affect accuracy  we use level 2 compression here  the two solid lines in the figure show that our system outperforms smurf by more than 10 % in both the low and high read rate ranges  in the low range  this is largely due to smurf being simply a smoothing technique  it can smooth in readings in certain cases when an object is missed by its reader  however  given several consecutive missed readings  it tends to believe that this object has moved away from its location  in contrast  our system exploits stable containment  i.e  using the location of the container or contained objects to infer this objects location  thereby overcoming the problem of consecutive missed readings  as the read rate approaches 100 %  the accuracy of smurf remains below 90 %  this is due to the de-duplication process employed by smurf  when an object arrives at a new location  its adaptive window for smoothing starts small  if it is observed by multiple readers  there is a good chance that the strength computed from the windows for these readers is all the same  then the choice between these readers is random  hence causing the errors  furthermore  we consider the cases where only a percentage  e.g  50 % and 0 %  of objects are placed in a container  hence permitting limited benefits of using the containment information in location inference  in the extreme case of 0 % containment  which is rare in practice   smurf outperforms our inference in the range  0.6  0.8  because without containment information  smurfs dynamic window technique is more adaptive than our location inference technique that records the last n observed colors and employs color fading  however  given larger read rates  > 0.8   smurf produces more inference errors than ours because its dynamic window produces more false positives than our technique and its deduplication process causes errors  the curve for 50 % containment lies in between those for 100 % and 0 % containment  compression ratio  to study the effect of compression  we measure the size of a compressed event output against the size of the initial input of raw rfid readings  i.e  compression ratio   fig  10  c  shows results for two output formats  first  we only include location information in the output  shown by the two dashed lines in fig  10  c    since our system supports containment inference  besides level 1 compression it can also apply level 2 compression to suppress location updates of contained objects  from the figure we can see that level 2 compression offers a greater reduction in output size than level 1 for almost all read rates  resulting in a compression ratio of 10 % when the read rate exceeds 0.8 we note that smurfs output size is always larger than level 2 compression  hence omitted in the graph for readability  second  we further include the containment information in the output  shown by the two solid lines in the figure   we again observe that level 2 compression significantly outperforms level 1 compression  it is also interesting to see that including both location and containment information using level 2 compression yields less output than including only location information using level 1 compression  it takes less than 20 % of the raw input data size to include such rich information when the read rate reaches 80 %  vii  r elated w ork rfid stream processing  several techniques have been proposed recently to clean noisy rfid data streams  10    6    11    20   the most relevant to our work is the hifi system  10    6   which performs per-tag smoothing using the smurf algorithm  11  and multi-tag aggregation  but does not capture containment relationships between objects or estimate object locations via containment  we have experimentally demonstrated the benefits of our techniques over smurf  our prior research considered the use of a single mobile reader to scan objects repeatedly from different angles and distances  and developed inference techniques to derive precise object locations  21   our work presented in this paper focuses on a network of static readers and infers both object location and containment relationships  other research on probabilistic rfid query processing has focused on the architectural design  12  or event pattern detection  13   but has not addressed combined location and containment inference  since our system produces an event stream with rich location and containment information  we can feed our output stream to probabilistic query processing to derive useful high-level information  rfid databases  general rfid data management issues including inference are discussed in  22   siemens rfid 14 middleware  15  uses application rules to archive rfid data streams into databases  the cascadia system  23  offers an infrastructure for specifying event patterns  extracting events from raw rfid data  and storing them into a database  insides rfid databases  advanced techniques are available to integrate data cleansing with query processing  24   to recover highlevel information from incomplete  noisy data by exploiting known constraints and prior statistical knowledge  25   and to use effective path encoding schemes to answer tracking queries and path oriented queries  furthermore  effective compression is available through the use of disk-based sorting and summarization operations  14   these techniques  however  are not designed for fast low-level inference and compression of raw rfid streams  furthermore  none of them supports containment inference or has demonstrated performance for inference over high volume rfid streams  sensor data management  recent work on gps sensor readings is related to our work since it supports user-defined views using model-based probabilistic inference  26   however  gps data differs from rfid data because it already reveals object locations and gps applications are not concerned object containment relationships  viii  c onclusions in this paper  we presented a novel data inference and compression substrate over rfid streams to address the challenges of incomplete data  insufficient information and high volumes  our substrate employs a time-varying graph model to capture inter-object relationships such as containment  and employs a probabilistic inference algorithm to determine the most likely location and containment for each object  our results showed that our techniques achieve error rates below 15 % for location estimates for a wide range of rfid read rates  and within 20 % for containment estimates when the read rate reaches 80 %  for future work  we plan to extend our inference and compression substrate to a mix of static and mobile readers and to handle query processing in distributed environments  acknowledgements this work has been supported in part by nsf grants iis0746939  iis-0812347  cns-0923313  and by a grant from the chinese nsfc-jst major international  regional  joint research project under grant no  60720106001 the authors would like to thank the anonymous reviewers for their helpful feedback  r eferences  1  s garfinkel and b rosenberg  eds  rfid  applications  security  and privacy  addison-wesley  2005   2  y yao and j gehrke  query processing in sensor networks  in cidr  2003   3  s madden  m j franklin  j m hellerstein  and w hong  the design of an acquisitional query processor for sensor networks  in sigmod  2003  pp  491502   4  a deshpande  c guestrin  s madden  j m hellerstein  and w hong  model-driven data acquisition in sensor networks  in vldb  2004  pp  588599   5  b feder  despite wal-marts edict  radio tags will take time  http  //www.epcglobalinc.org/  dec 2004   6  s r jeffery  g alonso  m j franklin  w hong  and j widom  declarative support for sensor data cleaning  in pervasive  2006  pp  83100   7  k finkenzeller  rfid handbook  radio frequency identification fundamentals and applications  john wiley and sons  1999   8  c floerkemeier and m lampe  issues with rfid usage in ubiquitous computing applications  in pervasive  2004  pp  188193   9  b violino  rfid opportunities and challenges  http  //www.rfidjournal  com/article/articleview/537   10  m j franklin  s r jeffery  s krishnamurthy  f reiss  s rizvi  e wu  o cooper  a edakkunni  and w hong  design considerations for high fan-in systems  the hifi approach  in cidr  2005  pp  290304   11  s r jeffery  m n garofalakis  and m j franklin  adaptive cleaning for rfid data streams  in vldb  2006  pp  163174   12  m n garofalakis  k p brown  m j franklin  j m hellerstein  d z wang  e michelakis  l tancau  e w 0002  s r jeffery  and r aipperspach  probabilistic data management for pervasive computing  the data furnace project  ieee data eng  bull  vol  29  no  1  pp  5763  2006   13  c re  j letchner  m balazinska  and d suciu  event queries on correlated probabilistic streams  in sigmod  2008  pp  715728   14  h gonzalez  j han  x li  and d klabjan  warehousing and analyzing massive rfid data sets  in icde  2006  p 83   15  f wang and p liu  temporal management of rfid data  in vldb  2005  pp  11281139   16  epcglobal inc  epcglobal tag data standards version 1.3 http  //www  epcglobalinc.org/  mar 2006   17  y nie  r cocci  y diao  and p shenoy  spire  efficient data interpretation and compression over rfid streams  department of computer science  university of massachusetts amherst  tech  rep  2009   online   available  http  //spire.cs.umass.edu/pubs/tkde-all.pdf  18  r s barga  j goldstein  m h ali  and m hong  consistent streaming through time  a vision for event stream processing  in cidr  2007  pp  363374   19  w m white  m riedewald  j gehrke  and a j demers  what is next in event processing ? in pods  2007  pp  263272   20  n khoussainova  m balazinska  and d suciu  towards correcting input data errors probabilistically using integrity constraints  in mobide  2006  pp  4350   21  t tran  c sutton  r cocci  y nie  y diao  and p shenoy  probabilistic inference over rfid streams in mobile environments  in icde  2009  pp  10961107   22  s s chawathe  v krishnamurthy  s ramachandran  and s e sarma  managing rfid data  in vldb  2004  pp  11891195   23  e welbourne  n khoussainova  j letchner  y li  m balazinska  g borriello  and d suciu  cascadia  a system for specifying  detecting  and managing rfid events  in mobisys  2008  pp  281294   24  j rao  s doraiswamy  h thakkar  and l s colby  a deferred cleansing method for rfid data analytics  in vldb 06  proceedings of the 32nd international conference on very large data bases  vldb endowment  2006  pp  175186   25  j xie  j yang  y chen  h wang  and p s yu  a sampling-based approach to information recovery  in icde  2008  pp  476485   26  b kanagal and a deshpande  online filtering  smoothing and probabilistic modeling of streaming data  in icde  2008  pp  11601169 yanming nie received the b.s  degree and the m.s  degree in mechanical engineering from northwest agricultural and forestry university  china in 1993 and 1996  respectively  he is currently pursuing the ph.d degree in the school of computer science at northwestern polytechnical university  china  he was a visiting student at the university of massachusetts amherst from august 2007 to september 2008 his research interests include data stream processing  uncertain data management  and rfid data management  15 richard cocci received the b.s  degree and the m.s  degree in computer science from the university of massachusetts amherst in 2006 and 2008  respectively  his research interests lie in rfid data management  he is presently a project manager at state street corporation in boston  usa  zhao cao received the b.s  degree in computer science from beijing institute of technology  china  in 2004 he is currently pursuing a ph.d in computer science at beijing institute of technology  he was a visiting student at the university of massachusetts  amherst from september 2008 to march 2010 his research interests are broadly in the areas of peer-topeer networks  sensor networks and distributed data management  yanlei diao received the bachelors degree in computer science from fudan university in china in 1998  the m.phil degree from the hong kong university of science and technology in 2000  and the ph.d degree in computer science from the university of california  berkeley in 2005 she is an assistant professor in the department of computer science at the university of massachusetts  her research interests are in information architectures and data management systems  with a focus on data streams  sensor data management  uncertain data management  large-scale data analysis  and flash memory databases  dr diao has been the recipient of the national science foundation career award  the finalist for the microsoft research new faculty fellowship  and the recipient of the ibm scalable innovation faculty award  she spoke at the distinguished faculty lecture series at the university of texas at austin in december 2005 her phd dissertation won the 2006 acm-sigmod dissertation award honorable mention  prashant shenoy received the b.tech degree in computer science and engineering from the indian institute of technology  bombay  in 1993  and the m.s and ph.d degrees in computer science from the university of texas  austin  in 1994 and 1998  respectively  he is currently a professor of computer science at the university of massachusetts  his research interests are in operating and distributed systems  sensor networks  internet systems and multimedia  dr shenoy has been the recipient of the national science foundation career award  the ibm faculty development award  the lilly foundation teaching fellowship  the ut computer science best dissertation award  and an iit silver medal  he is a distinguished member of the acm and a senior member of the ieee  