chapter-1 introduction 1 1 introduction in the present day world for all our queries  our homework etc we have become dependent on the use search engines  our demands do not stop with direct answers we want faster response times  related results etc  users are increasingly interested in getting related documents and finding their different questions answered instead of just being given a keyword based boolean retrieval based result  to support better retrieval  researchers are turning towards the semantic web  where the potentials for answering such queries are high  to achieve this  representation of semantic information is of primary importance  knowledge representations are used as a backbone for the semantic web  among the different data structures used for knowledge representation  graphs are being widely used to represent knowledge because of its simplicity  flexibility  understandability  and expressivity  e.g  if two nodes are linked in a graph then it means that the nodes are related   in these graphs each node contains the information about a data entry and the edges represent a relationship between two data entries  data entries are usually key terms which are specific to a domain  relationships between data entries capture the semantic connectedness between them  two terms are semantically related if they are terms describing a common topic  or if they indicate a topic-subtopic relation  the relationships can be defined at different domain granularities  for instance  two terms may be related under the domain computer science but may or may not be connected under the domain `data structures '  the power of the graph in terms of its knowledge expressivity is dependent on the accuracy and comprehensiveness of the relations  graphs that provide information about the type and nature of a relationship between two nodes are termed as concept graphs  one approach that has been widely used to represent knowledge is ontologies  ontology represents knowledge as a set of concepts  and also provides the relationships between these concepts  but they are quite a few disadvantages with this presently existing technique of concept graphs  2 one of the main disadvantages with it is that relationships in the ontology are a result of the views of different people involved in the domain  different views of people make any general purpose ontology impossible  in addition this involves more manual intervention  which is impractical for creating concept graphs from large data sets  the other approach is to use statistical techniques to generate graphs  they use limited knowledge and employ no pre-defined assumptions to produce a graph capturing certain concepts and relations between these concepts  however  statistically generated graphs while being faster and easier to generate  lack the expressivity of ontologies  the primary reason being  currently there are no known techniques to analyze these graphs for semantic relationships  between the statistical graphs that provide less information and the concept graphs that ca n't be generalized and require human intervention the goal is to strike a balance between them  statistically generated graphs can be used as a foundation  where relationships that are defined ontologically may be incorporated later  statistical techniques can help generate concept graphs by easily identifying co-occurring terms in documents  and their proximity to each other  these features can help provide high level semantic insights  using natural language processing over the generated graph  generating ontologies will become easier and involve lesser manual intervention  towards this goal  we have explored existing statistical representations  cooccurrence graphs  4   word distance graphs  2  for generating concept graphs  we also integrate these two graphs to generate a hybrid representation  the primary contribution of this project is to analyze the three representations for semantic properties  so that we can determine the power of such statistically generated graphs for expressivity  such studies have not been done so far on these graphs to the best of our knowledge  and are an essential exercise to use the power of these easily able to generate graphs  we first study explicit graph theoretic properties like edge-weight  betweenness  degree etc for semantic meaning  next  we apply graph algorithms to these graphs to analyze clustering properties  hierarchies  etc  3 in addition we study the efficiency  scalability  and reliability of these different graphs over different data sets  and spot the advantages/disadvantages of each method  finally we develop a guess based tool to support the concept graph creation and analysis  the guess based tool helps study both the explicit graph properties and supports functions for clustering  identifying cluster to cluster link  related terms for a search term  level based search  clustering  and minimum spanning tree generation  the next section will explain the literature survey and motivate the need for this work  4 chapter 2 literature survey 5 2 literature survey the value of time is increasing especially in the field of searching  '' the faster the better '' becomes our motto but it does not give us the authority to allow unwanted results  hence a statistically created concept graph to speed up search and to allow deeper term relationships seems to be the way out  the author of  1  tries to create a fast and adaptive layout algorithm to visualize graph layouts  the author uses the spring-embedded paradigm and introduces several new heuristics to improve factors like the convergence  local temperatures  gravitational forces and the detection of rotations and oscillations  the author believes that the proposed algorithm achieves visualizations of high quality on a wide range of graphs on standard settings  the algorithm used seems to reduce time  thus it may also be applicable on general undirected graphs of substantially larger size  our project uses the gem layout as one of the visualizations to help extract semantic data  the author also provides proof for the fact that it is indeed faster than kamada kawaii and fruchterman-reingold algorithms  the authors of  2  try and understand the framework behind the term occurrence graph  the author takes into account all the past notions of distance playing a factor in the information retrieval of different graphs  the authors compare different works that use distance based relevance calculations and try to bring up a theoretical framework  he explains the importance of term distance which even now is one of the key factors in information retrieval  for our project we have also created a graph using term distance and have been able to see quite a few relations and clusters being formed in it which proves the efficiency of the graph  the authors relate to the problem of graph drawing or graph layout where there are a set of nodes and the edge weight between them  they have taken the eades and modified it to provide better results .this layout is now known as the fruchterman6 reingold layout  they have mapped the graph system with physical forces to create a heuristic to simplify graph visualization  they assume the graph to be like that of the universe or a molecule where there are both attractive and repulsive forces  these graphs are commonly known as force directed graphs  the author was able to model it and compare it with the previous models of kamada kawaii and eades which provides good results  this is one of the few layout visualizations used for our project to understand simple statistical graphs  3   the key graph is the name the authors of  4  have given their co-occurrence graph  it is a graph created based on the co-occurrence of two or more words in a collection of documents  the algorithm is based on the segmentation of a graph  representing the co-occurrence between terms in a document  into clusters  each cluster corresponds to a concept on which author 's idea is based  and top ranked terms by a statistic based on each term 's relationship to these clusters are selected as keywords  this is another statistical graph that we have taken into account in our semantic pattern search  the author of  5  speaks of ways of improving the already present association algorithms such as apriori and concept lattice  the transitive path of association relation is discovered  by building a cyclic graph model  the keywords are divided into document keywords  domain keywords  the initial phase is to discover semantic relations such as `` if a then b ''  using association relations between two keywords semantic relations are determined using their semantic degree which is intersection of the two keywords by the union of them  ex  -if a and b are two keywords and if a is associated with  c  d   and b with  e  f  the ab will be nothing or `0 ' and aub will be  c  d  e  f  if =n  ab  /n  aub   is the semantic degree  then =0 which in this case would imply that a and b are not related   n implies the number of items   then a semantic matrix which will be an nxn matrix of the keywords and their 7 relations determined above will be created  different properties of the relations are studied like transitivity etc  the author says that it becomes hard to extract long length transitive paths between documents using the document level discovery algorithm and that the number of documents influences the transitive path  the author of  6  speaks of a higher studies degree map  the map is to relate professional profiles  subjects required for the profile  and competencies required for subjects and acquired from subjects and the relationship between these  the author proposes a concept map wherein each subject with be mapped with its previous competencies and what competencies it contributes to  the mapping helps explain precedence and dependency relationships  graphviz is the tool used  the relationships between entities are stored in the database which the graphviz tool uses to generate the map  the problems faced were adding new relations to the system which had to done directly to the database  adding an extra entity becomes impossible  the authors of  7  say that the apriori algorithm is used for item set mining and association rule generation  it is used on databases that contain transactions  it is a bottom-up approach  first it identifies frequent individual items in the database and then extends to larger item sets provided these sets appear often in the db  the item sets are then used to determine association rules  breadth-first search and hash tree are used to count the candidate sets  item sets that occur frequently  they can also contain only one element   apriori property says that a subset of any item set that is frequent must also be frequent .e.g    if  a  b  is frequent the  a  should be frequent and  b  should be frequent.if k is the number of items in a set then any  k-1  item set that is not frequent can not be a subset of the frequent k-item set  various approaches have been employed by systems in past years to develop techniques/methods that aid in the extraction of relationship from concept graphs  the 8 essential software tool for our project is the open source graph visualization software  guess  which helps visualize graphs in different layouts  our project employs statistical methods to extract information from concept graphs that are visualized by considering the nodes as keywords and the edges representing the relation between them in statistic terms  the existing approaches that aids the statistical approach we used is discussed in this section  the two major statistical approaches employed across all systems used for knowledge representation are co-occurrence  4  and word-distance  2   co-occurrence is based on the occurrence of two or more words together in multiple documents  using this technique yukio  4  came with a co-occurrence graph  it is a graph created based on the co-occurrence of two or more words in a collection of documents  the algorithm  key-graph  is based on the segmentation of a graph  representing the co-occurrence between terms in a document  into clusters  each cluster corresponds to a concept on which author 's idea is based  and top ranked terms by a statistic based on each term 's relationship to these clusters are selected as keywords  this strategy comes from con-considering that a document is constructed like a building expressing new ideas based on traditional concepts  since scanning through entire set of documents is time consuming the author employs a small set of terms to serve as representatives for the entire set of documents  this indexing process reduces the human effort in sifting through vast amounts of information  these keywords are used as a basis not for only for information retrieval but also as an abstract for a document  the terms that represents the view of the document is considered as keyword by yukio  4   the author compares his key-graph method to that of a construction of building as a metaphor this building has foundations  statements for preparing basic concepts   walls  doors and windows  ornamentation   but  after all  the roofs  main ideas in the document   without which the building 's inhabitants can not be protected against rains 9 or sunshine  are the most important  here the authors refer to the keywords as the roofs as they are the main terms in the document  co-occurrence of two or more terms is checked across multiple documents with help of a count  so more the count of the terms occurring together more they are related is what could be observed from the authors point of view  but co-occurrence alone can not give the exact semantic relation out of a concept graph  the authors of  2  try and understand the framework behind the term occurrence graph  the author takes into account all the past notions of distance playing a factor in the information retrieval of different graphs  the major benefit of this approach is that relevance scoring does not rely on collection frequency statistics and that distance between the terms as a primary estimate  the author proposes that in addition to document relevance  distance measures have application in identifying phrases with a greater degree of tolerance for word substitutions and unexpected intervening words  the span length i.e  the length between two words is estimated using two possibilities here 1 the length of the intervening sequence 2 the sum of the length of the interval spans both of these approaches have their pros and cons  though co-occurrence is able to find out the related terms across multiple documents  the terms that do n't span across multiple documents and are related are omitted by this approach  the author of  2  seems to achieve the outcome of getting relations between terms but the word distance approach fails in certain cases  for example in index pages of different books unrelated terms may occur next to each other  so word distance will be taken as 1 which shows maximum relationship but they need not necessarily be related  for our project we have considered both of these statistical approaches and tried 10 combining these approaches to get the precise relation between terms in a document  we came up with slightly different approach of co-occurrence by considering an adjacency matrix with rows and columns as keywords and the matrix is updated each time the keyword pair gets a hit in the file  we have also created a graph using term distance and have been able to see quite a few relations and clusters being formed in it which proves the efficiency of the graph  the same matrix approach has been followed in this approach  the minimum distance between keywords is found and the matrix is updated  after analyzing the pros and cons of both the approaches  to help understand the graph better we came up with minimum spanning tree that consists of mostly important edges which was done by removing all unwanted edges based on the edge weight criteria  11 chapter 3 design issues 12 3 design issues this chapter includes the problem statement which is considered to develop this project ; it also deals with the data sets taken as the input file for the various implementations in the code and algorithms  a few objectives of this module along with the assumptions made are following  3.1 problem statement to study statistical and graph theoretical approaches for generating and analyzing concept graphs  and to develop a guess based tool for supporting this  3.2 objectives the primary goal of this work is to study simple statistical approaches inorder to reveal the potential expressivity of such graphs  this can help generate concept graphs with less complexity  while capturing the semantic features necessary for current applications  to do so  a comprehensive understanding of the graphs generated using the above said methods by observing key factors like betweenness  edge weight  degree  and cut edge etc to analyze for semantic features is essential  once these factors explicit to any graph are analyzed  the goal is to apply algorithms like clustering and spanning tree generation that can reveal more insights into the graphs like topic based clusters  hierarchical clusters  topic subtopic hierarchies  links between classes etc  13 13 chapter  4 system design 14 14 4 system design the aim of the project is to design a system that automates the generation and analysis of graphs that represent knowledge extracted from documents containing data related to different domains  although there are various existing statistical approaches for generating concept graphs and extracting information from it  they have not been studied adequately for semantic patterns  our project combines the current graph generating methods and tries to create graphs that has terms as nodes  depicts the type of relations between these nodes  analyzes it using different properties  forms clusters using these relationships and also describes the relationship between different clusters  this helps us to capture the semantic information with the combined statistical approach and is more efficient  4.1 proposed system architecture relations graph  list of keywords system  extracts relations between keywords  query suggestions clusters sub clusters set of documents relation between clusters fig.4.1.system architecture 15 15 inputs for this system are list of keywords and set of documents  words from indices of different textbooks are taken as list keywords  documents are taken from wikipedia pages and different textbooks in corresponding domain  4.2 different approaches for graph generation the extraction of relations between keywords was done with the help of existing statistical approaches with a slight variation in the algorithm  4.2.1 co-occurrence method  co-occurrence is a graph created based on the co-occurrence of two or more words in a collection of documents  in our approach we came up with an adjacency matrix that contains keywords as rows and columns  every time a pair of keywords gets a hit in the document the matrix is updated  each cell in the final matrix depicts the relation between the corresponding pair of nodes in statistical terms  the greater the value of the cell in the matrix the higher the relatedness value of the corresponding nodes  algorithm for co-occurrence  input set  master file contains file names and set of keywords corresponding to each file  key file-file containing set of all keywords fp file pointer to master file wlist containing the keywords of corresponding file w1  w2pointers to the keywords in the file fn file pointer to the key file size no of keys in the key file array  size   size  matrix of length size size i  j variables function  fp  fn  array  size   for i in size  for j in size  do array  i   j  0  j j+1 ii+1 for w1 in w  for w2 in w  if w1 ! =w2  do array  w1   w2  array  w1   w2  +1 16 16 4.2.2 word distance method another statistical approach we used is word distance  as the term indicates it is the distance between two or more words across multiple documents  the same adjacency matrix approach is followed here  a time line of keywords is created according to the order in which the keywords occur in a document and the distance between adjacent terms and every keyword to another is updated in the matrix  the process is done for every document and each time the average value of the distance between the two terms is updated in the matrix  algorithm for word distance  input set  master file contains file names and set of keywords corresponding to each file  key file-file containing set of all keywords fpfile pointer to the master file fnfile pointer to the key file index     list of indexes of each keyword present in the corresponding file sizeno of keys in the key file wlist containing keywords of corresponding files array  size   size  matrix of length sizesize m  i  j  n variables used mina minimum value is set as a base line function  fp  fn  index  size  array   for i in 0 to size  for j in 0 to size  do array  i   j  0  jj+1 ii+1 n0 m0 for k in 0 to len  w   for l in k+1 to len  w-1   while m < len  index  i   and n < len  index  j    if abs  index  i   m  -index  j   n   < min  do min=abs  index  i   m  index  j   n   if index  i   m  < index  j   n   do m=m+1 else  17 17 4.2.3 combination of co-occurrence and word distance the analysis of the above two approaches was done for two different data sets and it was found that both approaches had their own pros and cons  we used a hybrid of the two approaches and came up with a graph which helped in retrieving relation in an efficient manner  algorithm for hybrid approach  input set  key filefile containing set of all keywords occur   list containing co-occurrence values of keywords word   list containing word distance values of keywords wordlist containing the keywords of corresponding file w1  w2pointers to the keywords in the file i  j  valuevariables fpfile pointer to the keyfile sizeno of keys in the key file array  size   size  a matrix of length size size function  fp  array  size  col  wd   for i in 0 to size  for j in o to size  array  i   j  0 for w1 in word  for w2 in word  if w1 ! =w2  array  i  4.3 different semantic parameters used to analyze the graphs  the analysis of the above two approaches were carried out with the help of few 18 18 parameters that brought out the semantic relations from the graphs  the parameters are as follows  degree  the number of links incident up on a node  it defines how many connections end up on a node or the edges that are directed towards the node  a high degree usually denotes that a node is potentially the key topic of a cluster  betweenness  quantifies the number of times a node acts as a bridge along the shortest path between two other nodes and this parameter helps in finding out the connection existing along two nodes with less edges on the path  betweenness along with degree identify the important terms in a graph  edge weight  it is one of the important parameter for analysis  the above two approaches are related to this parameter as it helps in related the nodes based on its value  it quantifies the relation between two nodes  that is  how semantically related the two nodes are  cut edge  it is an edge that connects two different topic clusters  the link or the edge that connects the two clusters usually indicates a pair of terms that are not closely related  but form links to different topics  we analyze these properties over the three types of graphs and identify the patterns which indicate specific semantic properties  4.4 extraction and analysis of semantic clusters though the above approaches were able to retrieve relations from the corresponding graphs  the study of these graphs requires lot of effort  since the nodes/terms are 19 19 connected to each other and formed a structure similar to that of a mesh network  it became a tedious task to analyze and experiment with the graphs  so we design an approach that can give us the hierarchical view of these graphs which will make analyzing and searching faster  a minimum spanning tree is constructed for the connected weight graph with the kruskal 's algorithm  this gave us a hierarchical view of the graphs for both of the approaches helping us to better understand the relationships existing between nodes and for analyzing individual clusters  and their relationships with each other  a minimum spanning tree or a maximum spanning for the co-occurrence graph  removes the weak links between terms  thereby clustering the closely related terms  in addition this captures the topic-subtopic relationships between terms  for clustering the graph  the minimum spanning tree is taken  and cut edges are identified based on the weakest links in the tree  clustering can be done at different levels  once a topic cluster is identified  it can be further clustered for subtopics  we use these clusters to identify related terms  terms serving as bridges between topics etc  to identify cluster we develop a greedy approach over the minimum spanning tree  an edge connecting two clusters connects vertices having high degree and betweenness  and its cost is high  word distance graph   or low for co-occurrence graph   analyzing the different properties like betweenness and edge weight of the spanning tree we noticed that we can retrieve different types of relationship between keywords or terms in the clusters  the different types of relationship include topic-subtopic relation  inter-topic link and intra-topic relation  these algorithms for finding these relationships have been implemented and integrated into an application which is an extension of guess  the interface of this application has been done in jython  this application not only helps us to retrieve the different kinds of relations but also helps in the analysis of these graphs  details of this application along with the rest of 20 20 the implementation details and analysis will be discussed in the next section  21 21 chapter-5 implementation  analysis and result 22 22 5 implementation  analysis and result in this section we analyze our three graphs  by first analyzing the overall graph  then their properties like edge weight  they we analyze these graphs by extracting domain classes and subclasses using our clustering algorithm  we also analyze the topic subtopic hierarchies in these graphs by using the minimum spanning tree generated over these graphs  finally we discuss the guess based extension and show how it provides functionalities to effectively study concept graph for potential applications  5.1 analysis of the co-occurrence graph fig.5.1  co-occurrence graph fig 5.1 shows a sample co-occurrence graph where nodes are terms in engineering  and edges connect terms occurring in the same document  each edge in co-occurrence 23 tells how related two key words are  edge weight of co-occurrence is the number of times both the key words occurred together  more the value of edge weight more related the key words are  by visualizing this graph using the kamada kawai graph visualization technique  2   we can observe that the terms belonging to the same topic are closer in the visualization  this demonstrates that a simple co-occurrence graph can effectively capture semantic similarity among terms  given this observation  we analyze this graph further for semantic information  5.1.1 analysis of edge weight fig.5.2 edges with weight > 80 % maximum in a co-occurrence graph  the large the edge weight  the closer the terms  since it indicates that these terms occur together in many documents  potentially indicating a topic similarity or relatedness  however since edge weights vary based on data set and size of data set  we perform an analysis to determine a common metric that can define the minimum edge weight which links two nodes which belong to a same topic  and are hence closely related  we do this by adding the edges in the graph in decreasing order of weight  till we reach a point when they do not play a role in intra24 topic relationships  as edges of a certain weight category are added  we observe topic based clusters being formed automatically  edge weight > 80 % of maximum edge weight  when edges with weight greater than 80  fig.5.2  were analyzed it has been observed that the edges of the nodes which are more related being selected  internet and computer networks belonging to the network cluster being displayed is one such example  many such cases have been found where nodes with more edge weight are being displayed related very closely to each other  few of the cases are binary tree and heap  hash and tries etc  these belong to the cluster data structures and algorithms  this analysis shows that the relation between two nodes or keywords is directly proportional to their edge weight  the more the edge weight between two nodes the more they are related  we observe that in general  the edges indicate that the connected terms have a generalization-specialization relationship  or are categories under the same parent types  edge weight > 40 % of maximum edge weight  when edges having weight greater than 40 % of maximum edge weight  the edges pertaining to the network clusters were displayed high in numbers leading to the fact that there are large number of keywords in network domain and that every node in the network cluster is related to at least another node in the cluster in one way or the other  to analyze this further let us take for example the edge between ethernet and lan  the term lan defines a network that works over small distance and the term ethernet defines a type of networking protocol used on lan  their semantic relationship is not that of similar terms describing same topic  or subtopics under the same topic  however here the relationships captured are of those between topics and subtopics like `uses '  `is a medium of '  `similar to ' etc  ethernet and internet  television and radio  radio and wireless are some examples that were found  25 edge weight > 30 % of maximum edgeweight  fig.5.3  edges with weight > 30 % maximum when edge weight is further extended to include those with weight greater than 30 %  fig.5.3   more edges are added to the clusters  making them more defined  intercluster edges do not appear in this level yet  however they do not add much to the semantic relationships except contributing to cluster formation  the relationships defined by these edges are similar to the one above  the figure shows that clusters are being more clearly defined and strong connections amongst the nodes are formed  the left side cluster shows the cluster of computer networks  right side is the algorithm and data structure cluster and the middle one shows database cluster  the clusters are completely disconnected and no inter-cluster relations were observed  transaction and query  query and database  internet-protocols and ieee were some of the edges that were observed  26 edge weight > 20 % of maximum edge weight  fig.5.4  edges with weight > 20 % of maximum more intense clusters were formed when edge weight limit was made 20 %  as you could notice  in the graph  the cluster on the left side shows the networks  on the right is the cluster of algorithm and data-structure and on the middle is database cluster  at this level inter-cluster edges are appearing  for example  null  database  and binary tree  data-structures   null is a common term occurring in both database and data-structures  another example is ieee  networks  and transaction  database   so we can define the edge weight for edges connecting different clusters  and those that may potentially be cut edges as being in the range between 20-30 % of maximum edge weight in the graph  this also helps find terms that link two topics  and this is very useful in many information retrieval applications  27 edge weight > 5 % of maximum edge weight  as we keep analyzing we get to view new patterns being formed in the graphs  this analysis resulted in the formation of new edges like quick sort and shortest path algorithm  but edge weight being the factor it was found that the edges formed were less related and resulting in the formation of many unrelated edges  these edges were formed when they occurred in one document coincidently  hence edges below 20 % of maximum edge weight are not considered  5.1.2 conclusions from analysis  the foremost conclusion that could be drawn from the graphs is that higher the edge weight the more semantically related the terms  the closest connections were formed when top 80 % of the edge weights were taken  for example internet and networks  as the analysis was done by decreasing the edge weight  different relationships were identified till a point when the edges indicated relationship between different topics  the analysis of edge weight around 20 % gave edges with completely unrelated terms or related terms with less number of related documents  28 5.2 initial word-distance graph fig.5.5 word-distance graph fig 5.5 shows a sample word distance graph where nodes are terms in engineering  and edges connect terms occurring in the same document  each edge in word-distance tells how related two key words are  edge weight of word-distance is the number of words between two keywords  more the value of edge weight more distance between 29 the keywords and hence less related  by visualizing this graph using the gem graph visualization technique  1   we can observe that the terms belonging to the same topic are closer in the visualization  for example all the nodes related to algorithms like sorting algorithm  complexity are together  similarly networks database and fluids  this demonstrates that a simple word-distance graph can effectively capture semantic similarity among terms  given this observation  we analyze this graph further for semantic information  analysis of edge weight  in word-distance graph the edge weight is inversely proportional to the semantic relationship  ie the lesser the edge weight the closer the terms  on analysis of edge weight on word-distance graph we found that when edge weight is greater than 80 %  relations like speed and radio  speed and ethernet were present  we know that speed is not related to radio and ethernet  hence these links are weak  edge-weight less than 5 % may have terms present in index which are close but not related  for example worst-case complexity and database  hence maximum relations lie in the range of 5 % to 80 % of maximum edge-weight  30 5.3 hybrid graph  31 fig.5.6  hybrid graph co-occurrence graphs fail to capture the relationships between terms if their frequency of occurrence is low in multiple documents  word distance based graphs may not capture the relationships when indexes are used for building the graph  since in indexes terms that occur close together need not be related  in hybrid graph  fig.5.6  we combine the co-occurrence graph and word-distance graph by considering the co-occurrence of two terms over a fixed word distance  to determine the new edge metric  we combine the metrics generated over the two graphs as follows  let r  a  b  be the relationship between two terms a  b  c  a  b  be the coocurrence value of a  b and w  a  b  the word-distance value of a  b  the combined relation rc  a  b  is given by rc  a  b  = k*c  a  b  / w  a  b   where k is the proportionality constant  32 5.3.1 analysis of hybrid graph  t he following analysis explains the edges with maximum and minimum edge-weights  fig.5.7 edges with weight > 90 % of maximum edge-weight > 90 % of maximum edge-weight  when edges with weight greater than 90 were analyzed  fig.5.7  it was found that the edges displayed more related edges  edges connecting array  sparse array  suffix array  variable length array were present  we see that these edges belong to the same topic array  there were also edges connecting algorithm to dijkstra 's algorithm  kruskal 's algorithm  greedy algorithm etc  hence we see that there are less insignificant edges compared to word distance in top 90 %  edge-weight < 10 % of maximum edge-weight  on analysing the bottom 10 % edges we found that the relation represented by these edges are also related  edges connecting heap to binary heap  af-heap were displayed  there were also edges connecting algorithms to hill-climbing algorithm and edges linking candidate key foreign key  armstrong axioms etc  hence unlike cooccurrence the lower edge weights also has significant relations  on completion of the analysis it is observed that hybrid graph unlike word-distance and co-occurrence graph represent significant edges in all range of edge weight  it includes the important edges from co-occurrence and word distance graph  33 5.4 clustering the graph based on topic similarity clustering of a graph in general is partitioning the graph into smaller components each with its own specific properties  the partitioning is said to be good when the number of links/edges between two partitions is small  to obtain such partitioning in a graph we use heuristics and approximation algorithms  in case of clustering conceptual graphs each cluster becomes a topic and our goal is to determine which key term belongs to which topic  this structuring itself forms the primary relationship between topics and subtopics  the clustering mechanism can further be continued inside every cluster taking it to be a separate graph giving us multilevel clustering and relationships  at the graph level every node seems to be connected to most other nodes among which a lot of these link are weak or unimportant or incorrect  to cluster terms based 34 on their relation to a particular topic  most of the weak links have to be removed to eliminate inter-topic relationships  to do so we analyzed the graph and employed the minimum spanning tree which was most effective in removing the weak edges  in addition the minimum spanning tree is fast and easy to create  provide a better topic subtopic understanding  provide the terms that may relate one topic to another  and a tree based structure enable faster search than a graph  we used the kruskal 's algorithm to generate the graph  the minimum spanning tree gives us topic subtopic relationship  the figure depicts the minimum spanning tree formed  5.4.1 analysis of betweenness and degree  35 fig 5.8 analysis of degree and betweenness betweenness of a node gives the number of shortest path passing through a node  greater betweenness for a node implies more significant is the node  degree of a node is the number of nodes connected to the node  in fig.5.8 red nodes are the nodes with high betweenness and degree  green nodes are with less betweenness and blue nodes are nodes with high betweenness and less degree  by analyzing the degree and betweenness it was observed that nodes with degree and betweenness greater that 20 % of the maximum were general topics representing clusters  hence topic-subtopic relationships can be identified using these  36 5.5 guess based tool fig.5.9 guess based tool analysis and visualization of graph become easy when there is a tool for supporting these functions  we developed an application  fig.5.9  which is an extension of existing guess  open source tool for graph exploration   for this purpose  several functions including clustering  topic wise individual clustering  level wise search  path between two clusters are integrated with this tool  the algorithms are designed using analysis of graph theoretical properties like degree and betweenness  is jython these functions are explained in detail in next chapter  the platform used for development 37 chapter-6 potential applications of our graphs 38 6 application of the concept graphs this section shows the applications of the graphs we created and the information they provide  6.1 query suggestion on searching a term  related terms of the corresponding node will be displayed fig.6.1 query suggestion the user will be given the option to search for any specific term  when he enters the term in the text box provided there will be a list of related terms given as output  fig.6.1   these related terms are the nodes which are connected to the term given by the user through edges  for example the user gives a term data structures for search then terms like stack  pop and push are given as output  this application eases out the task of searching for the user  the user is provided with more choice to choose from  39 6.2 clustering on searching a term  the cluster to which the term belongs to will be displayed fig.6.2 identifying clusters if the user wants to know to which topic does the term he is searching for belongs to  then he needs to enter the term in the text box named as cluster then the cluster name will be displayed  fig.6.2   this is implemented in the following way  when the term is given as input it will point out towards its root node  traverse back to its parent node  and this process of traversing back is repeated until we reach the top most root node of the cluster and that root node will be displayed as the result which is indeed the cluster name  40 6.3 sub clusters the system identifies clusters  within clusters fig.6.3 identifying sub-clusters this application will help the user to know what the different levels are and sub clusters present in a cluster  it provides a concept of hierarchy to the user  the user also gets to know about how a certain term is related to other terms and how important or generalized is that particular term which he is looking for  in this all the different levels present in a cluster are highlighted using different colors  fig.6.3   the user can get to know to which level does the term he is searching for belongs to and how many levels are present in a particular cluster  6.4 relation between two nodes relation or path between nodes and clusters were identified 41 fig.6.4 identifying path between nodes this application helps the user to know the path through which two nodes are connected to each other  the nodes can be present anywhere in the graph i.e they may be present in two different clusters or within the same cluster etc the path through which the two nodes are connected is highlighted using a red color line  fig.6.4   the line passes through a sequence of many intermediate nodes because of this the user will get to know what are the nodes present in the path and how they are connecting the two terms  42 chapter-7 conclusion 43 7 conclusion after going through the results of the experiment we have come to the conclusion that a perfect statistical graph with 100 % accurate relationships is not possible but it is possible to use these graphs to help generate the ontology/semantic graphs  to do so we need to improve the relations the statistical graphs display and extract certain conceptual information that lay dormant in the graphs  our method of generation of graphs seems to be much more efficient than the currently existing techniques like manual generation and automatic generation using predefined relationships  since our method needs no predefined knowledge and can be used for any domain of any size given the right data set  it already becomes a step up than the other methods  here we use some standard statistical techniques  so there is no need for manual intervention and also unlike manual generation the problem of `` different people having different interpretation '' is non-existent  our approach works well for large data sets with high standards of accuracy  our method of automatic generation of concept graphs finds applications in various domains such as a data structure for various internet applications  for query suggestion etc  44 references  1  arne frick  andreas ludwig  heiko mehldau  `` monitoring a fast adaptive layout algorithm for undirected graphs ''  universitat karlsruhe  fakultat fur informatik  d-76128 karlsruhe  germany  2  david hawking and paul thistlewaite  `` relevance weighting using distance between term occurrences ''  the australian national university  august 1996  3  thomas m j fruchterman and edward m reingold  `` graph drawing by forcedirected placement ''  university of illinois at urbana-champaign  `` software  practice and experience ''  vol  21  november 1991  4  yukio ohsawa  nels e benson and masahiko yachida  `` keygraph  automatic indexing by co-occurrence graphbased on building construction metaphor ''  graduate school of engineering science osaka university  5  xiangfeng luo  kai yan and xue chen  '' automatic discovery of semantic relations based on association rule ''  journal of software  vol  3  no  8  november 2008  6  miguel riesco  marin d fondn  daro lvarez  `` designing degrees  generating concept maps for the description of relationships between subjects ''  concept mapping  connecting educators proc  of the third int  conference on concept mapping a j caas  p reiska  m hlberg & j d novak  eds  tallinn  estonia & helsinki  finland 2008  7  rakesh agrawal  ramakrishnan srikant  `` fast algorithms for mining association rules '' proceedings of the 20th international conference on very large data bases  vldb  pages 487-499  santiago  chile  september 1994 appendix 45 software details under the following head  language with version python 2.7  jython operating system with version ubuntu 12.04 graph visualization guess graph visualization development environment python 2.7 idle installing guess prerequisites you will need to  1 download and install python  2 download and unzip guess graph exploration from guess official website  3 change and give executable permission of guess.sh file 4 run the guess.sh file 46 screenshots of the source code in the environment screenshots of the output 47 48 