a robust approach for improving the accuracy of imu based indoor mobile robot localization suriya d murthy  srivenkata krishnan s  sundarrajan g  kiran kassyap s  ragul bhagwanth and vidhya balasubramanian department of computer science and engineering  amrita school of engineering  amrita vishwa vidyapeetham  amrita university  coimbatore  india keywords  robot localization  indoor localization  sensor fusion  inertial sensors  dead reckoning  abstract  indoor localization is a vital part of autonomous robots  obtaining accurate indoor localization is difficult in challenging indoor environments where external infrastructures are unreliable and maps keep changing  in such cases the robot should be able to localize using their on board sensors  imu sensors are most suitable due to their cost effectiveness  we propose a novel approach that aims to improve the accuracy of imu based robotic localization by analyzing the performance of gyroscope and encoders under different scenarios  and integrating them by exploiting their advantages  in addition the angle computed by robots to avoid obstacles as they navigate  is used as an additional source of orientation estimate and appropriately integrated using a complementary filter  our experiments that evaluated the robot over different trajectories demonstrated that our approach improves the accuracy of localization over applicable existing techniques  1 introduction robotic indoor localization is a method in which the position and orientation of the mobile robot is determined with respect to the indoor environment and is an important part of any autonomous mobile robot  autonomous robot systems are commonly being used during disaster response  in industries  as assistive robots etc  in order to support the effective functioning of robots in such scenarios there is a need for accurate and efficient indoor localization  navigation and mapping methods  one of the fundamental challenges in indoor environments is localization and this supports the other two functionalities  in this paper our primary goal is accurate self localization of mobile robots in indoor environments  several approaches have been designed for mobile robot localization  which include infrastructure and non-infrastructure based methods  infrastructure based approaches use existing wifi or rfid installations to help localize the robot  choi et al  2011 ; li  2012 ; zhang et al  2014   however  the accuracy of these approaches are environment dependent  therefore we need solutions that do not rely on external infrastructure  approaches that do not rely on infrasthis work has been funded in part by dst  india  grant dyno  100/ifd/2764/2012-2013 tructure use laser range finders  on board cameras and inertial sensors for localization of the robot  desouza and kak  2002 ; guran et al  2014 ; surmann et al  2003   laser range finders have high accuracy  but are expensive  camera based approaches are effective in many situations  but are computationally complex and perform poorly in low light conditions and in presence of occlusions  cost effectiveness  practicality  and ease of use has popularized the use of inertial sensors and digital encoders for localization purposes  guran et al  2014   and are widely employed by dead reckoning based approaches  while accelerometers and encoders have been used for distance estimates  gyroscopes and magnetometer sensors have been used to estimate orientation  however the accuracy of these sensors are affected due to accumulation of noise and drift errors from accelerometers and gyroscopes respectively  digital encoders are affected by slippage and other errors  to overcome these errors  map matching techniques and sensor fusion techniques have been proposed  elmenreich  2002 ; xiao et al  2014   in map matching  accuracy is improved by considering salient features in the map and repositioning the robot based on it  techniques like those described in  xiao et al  2014  use probabilistic ap 436 murthy  s  s  s  g  s  s  k  bhagwanth  r and balasubramanian  v a robust approach for improving the accuracy of imu based indoor mobile robot localization  doi  10.5220/0005986804360445 in proceedings of the 13th international conference on informatics in control  automation and robotics  icinco 2016   volume 2  pages 436-445 isbn  978-989-758-198-4 c 2016 by scitepress science and technology publications  lda  all rights reserved copyright a robust approach for improving the accuracy of imu based indoor mobile robot localization proaches for map matching and thereby localization  sensor fusion techniques can be used to combine the data from different sensors and improve their accuracy  sensor fusion can be achieved by simple aggregation based approaches or by applying filters like kalman  extended kalman and complementary filters  kam et al  1997   the current state of art in kalman filter based approaches use two or more different position and orientation estimates  the inputs for the kalman filter are from sensors embedded in the robot and external features  landmarks  corridors  walls etc   present in the indoor environment where the robot navigates  however accurate both sensor fusion based and map matching based techniques may be  they are ineffective when maps are incomplete and the environment consists of dynamic obstacles  additionally they are computationally expensive  therefore a cost effective approach that does not completely rely on a map is essential  in this paper we propose an approach that effectively combines the advantages of the encoders and gyroscopes to improve the accuracy of localization of the robot  while existing techniques like gyrodometry  ibrahim zunaidi and matsui  2006  have been developed  which use kalman filters for sensor fusion  their performance is affected when accelerometers are ineffective  as in the case of wheeled robots   our approach exploits the curvature of the robots trajectory to determine when the gyroscopes and encoders are used  this helps limit the continuous use of the gyroscope thereby reducing drift errors  in addition we exploit the obstacle avoidance capabilities of autonomous robots to provide another source of orientation estimate  this is combined with the gyroscopes orientation estimate using complementary filter to improve the localization accuracy  the rest of the paper is organized as follows  section 2 outlines the state of art in robotic localization in detail and positions our work with respect to it  next we discuss how the individual sensors are used for distance and orientation estimate in section 3 and explain our proposed approach in section 4 finally we evaluate our approach for different scenarios  section 5  and conclude in section 6  2 related work many localization techniques have been developed and implemented in robots over a period of time  common indoor localization methods for robots rely on external infrastructures and sensors which are part of the robot  localization techniques that use rf technologies such as rfid  wifi and bluetooth  choi et al  2011 ; li  2012 ; zhang et al  2014  rely on additional external infrastructure such as rf antennas and ultrasonic transceivers placed in the environment  on the other hand  techniques that use cameras  laser range finders or inertial sensors  desouza and kak  2002 ; guran et al  2014 ; surmann et al  2003  do not rely on external infrastructure  in the infrastructure based approaches  rf based localization is one of the most prevalent techniques and is easy to implement  in rf based methods  the robot requires external infrastructure like antennas/reference tags placed in the environment for communication with the receiver carried by the robot  rf based localization techniques can either use fingerprinting or non-fingerprinting approaches  the non-finger printing approaches include trilateration method and angulation techniques  aoa   liu et al  2007   the trilateration method estimates the position of the target with respect to the reference points using the received signal strength indication  rssi  values  easton and cameron  2006 ; granados-cruz et al  2014   this method fails to locate mobile node in multipath dense environment  the wifi fingerprinting technique compares the rssi observations made by the mobile node with a trained database to determine the location of the moving object  li  2012   deterministic approaches such as k-nearest neighbors  k-nn   kelley  2015   decision tree methods  erinc  2013  and probabilistic methods that include bayesian  hidden markov model  hmm  have been used in fingerprinting approach  in rfid based localization  a large number of rfid tags placed in the environment act as reference points  hence  when a new rfid tag enters the space  the signal strength is compared with the reference points signal strength and location of the robot is determined  choi et al  2011 ; zhang et al  2014   in both rfid and wifi finger printing approaches  the collection of data set  offline phase  is tedious and time consuming  it also requires frequent updation of fingerprint maps and special approaches are needed to reduce the cost of updation  krishnan et al  2014   the techniques that do not rely on infrastructure involve the use of lidar  light detection and ranging   cameras and inertial sensors  camera based approaches use the entire visual information or interest points or combination of all these as input in determining the location of the robot  desouza and kak  2002   these methods are usually prone to errors due to occlusions  changes in scale  rotation and illumination  in lidar based approaches  lasers have been used to emit pulses that are reflected off a rotating mirror from which time of flight is determined and used to calculate distances  surmann et al  2003   the 437 icinco 2016  13th international conference on informatics in control  automation and robotics main drawback here is that the laser range finders are expensive  inexpensive sensors such as accelerometers  gyroscopes and encoders are increasingly being used in localization  these imu sensors are commonly used in dead reckoning  where new state estimates are calculated with the help of prior states  ahn and yu  2007   accelerometers and encoders have been used to find the distance while gyroscope and magnetometer sensors have been used to determine the orientation  guran et al  2014   these methods do not give accurate results because accelerometer suffers from noise error  gyroscope suffers from static drift and magnetometers are error-prone in places where magnetic interferences are high  especially in indoor environments  the encoders suffers from either systematic  unequal wheel dimensions and kinematic imperfections  or non-systematic errors  wheel slippage and irregularities of the floor  and can be removed to a certain extent using the umbmark test  borenstein and feng  1994   to overcome the inefficiencies of these sensors we need to effectively combine the data from multiple sensors  sensor fusion based approaches have been proposed to achieve this  the sensor fusion in this context is the translation of different sensory inputs into reliable estimates  elmenreich  2002   filters like kalman and complementary filters have been widely used for sensor fusion thereby improving the accuracy to some extent  kam et al  1997   map matching based techniques have also been applied  xiao et al  2014  that improve the accuracy of localization accuracy using probabilistic approaches  as mentioned in previous section  the above methods fail when the map is not reliable and environment is dynamic  therefore we need a better method that is able to effectively localize the robot with minimum overhead and no external infrastructure  in addition it must be able to exploit the advantage of different sensors to improve accuracy and without relying on maps  in this paper we aim to achieve this using a novel decision based approach that uses the scenarios where different sensors work accurately  the next sections will describe our approach for improving the localization accuracy of indoor robots using inertial measurement units  3 system overview before we describe our approach we first introduce our robot  its system and its sensors  an image of our robot is shown in fig 1 our robot is a four wheeled autonomous light weight vehicle and has a dimension 438 figure 1  our 4-wheeled mobile robot  of 105mm*55mm*57mm  each of the 4 wheels has a diameter of 110mm  the robot is equipped with bstem  bst  2015  single chip computer  an arduino micro controller  sensors and motor drivers  the environment in which the robot navigates is a partially known environment  in which walls and corridors are known but any furniture or moving obstacles are unknown  the robot is defined by its pose p which is given at time t as  p =  xt  yt  t  where xt  yt represents the position estimate and t represents the orientation estimate  to measure the distance travelled by the robot  two quadrature encoders associated with the stepper motors are added to the rear wheels of the robot  it must be noted that this is a rear wheel driven differential driven robot and speed is controlled to avoid slips and keep the kinematic center at the middle of the rear wheels  these encoders output the left and right ticks  l  r   using which the distance  dist  travelled by the robot is calculated as dist =  l + r  /2  since we do not use accelerometers  this is the only source of distance estimate  the encoders also provide us with an estimate of the robots orientation e  the tri-axial gyroscope present in bstem provides the angular rates with respect to each axis  using the z-axis angular rate we calculate the orientation angle g  two ultrasonic and two infrared sensors are embedded on either sides infront of the robot  both the sensors return the distance between the obstacle and robot  this data serves to provide us with an additional orientation estimate  we use the encoders to determine the distance estimate and a combination of gyroscope  encoders and obstacle avoidance sensors for orientation estimate to effectively determine pose  the mathematical determination of position and orientation estimate from encoders  gyroscope and obstacle avoidance sensors are explained below  a robust approach for improving the accuracy of imu based indoor mobile robot localization 3.1 position and orientation estimation using encoders the encoders provide ticks l and r from the left and right motors respectively  the width of the robot is defined as w  using these  the position and orientation is calculated by applying geometric techniques  jensfelt  2001   in this technique the pose is estimated for the following two cases  robot is taking turns  robot is moving straight  case 1  determination of pose when the robot is taking turns when the robot is taking turns  there is a difference in the ticks between the right and left encoders  this information is used to estimate the curvature of the 0 robots path  based on the previous position p  and hence determine the pose p the heading direction changes when the robot turns  the current orientation is the heading angle which is based on the previous orientation  and  the new orientation estimate e  t  is calculated as follows  e  t  = e  t 1  +  1  x  t  = cx +  r +  w /2   sin  e  t    2  y  t  = cy +  r +  w /2    cos  e  t     3  here the angle represents the change in heading angle of the robot during traversal from previous position to the current position  the new position estimate  x  t   y  t   is determined as follows  where c  cx  cy  is the center of the extended circle where the arc traversed by the robot from previous position to current position lies  similarly  this can be extended for the calculation of new pose when robot turns right  case 2  determination of pose when the bot is moving straight when the robot is moving straight  there is no change in its orientation  and the left ticks are equal to the value of right ticks  so  the new orientation estimate is same as the orientation at time t 1  e  t  = e  t 1   4  x  t  = x  t 1  + l  cos  e  t     5  now  the corresponding pose x  t   y  t  is calculated as given below  y  t  = y  t 1  + l  sin  e  t     6  3.2 orientation estimate using gyroscope gyroscope is a sensor used to measure angular velocity  in our robot  the l3gd20 mems motion sensor 3-axis digital gyroscope  which is part of the bstem provides angular velocities along three axes x  y  z respectively  the sensitivity of the gyroscope is set at 250dps  ideal for a robotic system that undergoes heavy vibrations due to motor and chassis movement  the goal is to calculate the orientation angle of the robot over a period of time  which is done by integrating the angular velocity about the z-axis over a certain time interval  in order to get reliable angle estimates we first calculate and eliminate the offset and noise of our gyroscope and they are calculated as follows  offset = 1 n n i=0  7  where is the angular velocity at that instant of time and n represents the number of angular velocity values taken  noise = max  |i offset|  0in  8  where i is the angular velocity at that instant  noise is calculated for all three axes  the angular velocities which fall in the range of  noise  noise  are ignored and the remaining values are taken for the calculation of orientation  the orientation angle g  t  at time t is given by  g  t  = g  t 1  + i dt  9  where dt represents the time period over which i can be periodically integrated to determine the angle  in our case this value is equal to 20ms  using the above g  t  value we estimate the orientation and position  the position estimate is calculated using previous position values  the newly calculated orientation value and the distance estimate from encoders  dist  and is determined as follows  x  t  = x  t 1  + dist cos  g  t   y  t  = y  t 1  + dist sin  g  t    10   11  3.3 orientation estimate using obstacle avoidance sensors in general orientation estimates come from the encoders  gyroscopes or magnetometers  here we are also using orientation estimates based on information from obstacle avoidance sensors  robots use obstacle avoidance systems to avoid obstacles  and plan their trajectory based on the distance between them and the obstacle  at each point  while the obstacle is 439 icinco 2016  13th international conference on informatics in control  automation and robotics still in range  the autonomous robot calculate the curvature angle required to safely maneuver itself  this angle is only known when the obstacle is detected  and it can be reasonably assumed that the robots actual trajectory during navigation is close to the estimated trajectory determined for obstacle avoidance  this estimated angle therefore is used by our algorithm as an additional source of orientation estimate  we explain the orientation estimation using the obstacle avoidance system below  in this paper we consider the ultrasonic and infrared sensors for obstacle avoidance  the sensor specifications are given below  1 ultrasonic range finder xl-ez3  mb1230  which has a range of 20cm to 760cm and a resolution of 1cm is used to detect obstacles present in long range  2 sharp gp2y0a41sk0f ir distance sensor which has a range of 4cm to 30cm is used to determine obstacles in close range  a robot that is moving autonomously takes navigational decisions based on the output from the above sensors  an obstacle avoidance method  widodo budiharto and jazidie  2011   is adapted to calculate the orientation of the robot as explained below  when an obstacle is detected  the two ultrasonic sensors return the distances  dlu  dru  between the obstacle and corresponding left and right sensors on the robot  dsa f e  the flank safety distance  is the minimum distance at which robot can start to maneuver to avoid obstacle  this value is calculated experimentally based on the range of the ultrasonic sensors and width of the robot  to determine the angle from the distances given by ultrasonic and infrared sensors  the following cases must be considered  case 1  dlu > dsa f e and dru > dsa f e when there is no obstacle in front of the robot  it can continue to move in the same direction  hence the orientation angle oa  t  at time t is same as the previous angle oa  t 1   case 2  dlu < dsa f e and dru > dsa f e in this case the robot has to take a right turn in order to avoid the obstacle  the closer the robot is to the obstacle the wider it has to turn and vice versa  the ideal angle  oa  t  for the robot to avoid the obstacle is calculated using the distance from left ultrasonic sensor as follows  ku  12  oa  t  = oa  t 1    2 dlu where ku represents the collision angle constant for the ultrasonic sensor  which is calculated based on dsa f e and dlu  case 3  dlu > dsa f e and dru < dsa f e when the bot has to turn left in order to avoid obstacle  the ideal 440 angle is calculated using the distance from the right ultrasonic sensor and the angle is calculated similarly  ku oa  t  = oa  t 1  +    13  2 dru case 4  dlu < dsa f e and dru < dsa f e when the robot is too close to the obstacle  this case comes into picture  here  we use the distances dri  and dli returned by the infrared sensors  the orientation is estimated similarly  the angle calculation for right turn is given by  ki oa  t  = oa  t 1  n      14  2 dri similarly  the angle calculation for left turn is given by  ki  15  oa  t  = oa  t 1  + n     2 dli where n is experimentally calculated to increase or decrease the rotation appropriately to avoid close obstacles  ki is the collision angle constant for infrared sensors  finally  if the robot is too close to the obstacle and is impossible for it to maneuver an obstacle smoothly  i.e  if dlu < dn  then the robot moves backwards by a certain distance and checks for all the above possibilities and estimates the orientation angle  dn is the maximum distance before which the robot should start maneuvering to avoid the obstacle  this value is calculated experimentally based on the range of the infrared sensors and width of the robot  the region  dsa f e  dn   is the buffer region for smooth maneuvering  using the above equations  we estimate the orientation angle based on the data from the obstacle avoidance sensors  now that we have estimated the pose using the individual sensors  the goal of our system is to improve the accuracy by appropriately combining the data from all of them  4 curvature based decision system this section describes our novel approach to improving the location accuracy by fusing information from the different sensors appropriately  in robotic localization inaccuracies in orientation measure affects the position more when compared to inaccuracies in distance measure  borenstein and feng  1994   for example  a robot with 8 inch wheel base and slip of 1/2 inch in one of the wheels results in an error of approximately 3.5 degrees in orientation  so it is important to have a highly accurate value of the orientation measure when compared to that of a distance measure  a robust approach for improving the accuracy of imu based indoor mobile robot localization our method assumes that the distance  l  r  measured using the encoders is reliable  since the systematic errors can be easily removed using umbenchmark and non-systematic errors do not affect the distance estimate much  as explained in the above example  the orientation estimate from encoders however are not reliable since they undergo huge non-systematic errors due to sudden turns and heavy slippage  slight deviations in distance due to non-systematic errors results in huge deviations in orientations  the gyroscopes are generally accurate in estimating orientation  and is preferred over encoders  however  over longer durations the performance degrades due to the accumulation of drift errors  our approach enhances the accuracy of orientation estimation by a  reducing the period over which the gyroscope value accumulates  so that drift errors are minimized and b  fusing the orientation estimates from gyroscope with orientation estimates obtained to avoid obstacles as discussed in section 3.3 inorder to actualize the above  the trajectory in which the robot travels is analyzed  we identify the part of the trajectory that is almost linear  so that orientation measure from encoders can be used reliably  in the other parts of the trajectory the gyroscope is employed  to improve the accuracy further  the gyroscope data is fused with the angle estimated at the current position by the robot to avoid obstacles  the main steps in our approach are as follows 1 estimation of curvature of the robots trajectory 2 using the trajectory to determine the appropriate sensor to use 3 enhance orientation estimation accuracy using a complementary filter to fuse orientation from above step and obstacle avoidance system a decision based approach is preferred to a fusion based approach for determining the sensor to use based on the curvature of the trajectory  since it is clear which sensors used perform better in different scenarios we employ this approach  4.1 curvature estimation the robots trajectory is a 2d smooth plane curve  the curvature of the past trajectory of the robot determines the sensor  s  to be used for the orientation estimation at the current point  in order to find the curvature  we find the angle between the slopes of tangents drawn to the previous points p1  x  y  and p2  x  y  on the trajectory  to draw the tangents we find the center of curvature c  cx  cy  of the curve  we then calculate the angle between the slopes of the tangents  which figure 2  determination of slope  gives the curvature at the point  this method is efficient in curvature estimation since we only store the previous two points of the trajectory and not the complete trajectory to decide on the appropriate sensors  to find the slope m1 of the tangent vector t at point p1  we first find the slope n1 of the normal vector n at p1 passing through center of curvature c the slope of n1 is calculated using n1 =  cy p2  y   /  cx p1  x    16  the slopes m1 of the tangent vector t  which is perpendicular to n is then calculated as the negative of the inverse of n1 ; m1 = 1/n1  similarly we calculate m2  which is the slope of the tangent vector at point p2  once these two slopes are calculated the degree of curvature dc can be determined as follows dc = tan1   m1 m2  /  1 + m1 m2    17  this dc value helps us determine if the robot is travelling in a near linear trajectory or on a curved path  using this we determine when to start aggregating the gyroscope values and when to only rely on encoders  a gyroscope controller is implemented to appropriately switch between the encoders and gyroscope  4.2 gyroscope controller the gyroscope controller starts accumulating the gyroscope readings  and switches completely to the encoder based on start and stop threshold values  the start threshold value 1 is calculated by computing the average of the degrees of curvature when the robot begins to make a turn  when the robot completes a maneuver by avoiding an obstacle and starts to follow a linear trajectory  the average of degree of curvatures of multiple linear paths is computed and used as the stop threshold value 2 when the dc keeps increasing and goes over start threshold value 1  we initiate the gyroscope and use it for orientation measure and allow it to summate as the robot traverses the curved path  the accumulation 441 icinco 2016  13th international conference on informatics in control  automation and robotics process since it is very easy and light to implement making it perfect for embedded systems application as in our case  the complementary filter is given by the equation  = g +  1  oa figure 3  determination of start and stop threshold  of gyroscope values is stopped when the degree of curvature becomes closer to the stop threshold value 2 figure 3 shows this scenario  by doing this we ensure that the gyroscope is not made to accumulate continuously for a long time  thereby ensuring that the drift errors from the gyroscopes are not a major source of localization errors  when dc keeps decreasing and becomes equal to or less than 2  the encoders are used for orientation estimation of the robot  using the encoders when the dc is less than or equal to the 2  reduces the effect of the non systematic errors resulting in an accurate position calculation  in order to achieve the above the following conditions have to be met  1  2 < < 1  ensuring the left and right ticks are equal in straight paths  2  1 is given higher precedence than 2  ensuring gyroscope is started as soon as the robot begins to make a turn  in order to further increase the orientation estimate obtained from gyroscope  it is fused with the angle obtained from the obstacle avoidance sensors explained below  4.3 gyro-obstacle fusion to improve the accuracy of orientation estimates during turns  the gyroscope values are fused with the ideal angle estimated to avoid obstacles as explained in section 3.3 as discussed earlier it is our hypothesis that robots tend to closely follow the trajectory estimated to avoid obstacles  however the actual angle taken by the robot to avoid the obstacle is not controllable and hence is only known by the gyroscope  therefore this estimated angle is fused with the gyroscope angle to potentially enhance the accuracy  we use a complementary filter for this fusion 442  18  where g is the angle obtained from gyroscope  oa is angle obtained from obstacle avoidance sensors and is the factor which is determined experimentally  therefore  we get a good orientation estimate from encoders  gyroscope and obstacle avoidance system  this orientation is then combined with the distance measure  l  r  from encoders to get the pose estimate  on fusing the data by considering the advantages of individual sensors  the position is obtained  5 experimental results the previous sections discussed our approach for improving the accuracy of localization by combining data from different sensors used in our robot  in this section we analyze the performance of this approach  first we discuss the arena and experimental setup followed by the metrics used for analyzing our algorithm  we also discuss the methodology for obtaining the ground truth  the experiments are carried out in an arena of dimensions 1710 480cm2 using a four wheeled autonomous robot as shown in figure 1 the arena is then transformed into a grid of squares  each of area 900cm2  obstacles are placed in different parts of the arena  different trajectories are considered for evaluating our algorithm  in order to generate the ground truth the grid is used to identify the points the robot passed through  these points are plotted on a raster map  the following algorithms are evaluated  it must be noted that in all the techniques encoders provide the distance estimate  and these techniques are used for angle estimation  1 encoder based approach 2 gyroscope based approach 3 curvature based decision approach without integrating obstacle avoidance information  cdano  4 curvature based decision approach that uses all three sensors  cda-o  to evaluate the performance of these approaches in comparison to the ground truth we use the metrics as discussed next  a robust approach for improving the accuracy of imu based indoor mobile robot localization 5.1 metrics of evaluation the most important indicator in evaluating the performance of a localization algorithm is accuracy  accuracy refers to how close the estimated trajectory as obtained from the localization method is to the ground truth  in order to determine the closeness  mean and standard deviation of the euclidean distance between the ground truth and estimated point is calculated  the mean euclidean distance is the sum of euclidean distance at each corresponding point divided by the total number of such corresponding points  p ni=0    xgt xest  2 +  ygt yest  2   =  19  n  xgt  ygt  refers to the ground truth and  xest  yest  is the estimated position from the localization method used  while the mean provides the error estimates  the standard deviation gives an idea of how consistently the algorithm performs  5.2 performance analysis we evaluate the above mentioned approaches over the arena specified  the evaluation is done by making the robot navigate along different trajectories in the arena  we have considered three trajectories and their corresponding ground truth is plotted and used for comparison  table 1 shows the mean and standard deviation of the euclidean distance error in the corresponding trajectories and methods used for comparison respectively  figure 5  performance for an eight shaped trajectory  tory  figure 4 shows the plot of the estimated trajectories for different techniques along with the ground truth trajectory  from the figure and the table we can see that the average error of encoder based localization is comparatively high  this is because  in the trajectory  the robot has taken sharp turns  which leads to slips and it affects the performance of the encoder based method  the performance of gyroscope based localization is better than encoder because its orientation estimates are more accurate during turns  and that dictates the accuracy of the trajectory  the curvature based approach improves the accuracy of the localization over the gyroscope on average  however using the orientation estimates from the obstacle avoidance system further improves the accuracy  this supports our hypothesis that the robot traverses nearly close to the estimated trajectory needed to avoid obstacles  additionally since the gyroscope is the only measure of the actual angle taken by the robot in this scenario  fusing with the estimated angles helps the estimated trajectory come closer to the ground truth  this leads us to the conclusion that orientation estimates during obstacle avoidance has high accuracies and can be reliably used  5.2.2 figure 4  performance for a rectangular trajectory  5.2.1 case 1  rectangular trajectory the first case is when the robot traversed along the periphery of the arena resulting in a rectangular trajec case 2  eight shaped trajectory figure 5 shows the output of the different localization methods when the robot is allowed to navigate in an eight shaped trajectory  here we can notice that since there are not many sharp turns  the encoder performance improves  here too our curvature based approaches  both with and without input from obstacle avoidance sensors  perform the best  while the gyroscope plays a major role in the improvement of accuracy  the nature of the trajectory limits the curvature based decision approach  however we can see that employing the input from obstacle avoidance im443 icinco 2016  13th international conference on informatics in control  automation and robotics table 1  performance analysis of different techniques for different trajectories  methods implemented encoders gyroscope cda-no cda-o trajectory 1  rectangular  avg error std dev  cm   cm  121.96 79.64 40.50 22.49 28.64 15.96 20.25 12.59 trajectory 2  eight  avg error std dev  cm   cm  89.27 51.74 44.22 26.86 39.50 24.91 34.68 24.66 trajectory 3  random path  avg error std dev  cm   cm  333.65 191.74 75.33 44.26 65.38 38.96 20.49 12.67 figure 6  random plot  proves the accuracy to a certain extent  5.2.3 case 3  random trajectory to obtain a better idea of how well our approach for localization works  we allowed the robot to take a random path filled with obstacles  and this path was much longer than the previous ones  when the path is longer  the drift errors in the gyroscope play a role  figure 6 shows the plots of the estimated trajectories for this path  we can see that here that our curvature based approach that utilizes the obstacle avoidance system performs exceedingly well in comparison to all other strategies  this is because the path has lot of obstacles and that is well exploited here  the gyroscope performs poorest here  since it is more prone to drift errors here  therefore using the curvature based approach helps reduce the drift error  and improve accuracy to a small extent  while the o f f set and noise is calibrated at the beginning  we propose to update it at regular intervals  specially when the robot is moving straight  overall we can see that our approach performs consistently well in all scenarios and gives an accuracy of around 20 to 30 cm  we also observe that considering the curvature and fus444 ing the estimate of the orientation from the obstacle avoidance systems with gyroscopes significantly improves accuracy  the trajectories generated by our approach are very close to the trajectory both in terms of distance and the overall pattern  as can be seen in the plots  the standard deviation values  specially for the rectangular and random plots show that our approach not only provides good accuracies  but does it consistently along the path  additionally the overhead of this approach is low and hence is easily implementable in any autonomous robot  6 conclusions and future research in this paper we have described out approach for improving the localization accuracy of indoor mobile robots  which use inertial sensors for localization  the curvature of the robots trajectory is analyzed to determine when the gyroscope and encoder data are to be used  in addition to improve the accuracy  orientation estimates from obstacle avoidance systems are fused with the gyroscopes orientation estimates  a robust approach for improving the accuracy of imu based indoor mobile robot localization our experiments have shown that by using only gyroscopes as and when needed  and employing the additional data from obstacle avoidance sensors  the location accuracy is improved significantly  it is seen that since environments usually have obstacles  and robots have to navigate around them  using that as a parameter is an effective approach for localization  our approach provides good accuracies at a low overhead  however when the turns are very sharp  and acute  our approach suffers  while our approach accounts for gyroscope drift errors  more work needs to be done to reduce it further for longer distances  references  2015   bstem developer kit & integrated robotics platform  www.braincorporation.com  ahn  h.-s and yu  w  2007   indoor mobile robot and pedestrian localization techniques  in control  automation and systems  2007 iccas07  international conference on  pages 23502354 ieee  borenstein  j and feng  l  1994   umbmark  a method for measuring  comparing  and correcting dead-reckoning errors in mobile robots  choi  b.-s  lee  j.-w  lee  j.-j  and park  k.-t  2011   a hierarchical algorithm for indoor mobile robot localization using rfid sensor fusion  industrial electronics  ieee transactions on  58  6  :22262235 desouza  g n and kak  a c  2002   vision for mobile robot navigation  a survey  pattern analysis and machine intelligence  ieee transactions on  24  2  :237 267 easton  a and cameron  s  2006   a gaussian error model for triangulation-based pose estimation using noisy landmarks  in robotics  automation and mechatronics  2006 ieee conference on  pages 16 ieee  elmenreich  w  2002   sensor fusion in time-triggered systems  erinc  g  2013   appearance-based navigation  localization  mapping  and map merging for heterogeneous teams of robots  granados-cruz  m  pomarico-franquiz  j  shmaliy  y s  and morales-mendoza  l j   2014   triangulationbased indoor robot localization using extended fir/kalman filtering  in electrical engineering  computing science and automatic control  cce   2014 11th international conference on  pages 15 ieee  guran  m  fico  t  chovancova  a  duchon  f  hubinsky  p  and dubravsky  j   2014   localization of irobot create using inertial measuring unit  in robotics in alpe-adria-danube region  raad   2014 23rd international conference on  pages 17 ieee  ibrahim zunaidi  norihiko kato  y n and matsui  h  2006   positioning system for 4wheel mobile robot  encoder  gyro and accelerometer data fusion with error model method  in cmu journal  volume 5 jensfelt  p  2001   approaches to mobile robot localization in indoor environments  phd thesis  royalinstitute of technology  stockholm  sweden  kam  m  zhu  x  and kalata  p  1997   sensor fusion for mobile robot navigation  proceedings of the ieee  85  1  :108119 kelley  k j   2015   wi-fi location determination for semantic locations  the hilltop review  7  1  :9 krishnan  p  krishnakumar  s  seshadri  r  and balasubramanian  v  2014   a robust environment adaptive fingerprint based indoor localization system  in in proceedings of the 13th international conference on ad hoc networks and wireless  adhoc-now2014   volume 8487  pages 360373 li  j   2012   characterization of wlan location fingerprinting systems  masters thesis  school of informatics  university of edinburgh  liu  h  darabi  h  banerjee  p  and liu  j   2007   survey of wireless indoor positioning techniques and systems  systems  man  and cybernetics  part c  applications and reviews  ieee transactions on  37  6  :10671080 surmann  h  nuchter  a  and hertzberg  j   2003   an autonomous mobile robot with a 3d laser range finder for 3d exploration and digitalization of indoor environments  robotics and autonomous systems  45  3  :181 198 widodo budiharto  d p and jazidie  a   2011   a robust obstacle avoidance for service robot using bayesian approach  international journal of advanced robotic systems  8:5260 xiao  z  wen  h  markham  a  and trigoni  n  2014   lightweight map matching for indoor localisation using conditional random fields  in information processing in sensor networks  ipsn-14 proceedings of the 13th international symposium on  pages 131142 zhang  h  chen  j c  and zhang  k  2014   rfid-based localization system for mobile robot with markov chain monte carlo  in american society for engineering education  asee zone 1   2014 zone 1 conference of the  pages 16 ieee  445 