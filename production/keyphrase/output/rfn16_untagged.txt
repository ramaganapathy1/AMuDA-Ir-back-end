rf vision  rfid receive signal strength indicator  rssi  images for sensor fusion and mobile manipulation travis deyle1  hai nguyen1  matt reynolds2  and charles c kemp1 1 healthcare 2 department robotics lab  georgia institute of technology  usa of electrical and computer engineering  duke university  usa abstract in this work we present a set of integrated methods that enable an rfid-enabled mobile manipulator to approach and grasp an object to which a self-adhesive passive  battery-free  uhf rfid tag has been affixed  our primary contribution is a new mode of perception that produces images of the spatial distribution of received signal strength indication  rssi  for each of the tagged objects in an environment  the intensity of each pixel in the rssi image is the measured rf signal strength for a particular tag in the corresponding direction  we construct these rssi images by panning and tilting an rfid reader antenna while measuring the rssi value at each bearing  additionally  we present a framework for estimating a tagged objects 3d location using fused id-specific features derived from an rssi image  a camera image  and a laser range finder scan  we evaluate these methods using a robot with actuated  long-range rfid antennas and finger-mounted short-range antennas  the robot first scans its environment to discover which tagged objects are within range  creates a user interface  orients toward the user-selected object using rf signal strength  estimates the 3d location of the object using an rssi image with sensor fusion  approaches and grasps the object  and uses its finger-mounted antennas to confirm that the desired object has been grasped  in our tests  the sensor fusion system with an rssi image correctly located the requested object in 17 out of 18 trials  94.4 %   an 11.1 % improvement over the systems performance when not using an rssi image  the robot correctly oriented to the requested object in 8 out of 9 trials  88.9 %   and in 3 out of 3 trials the entire system successfully grasped the object selected by the user  i i ntroduction radio frequency identification  rfid  is an umbrella term for a variety of transponder systems  including active  battery-powered  and passive  battery-free  tags of widely varying complexity and capabilities  in this work we concentrate on simple  low-cost passive uhf rfid tags  often called smart labels  based on the widely adopted epc global generation 2 communication protocol  1   currently available passive uhf rfid tags are battery-free  with a read range exceeding 5 meters and data storage capacities ranging from 128 bits to over 1k-bit  they currently cost less than $ 0.10 usd in volume  to date  rfid tags have typically been used in a purely binary fashion  returning tag ids for each tag in range  or indicating that no tag was found  prior work has shown how this binary tag sensing modality can be used to improve robot localization  mapping  navigation  and unique object detection  fig  1 the mobile manipulator  el-e with two articulated  long range rfid antennas  top  and short-range near-field rfid antennas on the end effector  bottom   recent work has shown that there is valuable information present in the tags rf signal itself  beyond the tag id  2   in our prior work  we have used estimates of received signal strength from passive rfid tags to inform robotic behaviors  both in the context of servoing an rfid-enabled robot toward a tagged object  3  and estimating a tags position relative to the robot via particle filtering  4   here we present a new method for rfid-based sensing that uses the receive signal strength indication  rssi  to form an rssi image that can ii  r elated w ork fig  2 method for producing a maximum-likelihood 3d point estimate for the location of an rfid-tagged object be fused with 2d images from a co-located camera and 3d point clouds from a co-located scanning laser range finder  we also show that the unique id of a tag can be associated with perceptual characteristics of the object to which it is affixed  which in turn can facilitate object detection with this fused image  by combining the camera image  laser range finder scan  rssi image  and object-specific data associated with this unique tag id  our method is able to efficiently produce an estimate of the 3d location of a selected tagged object  we have tested this approach in an object fetching application with el-e  the autonomous mobile manipulator shown in figure 1 in our tests  the robot first scans its environment to enumerate the tagged objects are in the environment  based on the tag responses from the enumerated tagged objects  the robot then constructs a user interface from which the user can select an object to be fetched  after selection  the robot servos its orientation such that the tagged object of interest is visible by both its camera and laser scanner  the servoing process maximizes the rssi obtained from two long-range  actuated antennas reading the selected tag  the robot then constructs an rssi image by panning and tilting one of these antennas and recording the rssi signal for each bearing in terms of azimuth and elevation  the robot also captures an optical image with a calibrated color camera and a range scan using a tilting laser range finder  each sensors output is geometrically transformed to produce an output as a function of bearing in the robots reference frame  so it is straightforward to fuse these three data sets into a single data set indexed by a single  azimuth  elevation  bearing pair  additional features associated with each tagged object that have been associated with the tags unique id are read from a database and used to aid in finding the object in the fused image  this process results in a 3d estimate of the objects location  which the robot uses to approach and grasp the object using a system we have previously described  5    6   a wide variety of research has been conducted on the application of rfid technology to robotics  this includes rfid-enhanced interaction between robots and tagged people and objects  such as that described in  7   where tags facilitate person/object identification  there is also a great deal of prior work in rfid augmented indoor navigation  8   where tags are used as either a waypoint navigation and landmarking system  9   or more commonly as a component of a robots localization and mapping system  several recent works employ long-range passive uhf  902-928mhz  rfid  in addition to laser rangefinders and odometry  as sensor inputs to a probabilistic slam algorithm  for example  10   in these prior results  the rfid system only reports the tag ids of visible rfid tags  or indicates that no tag is found  in contrast  our work explicitly takes advantage of rf signal information  an alternative rfid-enhanced navigation approach uses short-range  1m  magnetically coupled passive rfid tags  11  to detect when robots pass above tagged waypoints  again  however  a binary indication of tag presence or absence is all that is reported by the rfid system  recent work in active  battery-powered  tagging  12  demonstrates navigation to a relatively expensive  battery powered target tag in a cluttered environment  in the latter work  a mechanically rotating reader antenna with a deep null in its radiation pattern is used to find bearings from the reader to the active tag  other complex or expensive tag-centric antenna design techniques have also been explored to find range and bearing  13   numerous rfid tag localization approaches have been considered  including binary  read / no-read  histogram techniques  10  and rssi techniques employing both histograms and sensor models   2  and  4  respectively  while localization methods that estimate range and bearing posteriors may yield volumetric regions of interest  these methods tend to marginalize over the entire robot trajectory  we believe a compelling  potentially complementary  approach is to create 2d images of rf signal properties at a fixed robot location to provide valuable insights into the otherwise invisible rf world  to this end  we introduce rssi images  shown in figure 3  which capture the rssi signal characteristics as a function of bearing  azimuth and elevation  in a manner analogous to an optical cameras visible light images  in the fields of computer vision and augmented reality  there are many examples of systems that employ identifying fiducials  such as optical tags  coupled with a perceptual database  for example  qr codes have been used for both identification and 6dof estimation for a manipulation task  14   while rfid tags have also been used in a database approach to identify perception and action primitives in a scene  15   this work is differentiated in several ways  first  we believe this work to represent the first use of rssi images as a distinct sensing modality  further  we describe a probabilistic framework for sensor fusion  employing object fig  3 three camera images  top row  and corresponding blurred rssi images  bottom row  of a tagged red bottle  marked with red box  as the bottle is moved from left to right across the scene  the strongest rssi is depicted in red and corresponds with the location of the bottle in the images  centric features extracted from a database as indexed by the unique tag id  additionally  we demonstrate a user interface that allows users to select tagged objects from those present in the environment  finally  we employ this framework to create a system capable of performing mobile manipulation of tagged objects  iii  rssi i mages recent advances in rfid reader technology have made rf signal properties such as the receive signal strength indication  rssi  available as metadata for each tag read by the reader  in its most basic form  rssi is a scalar measurement of the tags rf signal power as received at the reader  for example  we employ a thingmagic mercury 5e rfid reader which returns an rssi value that is linearly related to received power in dbm  the raw reported rssi value ranges from 70 to 100 units  though it saturates with very strong tag responses at a value of 105 many system implementation and environmental factors affect the absolute value of the rssi reported for a particular tag  we are primarily interested in rssi variation with distance and bearing to each tag  but system implementation parameters such as transmit power  reader antenna characteristics  tag antenna characteristics also influence the absolute value of rssi  in our work these system parameters are fixed  and do not vary with tag range and bearing  the rf properties of the robots environment including occlusion  multipath  and interference are difficult to model and can also be significant  but we have found that they need not be modeled for nearby tags where line-of-sight propagation is dominant  to construct an rssi image  one of the robots two long range  far-field rfid antennas is positioned in front of the robot  placing it approximately coincident with the tilting laser rangefinder and camera  the antenna is panned and tilted through azimuth and elevation angles while recording rssi readings associated with a desired tag id  a single slice of the radiation pattern for the antenna we employ  the cuschcraft s9028pc circularly polarized patch antenna  is illustrated in figure 4 this antenna has horizontal and vertical half-power beamwidths of 60  this antenna beamwidth is the limiting factor in the precision of this sensor ; advances in digitally scanned array antennas could fig  4 a 2d slice of cushcraft s9028pc antenna radiation pattern in polar format  with pan angle varying at a fixed tilt angle of 0  peak antenna gain is 6.5dbi with a half-power beamwidth of 60  produce higher resolution images much faster than the pantilt mechanical scanning we are currently using  the resulting rssi values are then mapped into a single image  roughly corresponding to a camera image  next  the raw image is smoothed using a gaussian filter with a standard deviation of 45 pixels  corresponding to 7 of pan or tilt  lastly  the intensity values of the rssi image are scaled to occupy the range  0.0  1.0   iv  s ensor f usion the goal of our approach to sensor fusion is to combine the rssi image for a particular tagged object id with objectspecific features extracted from other sensing modalities  we provide a probabilistic framework for fusing these sensing modalities and associated features to produce a single maximum likelihood 3d location that is used by the mobile manipulation system to retrieve the tagged object  a registering the sensors in this work  we consider the output of three approximately coincident sensors with overlapping fields of view  the rssi image  a low resolution  640x480  camera image from a rectified camera  and a 3d point cloud from a tilting laser rangefinder  in order to fuse the output of these three sensors  we first geometrically register them with one another  we accomplish this by transforming both the rssi image and the 3d point cloud into the camera image  for the 3d point cloud  we estimated the 6dof transformation from the laser rangefinder to the camera by hand measurements  and then refined this estimate using visualization software that displays the transformed 3d point cloud on the corresponding camera image  transforming the 3d point cloud results in a range image  irange  x  y   that is registered with the camera image  icam  x  y   for the rssi image  we first convolve the raw rssi image with a gaussian  g    rssiraw     and then scale the resulting values to occupy the range  0.0  1.0   we then transform the resulting smoothed rssi image into the camera image with a simple linear interpolation based on hand measured correspondences between the rfid antennas azimuth and elevation      and the cameras pixels   x  y   this results in the registered rssi image  irssi  x  y   as shown in figure 3 given the low spatial resolution of our current rssi images and the near coincident location of the camera and antenna  this transformation is effective  however  we expect that more accurate registration would improve system performance  b inferring a tags 2d image location the fused image i consists of a set of n feature images i0  in  where each feature image ii represents the spatially varying value of feature fi  we model each of these features as being generated with some probability pfi |tag  fi  true   if a tag is at the bearing associated with the location  if a tag is not at the bearing associated with the location  we model the probability of a given feature value as pfi |tag  fi  false   we further model these feature values as being conditionally independent given the presence or absence of the tag at the bearing associated with the location  and as independent from one another  given these assumptions  we can find the probability that a tag is at a given location using bayes rule  ptag|f0  fn  v  f0  fn  = qn = i=1 pf0  fn |tag  f0  fn  v  ptag  v  pf0  fn  f0  fn   1   pfi |tag  fi  v  ptag  v  qn i=1 pfi  fi  = ptag  v   2  n y pfi |tag  fi  v  i=1  3  pfi  fi  we assume a uniform prior on the position of each tag  ptag  v   assuming independence of the feature vectors for each x  y location of the fused image i  pimage  i  = q pf0  fn |tag  i  x  y   v  x  y    and  pfi  ii  x  y   = pfi |tag  ii  x  y   true  +  4  pfi |tag  ii  x  y   false   the maximum likelihood  ml  estimate of the location of the tag is then  argmaxx  y n y pfi |tag  ii  x  y   true  i=1 pfi  ii  x  y      5  the result of this argmax operation selects a pixel   xml  yml   in the fused image  in the subsequent section  we will show how this pixel may be mapped into the 3d point cloud produced by the tilting laser range finder to produce a single maximum-likelihood 3d location for the tagged object  this interaction is illustrated in figure 2 c inferring a tags 3d location in order to effectively apply this method of probabilistic inference  we select discriminative feature  s  from each sensing modality  the selection of discriminating features could employ feature sets  models  and / or training data available from a database indexed by the tags id  in this work  we consider three features indexed by tag id  first  the feature from the rssi image consists of the rssi value from irssi  x  y   the associated probabilities  prssi|tag  rssi  true  and prssi|tag  rssi  false   were obtained as a histogram from 60 hand-labeled ground-truth observations as shown in figure 5 from the camera image  we employed color histograms as the visual feature  we selected color histograms for their simplicity  other visual features could be integrated into this framework and may be more discriminative  for the color histogram  the object probability  pcolor|tag  icam  x  y   true   is obtained from an image of the tagged object stored in the tagindexed database  meanwhile  the non-object background probability  pcolor|tag  icam  x  y   false   is generated from a color histogram accumulated over the set of images of the environment collected during navigation  for the laser  there are many candidate features  from spin images  16  to 3d segmentations as applied in our previous work  17   in this work we have treated the laser as a special case  where point 3d  p3d  features are used to produce a binary mask on the image   pp3d|tag  p 3d  v  1.0 p3d laser scan = 0.0 p 3d laser scan pp3d  p 3d  this ensures that any pixel selected by argmaxx  y produces a direct mapping to a valid 3d location based on laser range scans  after all three sensor images are fused  the maximum likelihood pixel is selected  and the corresponding 3d location from the laser is chosen  a montage showing this method is shown in figure 6 v m obile m anipulation s ystem the fused sensor image is incorporated as follows into the mobile manipulation system  the robot first uses the rfid antennas to scan the environment for tagged objects in the environment  the object names associated database images corresponding to each tag id are presented to a remote user via the graphical user interface shown in figure 7 the user can select an object from an array of database photos  indexed by the observed tag id of each tagged object  after the user selects a tagged object  the robot estimates a bearing to the tag of interest  the robot rotates to that bearing  placing the object within the other sensors field of view  the robot proceeds by performing sensor fusion as previously described  which results in a 3d estimate of the objects location  the robot then uses the 3d estimate of the objects location to approach and grasp the object using an overhead grasp with methods we have previously described  5    6    17   finally  after the grasp attempt is completed  the rfid antennas in the robots end effector  see figure 1  are used to determine success or failure by fig  6 intermediate steps to selecting a beverage bottle  from top to bottom  left to right  the desired object  the raw camera image with the bottle highlighted  the camera probability image  the rssi probability image  the intermediate fusion result  the laser rangefinder mask probability  shown as white points in the image   the selected pixel in red  and the projection of this pixel back into the 3d laser rangefinder data  note  in this example  the fused result is correct  while the color histogram alone is ambiguous and yields an incorrect result  fig  7 dynamically generated user interface presenting a menu of tagged objects available to be grasped by the robot  fig  5 rssi feature probability distributions determined from 60 hand-labeled training examples  prssi|tag  rssi  true  is on the top ; prssi|tag  rssi  false  is on the bottom  confirming the tag id of the object being grasped  in our experimental work thus far  we classify experimental trials as successes or failures depending on whether the correct object is successfully grasped  a bearing estimation in order to successfully fuse images from multiple sensors  the robot must servo its pose so that the field of view of all sensors includes the desired object  in previous work we rotated the robot toward the tagged object by maximizing selected-tag rssi from an antenna mounted at a fixed pose on the robot  3   in the present work the pan/tilt antennas mounted on the robot permit keeping the robot in a fixed position while scanning only the rfid antennas  the rssi values from the two pan/tilt antennas are combined to form a dataset of rssi versus robot bearing  we obtain the bearing from the robot to the tag by fitting to a secondorder exponential parameterized by      t using least squares  n o 2 argmaxx e  x  / +  6  this operation is shown graphically in figure 8 the bearing process is repeated twice to account for the uncertainty behind the robot where the articulated antennas can not sense tags due to mechanical interference with the robots body  fig  8 rssi readings from the two antennas  in this instance  the left antenna had no reads  fitted with a 2nd order exponential function  the argmax   from this fit is the estimated bearing to the tagged object  vi  r esults a evaluating 3d location estimation we performed a number of tests of the sensor fusion systems accuracy when estimating tagged object locations in 3d  for our test scenario  we chose three objects with distinct color histograms  a red water bottle  a blue medication box  and an orange disposable beverage bottle  shown in the top of figure 9 we chose two cluttered but unobstructed scenes and three locations  shown in figure 9  within each scene where each of the three objects was tested  resulting in a total of 18 3d location estimation trials  the algorithm from figure 6 was executed for each trial and was deemed successful if the 3d point derived from the fused image belonged to the desired object  the 3d location estimation was successful in 17 of the 18 trials  94.4 %   with the only failure occurring for the orange disposable drink bottle due to a proximal orange distractor in the color histogram image  it is worth noting that the success rate without the rssi image on the same dataset was 15 of 18  83.3 %  ; thus  incorporation of the rssi image resulted in an 11.1 % improvement in the systems performance  fig  9 three test objects  red water bottle  blue medication box  and orange disposable bottle  top   the two scenes and their three associated object placement locations are indicated  the sole failure occurred when the orange disposable bottle was placed in the upper-right placement location in the bottom scene  fig  10 three different bearing estimation scenarios  with object locations highlighted  bearing estimation was attempted for all three objects  each in the three different locations  9 total attempts   in 8 of 9 instances  the robot correctly achieved a bearing that placed the tagged object in the fused sensor image  b evaluating bearing estimation we tested bearing estimation in 3 different positions  for the same three objects used in the fused image experiments  the bearing estimation cases  illustrated in figure 10 where successful in 8 of 9 trials  88.9 %   where success was defined by halting with the desired object within fused images field of view  c evaluating mobile manipulation we performed three tests of the entire mobile manipulation system  in all three trials  the robot successfully grasped the correct object and verified the id of the object post-grasp using the rfid antennas in the manipulator  see figure 1   vii  l imitations and f uture w ork the methods we propose have a variety of limitations which may be mitigated in future work  the performance of rfid tags can vary considerably depending on their orientation  the materials composing the object  and the rf properties of the environment  e.g  transmission  absorption  reflection  multipath interactions  etc   we expect that issues with orientation and some forms of environmental obstruction can be mitigated by affixing multiple tags to the same object  or by using recently-developed uhf rfid tags with improved omnidirectional performance  recently developed uhf rfid tags have also been introduced for challenging object materials  including metal objects which would not work well with the tags we used in our experiments  faster methods to acquire scanned rfid data  including digitally scanned antenna arrays  would have a variety of advantages  including the ability to handle dynamic environments and make additional estimates from different perspectives  we expect that flash lidar range cameras and digitally scanned rfid antenna arrays could achieve performance at rates that are comparable to conventional video camera framerates  being able to quickly make additional estimates from various perspectives would be advantageous for overcoming environmental rf issues and could be inte r eferences fig  11 two camera images  top row  and corresponding gaussian-filtered rssi images  bottom row  of a tagged bottle inside the top drawer of a wooden cabinet being moved from left to right  the strongest rssi signals are depicted in red and correspond with the location of the bottle in the images  grated into 3d estimation techniques related to our previous work on particle filters  4   the choice of discriminating features for each sensing modality is critical to the robustness of the system  in this work we used color histograms as a straightforward example  but an unfavorable environment could easily lead to confusion  for future work  we plan to incorporate additional descriptive features from the various sensing modalities  further  as shown in figure 11  the rssi is informative even when the remaining sensors can not perceive the desired object  we believe this represents an interesting avenue for further research  viii  c onclusions we have presented an integrated set of methods that enable a mobile manipulator to grasp an object to which a selfadhesive uhf rfid tag has been affixed  among other contributions  we have introduced the use of rssi images to help detect and localize tagged objects  along with a framework for estimating a tagged objects 3d location using a fused sensory representation and sensory features associated with the unique identifier obtained from the objects rfid tag  we evaluated our methods using a robot that first scans an area to discover which tagged objects are within range  creates a user interface  orients to the user-selected object using rf signal strength  estimates the 3d location of the object using an rssi image with sensor fusion  approaches and grasps the object  and then uses its finger-mounted antennas to confirm that the desired object has been grasped  this work demonstrates that rfid-based perception has the potential to become integral to all aspects of mobile manipulation including the discovery of what objects are available  the production of customized user interfaces  the navigation of the robot to objects  and the manipulation of objects   1  epc global us  class 1 generation 2 uhf rfid protocol for operation at 860mhz-960mhz  version 1.0.9  available online  http  //www.epcglobalus.org/   2  d joho  c plagemann  and w burgard  modeling rfid signal strength and tag detection for localization and mapping  in ieee international conference on robotics and automation  2009   3  t deyle  c anderson  c c kemp  and m s reynolds  a foveated passive uhf rfid system for mobile manipulation  intelligent robots and systems  2008 iros 2008 ieee/rsj international conference on  pp  37113716  2008   4  t deyle  c c kemp  and m s reynolds  probabilistic uhf rfid tag pose estimation with multiple antennas and a multipath rf propagation model  intelligent robots and systems  2008 iros 2008 ieee/rsj international conference on  pp  13791384  2008   5  y s choi  c d anderson  j d glass  and c c kemp  laser pointers and a touch screen  intuitive interfaces to an autonomous mobile robot for the motor impaired  in acm sigaccess conference on computers and accessibility  2008   6  h nguyen  c d anderson  a j trevor  a jain  z xu  and c c kemp  el-e  an assistive robot that fetches objects from flat surfaces  in robotic helpers  int  conf  on human-robot interaction  2008   7  m shiomi  t kanda  h ishiguro  and n hagita  interactive humanoid robots for a science museum  in proceedings of acm sigchi/sigart conference on human-robot interaction  new york  ny  usa  acm  2006  pp  305312   8  o kubitz  m berger  m perlick  and r dumoulin  application of radio frequency identification devices to support navigation of autonomous mobile robots  in proceedings of ieee 47th vehicular technology conference  vol  1  4-7 may 1997  pp  126130   9  v kulyukin  c gharpure  j nicholson  and s pavithran  rfid in robot-assisted indoor navigation for the visually impaired  in proceedings of ieee/rsj international conference on intelligent robots and systems  vol  2  28 sept.-2 oct 2004  pp  19791984   10  d hahnel  w burgard  d fox  k fishkin  and m philipose  mapping and localization with rfid technology  in proceedings of ieee international conference on robotics and automation  vol  1  26 april-1 may 2004  pp  10151020   11  v ziparo  a kleiner  b nebel  and d nardi  rfid-based exploration for large robot teams  in proceedings of ieee international conference on robotics and automation  10-14 april 2007  pp  4606 4613   12  m kim  h w kim  and n y chong  automated robot docking using direction sensing rfid  in proceedings of ieee international conference on robotics and automation  10-14 april 2007  pp  4588 4593   13  segon roh  y h lee  and h r choi  object recognition using 3d tag-based rfid system  in proceedings of ieee/rsj international conference on intelligent robots and systems  oct 2006  pp  5725 5730   14  r katsuki  j ota  y tamura  t mizuta  t kito  t arai  t ueyama  and t nishiyama  handling of objects with marks by a robot  intelligent robots and systems  2003   iros 2003   proceedings  2003 ieee/rsj international conference on  vol  1  pp  130135 vol.1  oct 2003   15  n y chong  h hongu  k ohba  s hirai  and k tanie  a distributed knowledge network for real world robot applications  intelligent robots and systems  2004   iros 2004   proceedings  2004 ieee/rsj international conference on  vol  1  pp  187192 vol.1  sept.-2 oct 2004   16  a johnson and m hebert  using spin images for efficient object recognition in cluttered 3d scenes  pattern analysis and machine intelligence  ieee transactions on  vol  21  no  5  pp  433449  may 1999   17  a jain and c c kemp  behavior-based door opening with equilibrium point control  in rss workshop  mobile manipulation in human environments  2009  