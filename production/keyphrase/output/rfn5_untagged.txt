distributed inference and query processing for rfid tracking and monitoring zhao cao  charles sutton  yanlei diao  prashant shenoy department of computer science university of massachusetts  amherst school of informatics university of edinburgh caozhao  yanlei  shenoy @ cs.umass.edu  csutton @ inf.ed.ac.uk abstract tal before it was misplaced  such tracking queries are location queries that require object locations or location histories  in this paper  we present the design of a scalable  distributed stream processing system for rfid tracking and monitoring  since rfid data lacks containment and location information that is key to query processing  we propose to combine location and containment inference with stream query processing in a single architecture  with inference as an enabling mechanism for high-level query processing  we further consider challenges in instantiating such a system in large distributed settings and design techniques for distributed inference and query processing  our experimental results  using both real-world data and large synthetic traces  demonstrate the accuracy  efficiency  and scalability of our proposed techniques  1 containment queries  which include queries such as raise an alert if a flammable item is not packed in a fireproof case  or verify that food containing peanuts is never exposed to other food cases for more than an hour  this class of queries involve inter-object relationships  e.g  containment between objects  cases  and pallets  and are useful for enforcing packaging and shipping regulations  hybrid queries  which include for any temperature sensitive drug product  raise an alert if it has been placed outside a freezer and exposed to room temperature for 6 hours  this class of queries combine sensors streams  e.g  temperature  and rfid streams  e.g  object location and containment  to detect various conditions  unfortunately  the nature of rfid data makes these queries difficult to answer  the key challenge is that although such anomaly detection queries typically involve object locations and inter-object relationships such as containment  the rfid data does not directly contain this information  rather  the data contains only the observed tag id and the reader id ; this is a fundamental limitation of rfid technology  to enable queries on the data that is not actually available  the key is to exploit statistical regularities in the tag id and reader information so that one can estimate object locations and object relationships  the estimation problem is complex  however  because rfid readings are inherently noisy due to the sensitivity of radio frequency to occluding metal objects and interference  7   for example  in our lab setup  section 5.2   we observed read rates of 70 % -85 % even with state-of-the-art readers and tags  real deployments in complex environments such as hospitals would be expected to experience similar issues  a second key challenge is that environments such as large hospitals or supply chains are distributed in scope  for which a centralized approach may be limiting  in a centralized approach  rfid streams from various readers are sent to a central location for query processing  this approach can fail to scale because of the bandwidth overheads incurred due to high data volume and can also potentially increase the latency of detecting anomalous events  especially in geographically distributed settings  in contrast  a distributed approach processes data streams as they emerge  thereby reducing the delay of answering queries  however  as objects move from one location to another  tracking and monitoring queries must also move with these objects  to do so  both the state of objects and the state of monitoring queries relevant to these objects must be transferred to the new location to seed the computation there  research contributions  in this paper  we present the design of a scalable  distributed stream processing system for rfid tracking and monitoring  our system combines location and containment introduction rfid is a promising electronic identification technology that enables a real-time information infrastructure to provide timely  highvalue content to monitoring and tracking applications  an rfidenabled information infrastructure is likely to revolutionize areas such as supply chain management  healthcare  and pharmaceuticals  9   consider  for example  a healthcare environment such as a large hospital that tags all pieces of medical equipment  e.g  scalpels  thermometers  and drug products for inventory management  each storage area or patient room is equipped with rfid readers that scan medical devices  drug products  and their associated cases  such an rfid-based infrastructure offers a hospital unprecedented near real-time ability to track and monitor objects and detect anomalies  e.g  misplaced objects  as they occur  the use of rfid tags provide similar benefits in distributed supply chains where objects  cases and pallets must be tracked  and in pharmaceutical environments that require combating counterfeit drugs and preventing pilfering  to illustrate  consider the following types of continuous queries that may be posed on the rfid streams  tracking queries  which include queries such as report any pallet that has deviated from its intended path  or list the path taken by a medical device equipment through the hospithis work has been supported in part by nsf grants iis-0746939  iis-0812347  and cns-0923313  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page  to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee  articles from this volume were invited to present their results at the 37th international conference on very large data bases  august 29th  september 3rd 2011  seattle  washington  proceedings of the vldb endowment  vol  4  no  5 copyright 2011 vldb endowment 2150-8097/11/02  $ 10.00  326 inference with stream query processing into a single architecture  with inference as an enabling mechanism for high-level query processing for tracking  monitoring  and anomaly detection  we further scale such inference and thus enable query processing in large distributed environments that span multiple sites and numerous objects  more specifically  our contributions include the following  novel statistical framework  section 3   the key novelty in our approach to location and containment inference is to introduce the notion of smoothing over object relations  whereas all existing work on rfid data cleaning  8  11  and location inference  14  16  is limited to the traditional approach of smoothing over time  in contrast to temporal smoothing approaches  smoothing over containment in our work leads to a much simpler graphical model  thereby allowing more efficient inference  at the same time  our model and inference techniques can still accurately estimate location and containment information  so that high-level query processing can return high-quality answers  our general approach is as follows   i  our probabilistic model describes a physical world comprising object locations  containment relationships  and noisy rfid readings   ii  we devise an inference algorithm  called r f i nfer  for our model  working within an expectation maximization  em  framework  the design of our model allows us to derive a simple customized m-step  which is essential for working at scale but still offers provable optimality  furthermore  our algorithm is developed in an unsupervised learning framework ; that is  it does not use machine learning techniques that require access to any specially-generated training data   iii  we finally extend our algorithm to also detect changes of containment using a statistical method called change point detection  distributed inference and query processing  section 4   to suit the increasing scale of rfid tracking and monitoring  we develop a distributed approach that performs inference and query processing locally at each location  but transfers the state of inference and state of query processing as objects move across sites  a naive inference algorithm would incur high transfer overhead by requiring the entire history of observations collected from multiple sites over a long period of time  instead  we propose to truncate history by sifting out the observations most informative about the true containment  and further distill such useful history into a few numbers for each object to minimize the inference state transferred  in distributed processing of tracking and monitoring queries  the main issue is that we need to transfer one copy of query state for each object  our work exploits the inference results  in particular  stable containment to share query state among objects  performance evaluation  section 5   our evaluation  using both real-world data and large synthetic traces  shows the following   i  our inference algorithm is highly accurate  with less than 7 % error on containment and 0.5 % error on location  for noisy traces with stable containment   ii  with containment changes  our algorithm can achieve 85 % accuracy when read rates reach 0.7 while keeping up with stream speed  as shown using real lab traces and simulations   iii  our distributed inference method offers 3 orders of magnitude reduction in communication cost over a centralized method without compromising accuracy  and scales to millions of objects over multiple sites   iv  our highly accurate inference allows a query processor to produce high-quality answers and further exploit sharing of query state across objects for state migration  2 houses  etc  for ease of exposition  the rest of this paper assumes that the environment is a distributed supply chain ; however  our techniques are general and can be applied to other domains as well  each item in the supply chain is assumed to be packed into a case  and multiple cases packed onto a pallet  which yields a containment relationship between items  cases and pallets  items  cases  and pallets are assumed to be tagged  each tag has a unique identity ; the tag id can also indicate the level of packaging  e.g  a pallet  a case  or an item  we focus on passive rfid tags  which are batteryless and have a small amount of on-board memory  e.g  4-64 kb  this memory is writable and can be exploited to store supply-chainspecific object state and enable querying anytime anywhere.1 we assume that each distribution center employs multiple rfid readers  for example  at the entry and exit points as well as at the belt and shelves to scan resident objects  each such reader periodically interrogate tags in its read range and immediately returns the sensed data in the form of  time  tag id  reader id   the local servers of a distribution center collect raw rfid data streams from all readers and process these streams  the data streams from different centers are further aggregated to support global tracking and monitoring  we next illustrate the tracking and monitoring queries that our work aims to support  such queries assume an event stream with rich information including  time  tag id  location  container  and optional attributes describing object properties  such as the type of food or type of container  which can be obtained from the manufacturers database   note the different schemas for raw rfid readings and events used in query processingevents in the latter schema are produced by an inference module as we discuss shortly  query 1 is an example of a hybrid query that combines object locations  containment relationships  and temperature sensor readings  this query raises an alert if a frozen food or drug product has been placed outside a freezer and exposed to room temperature for 6 hours  the query is written using the cql language  2  with an extension for pattern matching  1   the inner  nested  query checks for each product if its container is not a freezer or does not exist  and if so retrieves the temperature based on the products location  the outer query aggregates the retrieved temperatures for the product and checks if it has been exposed to room temperature for 6 hours  the query finally returns all the temperature readings in the 6 hour period and the tag id of the object  q1  select tag id  a   .temp from  select rstream  r.tag id  r.loc  t.temp  from products  now  as r  temperature  partition by sensor rows 1  as t where  !  r.container isa freezer  or r.container = null  and r.loc = t.loc and t.temp > 0 c  as global stream s  pattern seq  a+  where a  i  .tag id = a  1  .tag id and a  a.len  .time > a  1  .time + 6 hrs  3 inference algorithm in this section  we present our inference module that translates raw noisy rfid readings   time  tag id  reader id   into highlevel events with rich attributes  time  tag id  location  container  and optionally other attributes about object properties from the manufacturer  our solution to this problem makes use of techniques from probabilistic reasoning  statistics  and machine learning  background in this section  we provide background on rfid technology and rfid tracking and monitoring applications  our system targets any distributed environment with multiple locations such as hospitals with multiple storage areas  supply chains with multiple ware 1 this technology trend motivated us to minimize the computation state associated with a tag  as discussed in section 4  so it can be held in a tags local memory to enable querying anytime anywhere in the future  327 time 1 3 t=1 containers 4 t=2 2 5 6 objects locations  1 a 3 4 b 5 1 6 c 3 1 2 4 d e container 1 t=4 t=3 2 5 6 c 3 2 4 f e 5 t  c=1 6 container 2 t  o=2 t  c=2 t  o=1 d figure 1  example of noisy rfid readings and containment changes t  o=3 reader 1 reader 1 2 2 r  xt  c=1 yt  o=1 yt  o=2      intuitively  the idea is that whenever an object is read  its container is likely to be read as well  over time  we can use the colocation history of containers and objects to derive the containment relationships  to develop this intuition into a robust system  however  several design considerations must be addressed to effectively handle the noisy and incomplete input  to explain these considerations  we use the example in figure 1  each node represents a tag  and each edge a containment relation  the shaded nodes represent tags that were read  by the reader specified in the bottom row   and the unshaded nodes are tags that were missed by all readers  a main design consideration is how to handle missed readings  if some objects are not observed  it is difficult to accurately determine their locations  which makes it also difficult to tell when objects are co-located  if the containment relations were known for certain  then a powerful way to determine object locations would be to smooth over containment relations  meaning that whenever we read one object in a container  we know that all of the other objects must be in the same place  for example  in figure 1  at time t = 3  we miss reading container 2  but we do read object 5 if we knew that container 2 contained object 5  then we could correctly infer that container 2 is also present at location c unfortunately  the containment relationships are not known in advance  so instead we use an iterative approach  first  we start with the best available information about object locations and have a guess about containment relationships based on co-location  then we can improve our understanding of object locations via smoothing over containment relationships  for example  in figure 1  container 2 and object 5 are repeatedly co-located in the raw readings  so we can infer a containment relationship right away  given the containment relationship  we can infer the location of container 2 at t = 3 the resulting better understanding of locations allows us to further improve our understanding of containment relationships  revisit figure 1 we did not have strong evidence about the container for object 6  but with the new location information about container 2  we see that it is consistently co-located with object 6 a second main design consideration is how to detect changes in containment relationships  consider an object and a container that have been consistently co-located  such as container 1 and object 4 in the first two time steps of figure 1 if later on  t = 3 in the example   we fail to read the object  then following the idea of smoothing over containment  it is reasonable to infer than object 4 is still co-located with container 1 but at some point  if we repeatedly fail to read the object  as at t = 4   we may suspect that the object has actually been moved  to distinguish between these two competing explanationseither the object has been removed from the container  or it has not moved but its tag has been missedwe need a way to decide when there is enough recent evidence to conclude that the containment relationship has actually changed  how much evidence is enough should depend on the read rate  if the readers are less accurate  then we ought to demand more evidence  we resolve all of these difficulties in a principled way using a general methodology based on graphical modeling  we design a graphical model that represents the probabilistic dependencies between the observed rfid readings and the latent object locations and containment relationships  inference algorithms infer containment and locations in a unified way  naturally smoothing over the dependencies between them  in the following  we propose t  o=4 xt  c=2 yt  o=3 yt  o=4 r figure 2  graphical model of locations and rfid readings  a graphical model  section 3.1  and a new algorithm r f i nfer  section 3.2  for inferring containment and location from rfid readings  containment changes can be further detected using a change point detection algorithm  section 3.3   moreover  inference must run at stream speed  which poses a challenge to existing machine learning methods  the techniques we employ include optimizations  appendix a.3  and history truncation  section 4   3.1 graphical model in this section we describe a probabilistic model of container locations  object locations  and rfid readings  the model is a probability distribution over random variables that represent both the true state of the world  which we do not observe  and the rfid readings  which we do  for the purposes of describing the model  we assume that we know the containment relationships exactly ; in fact  we infer them from rfid data  as explained in section 3.2 we discretize both time and space  we divide time into a set of discrete epochs of  for example  one second in duration  all rfid readings that occur in the same epoch are treated as simultaneous  as for locations  given the set of tracking and monitoring queries we aim to support  it suffices to localize objects to the nearest reader  therefore  we model locations as a discrete set r  which is the set of locations of all of the static readers  finally  we assume that there are c containers  denoted by integers c  1  c   and there are o objects  denoted by integers o  1  o   the random variables in the model are as follows  for each epoch t  and each container c  let `tc be the true location of the container  this is a random variable which takes values from the set of locations r similarly  let `to be the true location of each object o as for the readings  let xtrc be a binary random variable that indicates whether the reader at location r r received a reading of the container c define ytro similarly for each object o to make the notation more compact  let ` =  `tc |t  c   `to |t  o  be the vector of all the true object and container locations over all time  and similarly define x =  xtrc |t  r  c  for the container readings and y =  ytro |t  r  o  for the object readings  the model is a joint distribution p  `  x  y  over all of these random variables  our model is depicted graphically in figure 2 for a single epoch  to describe the model  we explain how to sample from the probability distribution that describes the world  assuming that the world behaves exactly according to our model  at every epoch t  first the true location `tc is sampled for each container c because we do not assume any prior knowledge about the layout of the factory  we model this distribution as uniform over the set of all possible locations r now there is no need to sample object locations  because each object must be in the same place as its container  now we can generate the rfid readings  each reader has a read rate  which we denote  r  r   which is the chance of the reader at location r reading an object which is actually at location r typically  a reader detects an object if both are at the same location  328 however  with a small chance a reader can detect an object that is closer to a nearby reader  in an actual deployment  one can measure the read rates periodically by using reference tags fixed to known locations and listening for these tags responses to a given number of interrogations  11  16   to create readings  each reader independently interrogates the tag on every container and the tag on every object  formally  each binary observation variable xtrc is sampled independently with probability according to the read rate ; that is  xtrc is true with probability  r  `tc   we write this probability as   r  `tc  if xtrc = 1  tag read   1  p  xtrc |`tc  = 1  r  `tc  if xtrc = 0  otherwise   given our current guess of the container locations  we iterate these two steps until the containment relations do not change  in the e-step  the distribution that we want to compute is the conditional distribution p  `|x  y  over the location of each container that results from the joint distribution of eq  3  this distribution is called the posterior distribution of the container location and sometimes denoted as qtc   for simplicity  from the definition of conditional probability  it can be shown that p  `tc |x  y  t y c y p  `tc  t=1 c=1 y p  xtrc |`tc  rr y t x c x t=1 c=1 log x ar p  `tc  y p  ytro |`to  o|  o  c  c rr p  xtrc |`tc  y wco = t x x t=1 ar p  xtrc |`tc  y p  ytro |`to    4  o|  o  c  c p  `tc = a|x  y  x log p  ytro |`to = a    5  rr this score measures how likely are the readings of object o if it were always co-located with container c to estimate the container for o  we simply pick the best container c  o  = arg max wco  note that r f i nfer also computes location information  when the algorithm has converged  the final values of p  `tc |x  y  are our best estimates of the location of each container at each time step and the locations of objects believed to be in the container  finally  the following theorem states that our algorithm is guaranteed to converge to an optimum of the likelihood  theorem 1 the r f i nfer algorithm converges  and the resulting values c are a local maximum of the likelihood defined in eq  3   the proof is given in the appendix a the key step is to show that our simple  custom m-step indeed maximizes the likelihood  complexity  optimizations  and extensions  we refer the reader to the appendix a for the complexity analysis  implementation  optimizations  and extensions of our algorithm  after a series of optimizations  our algorithm achieves a linear complexity  o  c + o   in each iteration  and usually converges in just a few iterations  p  ytro |`to  o|  o  c  c  3  the log likelihood measures how probable the rfid readings are under the current set of containment relationships  it will be an important quantity for inferring the containment relationships  3.2 y where s is a constant that does not depend on `tc  in the m-step  we update the current estimates of containment relationships based on the current belief about locations  we do so by defining a score wco to measure the strength of co-location between object o and container c   2  it can be seen that this model treats all time steps as independent and all containers as independent  for each epoch and container  it iterates over all readers and considers the probabilities of each reader observing the container as well as its contained objects  because the model treats all epochs as independent  it does not perform any temporal smoothing over readings ; however  it compensates for this by smoothing over containment relations instead  to smooth the readings over time as well would add significant complexity to the model  and significant computational cost to the inference procedure  in section 5  we verify experimentally that smoothing over containment relations is effective at inferring object locations  an important quantity is the probability p that the model assigns to the observed data  that is  p  x  y  = ` p  `  x  y   this quantity is called the likelihood of the data  note that the likelihood is a function of the containment relationships c to emphasize this  we define l  c  = log p  x  y   according to our model  this is l  c  = s rr and similarly for ytro  putting it together  this defines a joint probability distribution as p  `  x  y  = = 3.3 change point detection in this section  we describe how we infer changes in containment relationships  this type of problem  called change-point detection  is the subject of a large literature in statistics  see  3  for an overview   a change point is a time t at which the containment relationships change  that is  some object has either changed containers or been removed altogether  finding change points is challenging because of the noise in rfid readings  for example  in figure 1 at t = 4  it may be unclear if object 4 has actually been removed from container 1  or it has simply been unlucky enough to be missed twice in a row  to distinguish these two possibilities  we need a way to quantify the unluckiness of a set of readings  we propose a statistical approach based on hypothesis testing  suppose that we have received readings from epochs  0  t   then we define a null hypothesis  which is that the containment relationships have not changed at all during epochs  0  t   then  if under the null hypothesis  it turns out that the observed rfid readings are highly unlikely  we reject the null hypothesis  concluding that a change point has in fact occurred  to measure whether the observed readings are unlikely  we again use the likelihood eq  3   consider a single object o let c0  t be the maximum likelihood containment relations based on the full data  so that l  c0  t  is inferring containment relationships to infer containment relationships from rfid readings  we use a maximum likelihood framework  that is  we determine the containment relationships such that  according to the model  the observed readings are most likely  formally  this amounts to maximizing the log likelihood l  c  with respect to the set of containment relationships c in this section  we describe the algorithm that performs this maximization  which we call r f i nfer  the idea is that determining containment relationships would be simple if  besides the rfid data  we also observed the true locations of all containers  however  the true container locations are in fact unknown  to handle this  we develop r f i nfer in the em framework  which offers a general approach for maximizing likelihood functions in the presence of missing data  in our case the container locations  the algorithm alternates between two steps  in the first step  the expectation step  or e-step   we infer a distribution over the locations of each container  given some current guess about the containment relations  in the second step  the maximization step  or m-step   we choose the best containment relations 329 20 global proc  state migration local proc  object events  tag  loc  cont    sensor readings state migration inference site 1 rfid readings  tag  reader  time  -40 -60 -80 -100 r nrc nrnc -120 -140 40 -1 -2 r nrc nrnc -3 -4 60 80 100 120 140 160 180 200 time t 60 80 100 120 140 160 180 200 time t state migration can be realized in several ways   i  when an object is scanned at the exit of a site  if domain knowledge about its next location is available  its inference and query processing state can be transferred directly to that site   ii  alternatively  when an object reaches a new site  the server there can locate the objects previous place using the object naming service  ons  and retrieves its state from that place   iii  finally  it is desirable to write the objects state to the local storage of the rfid tag  once the technology of writable tags matures for large deployments   while leaving a copy of the state at the current site as backup  this method will enable querying instantly when a tag is in sight  with minimum delay of answering queries and minimum communication costs  to reduce communication costs or cope with limited local tag storage  it is important to minimize inference and query processing state while ensuring accuracy of query answers  we address this issue in both inference and query processing as described below  figure 3  a distributed rfid data management system  the best possible likelihood if there is no change point  alternatively  suppose there is a change point at some time t0  then let c0  t0 and ct0  t be the best containment relations that allow object o to change locations at time t0  maximizing over possible change points  the best possible likelihood if there is any change point for o is maxt0 l  c0  t0  + l  ct0  t   we perform change point detection using the difference of these two log likelihoods  that is  o  t  = l  c0  t  0max  l  c0  t0  + l  ct0  t   40  a  cumulative evidence  b  point evidence figure 4  evidence of co-location of three candidate containers  site 3 site 2  6  essentially  this measures how much better we can explain the data if we use two different sets of containment relationships instead of one  this is a type of generalized likelihood ratio statistic  which is a fundamental tool in statistics  the change point detection procedure will signal that there has been a change point whenever the value of o  t  is greater than a threshold  intuitively  to choose the threshold we would like to know what values of o  t  would be typical if there were no change point  fortunately  we can obtain as much of this data as we want  simply by sampling hypothetical observation sequences from the model  exactly as described in section 3.1 since none of the hypothetical sequences actually contain a change point  if our procedure signals a change point on one of them  it must be a false positive  in practice  all of the hypothetical o  t  values are quite small  so we choose to be their maximum  furthermore  all of this computation can be done in advance before any rfid data is observed  the details of the change point detection procedure are given in appendix a.2  4  -20 -160 objects  tags  t  0  t  0 0 point evidence  log  cumulative evidence  log  query processing 4.1 state migration for inference our inference algorithm presented in the previous section requires the entire history of readings associated with each object produced from all the sites that this object has passed  when an object leaves one site for another  the history of this object and the history of all of its possible containers  collectively called the inference state of the object  need to be transferred to the new location for subsequent inference  evidently  transferring the complete history of objects and containers would incur both a high communication cost across sites and a high processing cost at the new location  below  we describe two techniques to address these problems  truncating history  the goal of history truncation is to sift out the observations that are most informative about true containment relationships from history  and retain only those for future processing  this can be accomplished by monitoring the strength of co-location computed in our containment inference algorithm r f i nfer  recall from eq  5  for the m-step of the r f i nfer algorithm  the co-location strength wco for each object o and container c is a sum over all time steps of a quantity which we call the point evidence of co-location  we denote this quantity by  x x eco  t  = qtc  a  log p  ytro |`to = a    7  distributed processing as object tracking and monitoring systems grow into many geographically separate sites and millions of objects  the sheer volume of data poses a scalability challenge  a centralized approach  like centralized warehousing  requires all the data to be transferred to a single location for processing  this approach incurs both delay of answering queries and high communication costs  in this work  we propose a distributed approach natural for object tracking and monitoring  which performs querying where an object  and data  is located  the architecture of such a distributed system is illustrated in figure 3 as can be seen  each site performs inference and query processing on local rfid streams as objects are observed  inference runs on raw rfid streams and produces an object event stream describing the location and container of each object  query processing runs continuously on the object event stream and other sensor streams to return all answers  inference and query processing  however  often require information from the previous sites that an object has passed  to solve this problem  we perform state migration  which transfers the state of inference and query processing for an object when it moves across sites  ar rr then the cumulative evidence of co-location can be computed as p eco  t  = tt0 =1 eco  t0   to see how these quantities are used  suppose that in a warehouse an object started at the entry door at time 0  was scanned on the conveyor belt around time 100  and then placed on a shelf at time 150 consider three candidate containers that were co-located with this object at the entry door  the real container  denoted by r  always traveled with the object ; a second container  n rc  was co-located at the door and at the shelf  but not at the belt ; a third container  n rn c  was not co-located after the door  figure 4  a  shows the cumulative evidence of co-location of three candidate containers with the object  around time 100  the belt reader scanned the real container alone with the object  causing the cumulative evidence of 330 4.2 the other two containers to drop fast  this is exactly the informative region we want to find in history truncation  the information afterwards is less useful  because the false container n rc is colocated with the object again on the shelf  while the false container n rn c was already eliminated from contention by the belt reader  our history truncation algorithm aims to find a time period  called the critical region  whose observations are most informative for determining containment  while our intuition was explained using the cumulative evidence of co-location  our algorithm actually uses the point evidence of co-location  as shown in figure 4  b   in log space   during the critical region around time 100  the real container has much higher point evidence than the two false containers ; this is not true either before or after the region  after containment inference completes  our history truncation algorithm runs as follows  it searches through time by applying a small sliding window  tw  t   given the current p window  for each object o  it computes the sum of point evidence tt0 =tw eco  t0  for each possible container of o if the difference in sum between the best container and the second best is large enough  using a heuristic-based threshold   the current window is considered a critical region cr of the object  overwriting the previous cr if existent  when the search reaches the end  the most recent cr is the final critical region of the object  readings of the object and its possible containers outside the critical region will be all ignored  after running the algorithm  we have compressed the entire history from  0  t  to a small region cr  when the new readings arrive in the time period  t  t 0   rather than running inference over the entire period  0  t 0   we run inference only over the data in the the critical region cr and in recent history denoted by h if containment is stable  it suffices to have h =  t  t 0   i.e  including all new readings obtained since last inference  to support change point detection  however  we may need a somewhat larger recent history h according to eq  6   the change point can be any point in the entire history  in practice  it is more likely to be in the recent history since it was not detected last time  however  it may be imprudent to restrict the change point only to the most recent period  t  t 0  because a change point before the time t might not get sufficient evidence in the previous change point detection  our experimental results in section 5.1 show that the sufficient size of h is within a factor of 2 of t 0 -t  as time elapses  the recent history h moves forwards and we can truncate the readings falling behind h by applying the critical region algorithm again  collapsing inference state  when an object leaves a site for the next  the inference state for the object includes the readings of the object and the readings of its candidate containers in both the critical region cr and the recent history h one solution is simply shipping the inference state to the next site to seed inference there  however  the inference state for an object may not be small since each object can have dozens of candidate containers  and each container or object can have hundreds of readings in cr and h in our work we employ a technique to collapse the inference state to a single number for each container-object pair  i.e  the co-location weight wco  hence avoiding the overhead of transferring readings entirely  this dramatically reduces the inference state transferred between sites  then the inference algorithm at a new location simply adds the old transferred weights to the new weights that are computed from the readings at the new site  this technique  however  can affect accuracy  if later evidence shows that the containment inference results from the old location were incorrect  we can no longer revise the old estimates as the corresponding readings have been discarded  even in this case  however  inference in the new place still has a chance to correct the old estimates because readings obtained there will eventually overrule the old weights  state migration for querying given an event stream with object location and containment information  the query processor processes this stream and other sensor streams to answer monitoring queries  our discussion below assumes cql-based relational stream processing  2  extended with the pattern matching functionality  1   under our approach querying where an object is located  a monitoring query is registered with every site  it is split into local processing and global processing parts based on the labels of input streams specified in the query  for each query block  if any of the input streams is labeled as global  then this block uses global processing across sites ; otherwise  it is processed only on the local streams  while local processing can be performed by existing stream systems  1  2   global query processing requires additional mechanisms  first  global query processing needs to maintain computation state for each object  since all stream systems maintain computation state  a.k.a  synopsis  2   and update it with each arriving tuple  our work further partitions the state according to individual objects  then as an object leaves one site for another  we perform state migration using one of the three strategies mentioned at the beginning of the section  see appendix b for illustration of the above approach using query 1 in section 2 a main issue in state migration is that the total amount of state to be transferred can be enormous given a large number of objects  to reduce communication costs  we exploit stable containment to share query states across objects  at the exit point of a storage area  we consider the objects in each container  e.g  frozen food products considered in query 1 these objects have the same container and location at present  but possibly different histories   the query states for these objects are likely to have commonalities  hence  we propose a centroid-based sharing technique that finds the most representative query state and compresses other similar query states by storing only the differences  details are available in appendix b  5 performance evaluation we have implemented a prototype of our inference approach   including the optimizations in appendix a   connected it to a stream query processor  1   and extended both to distributed processing  we evaluate our system using both synthetic traces emulating rfidbased supply chains and real traces from a laboratory setup  5.1 single-site inference we first evaluate our inference algorithm on synthetic rfid streams from a single warehouse  the detailed experimental setup  performance metrics  and additional results are given in the appendix c by default  we run inference every 300 seconds  inference with stable containment  we evaluate our inference algorithm first using traces with stable containment  to deal with traces of various lengths  we consider the critical region  cr  method that we proposed for history truncation in distributed processing  section 4.1  as an optimization also for traces produced at a single warehouse  this method results in the use of the critical region and a short recent history h  by default  the most recent 600 seconds  for inference  for comparison  we also include a simple window-based truncation method that keeps the most recent w readings for inference  w =1200 seconds here   we first test the sensitivity of these methods to the read rate rr  as figure 5  a  shows  while all three methods offer high accuracy for location inference  all three lines for location inference are very close  so we show only the line for location  cr  for readability   they differ widely for containment inference  the window method has the worse accuracy because when the useful observations  such 331 containment  w1200  containment  all  containment  cr  location  cr  time cost  s  error rate  %  8 6 4 2 1200 100 1000 80 f-measure  %  10 800 inference  w1200  inference  all  inference  cr  600 400 0.7 0.8 read rate 0.9 0 600 1.0 1200 1800 2400 trace length 3000  b  basic  all history   fixed window  and history truncation methods with varied trace lengths 15 10 20 15 15 none cr centralized 5 40 60 80 100 containment change interval 120 frequencies  against s murf  20 10 20  c  change point detection with varied anomaly error rate  %  error rate  %  error rate  %  smurf cont  smurf loc  rfinfer cont  rfinfer loc  0 3600 truncation methods with varied read rates 30 rr=0.8 h=500 rr=0.7 h=500 rr=0.8 smurf rr=0.7 smurf 20  a  basic  all history   fixed window  and history 20 40 200 0 0.6 25 60 none cr centralized 10 5 5 0 0 t1 t2 t3 t4 t5 t6 t7 t8  d  r f i nfer vs s murf using real lab traces 0 0.6 0.65 0.7 0.75 0.8 0.85 read rate 0.9 0.95 1  e  distributed inference with varied read rates 20 40 60 80 100 containment change interval 120  f  varied containment change intervals figure 5  experimental results for single-site inference  a-c   our lab warehouse deployment  d   and distributed inference  e and f   given read rates 0.7 while keeping up with stream speed  by using a relatively small h   the details are shown in appendix c.4  frequency of unexpected containment changes  we next test the sensitivity of our algorithm to the frequency of unexpected containment changes  i.e  without using special readers to scan containers separately  we varied the interval between two containment changes  from 10 to 120 seconds  for comparison  we include an alternative method  called s murf  that extends the state-of-theart s murf  11  method for rfid data cleaning with heuristics for containment inference  see appendix c.3 for details   for our algorithm  we chose the h size to keep up with stream speed based on table 4 in appendix c.4  i.e  h=500 for both rr=0.7 and rr=0.8  as figure 5  c  shows  our algorithm is much more accurate than s murf and is not very sensitive to the containment change interval  s murf is much worse because it lacks a principled approach to exploiting the iterative feedback between location and containment estimates  as the belt readings  fall outside the window  the inference algorithm can no longer use them to infer containment  using the entire history or the cr method gives better accuracy as expected  interestingly  while the cr method was initially proposed for improving performance  it also improves over the basic algorithm in accuracy due to the removal of noise  e.g  co-location of a false container and an object on a shelf  from inference  moreover  its sensitivity to the read rate is comparable to that using the full history  which is the best that one can expect  we next vary the trace length from 600 to 3600 seconds and compute the total inference time when using the entire history  the window  and the cr methods in figure 5  b   here we see that using the entire history severely penalizes the performance  the window based truncation stays in the middle  and the cr method performs the best with its running time insensitive to the trace length  containment change detection  we next employ the change point detection algorithm from section 3.3 to detect containment changes  we use a recent history size of h  600 seconds by default  in addition to the detected critical region for inference with change point detection  to generate events of interest  we inject anomalies that randomly choose an item and move it to a different case in the warehouse  the frequency of such anomalies is every 20 seconds by default but also varied over a wide range  each run simulates a warehouse with 32,000 items in steady state over 4 hours  choice of threshold  we first examine the effect of the threshold for change point detection  we consider fixed values in a range as well as our offline method as described in section 3.3 due to space constraints  the details of this study are left to appendix c.4  see table 3   in summary  our chosen threshold always approximates the optimal value within 2 % across all read rates  tradeoff between accuracy and efficiency  we further study the tradeoff between accuracy and efficiency  the change point detection algorithm requires a recent history  whose size is h   besides the critical region in the past  to detect containment changes  results of our study show that a longer recent history helps improve accuracy especially when read rates are low  while it may increase inference cost  overall  our algorithm can achieve 85 % accuracy 5.2 evaluation of lab rfid deployment to evaluate our system in real-world settings  we developed an rfid lab with 7 readers and 20 cases containing 5 items each to simulate a small warehouse  we created 8 traces  labeled t1      t8  with different characteristics regarding the environmental noise and overlap among readers  for details see appendix c.2   we ran inference every 5 minutes using a 10-minute history  or all the data available if the history is less than 10 minutes   for comparison  we include the s murf algorithm described in appendix c.3  figure 5  d  shows the inference error rates for r f i nfer and s murf  as can be seen  r f i nfer is much more accurate than s murf across all traces although they both use intuitions such as smoothing and co-location  again  this is because r f i nfer uses smoothing over containment relations and a principled approach for the iterative feedback between location and containment estimates  this is shown to be more effective than smoothing over time for individual objects and then combining such location evidence in a heuristic way to infer containment as in s murf  for r f i nfer  the location error rates are low across all traces  in the absence of containment changes  the containment error rates are 332 within 5 % in traces t1 to t4 despite the heterogeneous read rates  added environmental noise  and significant overlap between readers  containment changes cause containment error rates to rise  especially given lower read rates or higher overlap rates  but with a maximum of 13 % with all the noise factors combined in t8  5.3 ing with query processing  13   and exploiting known constraints to derive high-level information  19   our system addresses a different problem  it processes raw data streams to infer object location and containment  thereby enabling stream query processing  and scales inference and query processing to distributed environments  inference in sensor networks  various techniques  12  6  15  10  have been used to infer true values of temperature  light  object positions  etc  that a sensor network is deployed to measure  our inference problem differs because the inter-object relationships  such as containment  can not be directly measured  hence requiring different statistical models and inference techniques  we further address distributed inference and query processing for scalability  distributed inference accuracy and communication cost  we next compare centralized and distributed approaches to inference by simulating 10 warehouses for 4 hours  each warehouse has 32,000 items in steady state  totally 0.32 million items  our system runs inference at stream speed for each warehouse  figure 5  e  shows the error rates for varied read rates  the naive no state-transfer method  labeled none  has a high error rate  while our critical region  cr  method perform close to the centralized method  figure 5  f  shows similar results when the containment change frequency varies  the communication costs  shown in appendix c.5  show that our cr methods offer 3 orders of magnitude reduction in communication cost over a centralized approach while approximating its accuracy  scalability  we further test the scalability of our inference system by using larger numbers of objects in simulation  our inference system can scale to 150,000 items per warehouse while keeping up with stream speed  totaling 1.5 million objects over 10 warehouses  the above reported results on accuracy and communication costs stay true  one way to support more objects is to use mobile readers for scanning objects on shelves  which is a more cost-effective deployment than static readers   in another simulation  we use a mobile reader to scan each isle of 90 shelves  the mobile reader reads every second and spends 10 seconds scanning each shelf  given such reduced shelf readings  our inference system can scale to 1.21 million items per warehouse while running at stream speed  totaling 12.1 million objects over 10 warehouses  5.4 7 in this paper  we presented the design of a scalable  distributed stream processing system for rfid tracking and monitoring  our technical contributions include  i  novel inference techniques that provide accurate estimates of object locations and containment relationships in noisy  dynamic environments  and  ii  distributed inference and query processing techniques that minimize the computation state transferred across warehouses while approximating the accuracy of centralized processing  our experimental results demonstrated the accuracy  efficiency  and scalability of our techniques  in future work  we plan to extend our work to include probabilistic query processing  exploit on-board tag memory to hold object state and enable anytime anywhere querying  and explore smoothing over object relations for other data cleaning problems  8 distributed inference and querying q1 q2 6 rr=0.6 89.2 65,500 6,986 93.5 80,248 7,296 rr=0.7 94 66,000 5,737 96.1 85,510 6,108 rr=0.8 95.1 67037 5,589 97.3 87,029 5,341 references  1  j agrawal  y diao  et al  efficient pattern matching over event streams  in sigmod  147160  2008   2  a arasu  et al  the cql continuous query language  semantic foundations and query execution vldb j  15  2   121-142  2006   3  m basseville and i v nikiforov  detection of abrupt changes  theory and application  prentice-hall  1993   4  h chen  w.-s ku  et al  leveraging spatio-temporal redundancy for rfid data cleansing  in sigmod 10  5162  2010   5  n n dalvi and d suciu  efficient query evaluation on probabilistic databases  vldb j  16  4  :523544  2007   6  m cetin  l chen  et al  distributed fusion in sensor networks  ieee signal processing mag  23:4255  2006   7  k finkenzeller  rfid handbook  radio frequency identification fundamentals and applications  john wiley and sons  1999   8  m j franklin  s r jeffery  et al  design considerations for high fan-in systems  the hifi approach  in cidr  290304  2005   9  s garfinkel and b rosenberg  editors  rfid  applications  security  and privacy  addison-wesley  2005   10  a ihler  j fisher  et al  nonparametric belief propagation for self-calibration in sensor networks  in ipsn  225233  2004   11  s r jeffery  et al  an adaptive rfid middleware for supporting metaphysical data independence  vldb journal  17  2  :265289  2007   12  m paskin  c guestrin  et al  a robust architecture for distributed inference in sensor networks  in ipsn  5562  2005   13  j rao  s doraiswamy  et al  a deferred cleansing method for rfid data analytics  in vldb  175186  2006   14  c re  j letchner  et al  event queries on correlated probabilistic streams  in sigmod  715728  2008   15  j schiff  d antonelli  et al  robust message-passing for statistical inference in sensor networks  in ipsn  109118  2007   16  t tran  c sutton  et al  probabilistic inference over rfid streams in mobile environments  in icde  1096-1107  2009   17  f wang and p liu  temporal management of rfid data  in vldb  11281139  2005   18  e welbourne  et al  cascadia  a system for specifying  detecting  and managing rfid events  in mobisys  281294  2008   19  j xie  j yang  et al  a sampling-based approach to information recovery  in icde  476485  2008 we finally extend our distributed inference experiment with query processing  we report results using two representative queries  q1 from section 2  and q2 that reports the frozen food that has been exposed to temperature over 10 degrees for 10 hours  the table below reports the f-measure of query results and the total size of query state with and without the containment-based sharing method  section 4.2   we see that the overall accuracy of query results is high  > 89 %   also  state sharing yields up to 10x reduction in query state size  finally  the accuracy and query state reduction ratio of q1 are lower than those of q2  this is because q1 combines inferred location and containment  but q2 only uses the inferred location which is more accurate than the inferred containment  f-m  %  state w/o share  bytes  state w share  bytes  f-m  %  state w/o share  bytes  state w share  bytes  conclusions rr=0.9 96 67,000 5,156 97.5 87,000 5,273 related work rfid stream processing  recent research has addressed rfid data cleaning  8  and location inference for static readers  11  14  4  and mobile readers  16   however  containment inference is more challenging since inter-object relationships can not be directly observed  our work is the first to employ smoothing over object relations in rfid inference  with demonstrated performance  our work further supports distributed inference and querying  rfid databases  existing work has addressed rfid data archival  17   event specification and extraction  18   integrating data cleans 333 r c o o c t r `tc `to  r  r  ytro xtrc x y wco c l  c  o  t  number of reader locations number of containers number of objects index of a single object ; o  1  o  index of a single container ; c  1  c  index of time epoch  e.g  1 second long  set of possible reader locations true location of container c at time t true location of object o at time t read rate  probability that reader at location r r detects an object at location r r binary variable indicating whether object o was read by reader at location r at time t binary variable indicating whether container c was read by reader at location r at time t binary vector of all container readings binary vector of all object readings strength of co-location between container c and object o containment relations ; set of pairs  object id  container id  likelihood of the observed readings  given containment relations c change-point statistic for epoch t algorithm 1 pseudocode of r f i nfer for inferring containment while not converged do // e step  compute new q for t = 0 to t do // for each epoch for c = 1 to c do // for each container 5 for all a r do // for all possible locations y y p  ytro |`to = a  qtc  a  p  xtrc |`tc = a  rr o|  o  c  c 15 // nowp qtc  a  = s 1 p  `tc = a|x  y  s ar qtc  a  for all a r do // for all possible locations qtc  a  qtc  a  /s // now qtc  a  = p  `tc = a|x  y  // m step  compute new w for o = 1 to o do // for each object for c = 1 to c do // for each container t x x x wco qtc  a  log p  ytro |`to = a  20 // m step  compute new containment set c for o = 1 to o do // for each object c arg maxc  1  c  wco c c   o  c   10 t=1 ar table 1  notation used in this paper rr appendix the notation used in this paper is summarized in table 1 a computation complexity  each iteration of r f i nfer requires o  t cor2  time  where by iteration we mean a single execution of lines 220 this is due to two reasons  first  the computation of qtc  a  in line 6 requires o  or  time  and is executed o  t cr  times by the outer loops  second  the computation of wco in line 15 requires o  t r2  time  and is executed o  co  times by its outer loops  this is the running time of a naive implementation of the algorithm ; in appendix a.3  we describe several optimizations that improve the performance significantly  also  note that this is the computational complexity per iteration  in general  it is difficult to characterize the number of iterations required for em to converge  because this depends strongly on characteristics of the unknown true distribution  however  we observe empirically that our inference algorithm usually converges in just a few iterations  enhancements of rfinfer below we present additional details about our inference algorithm  its implementation and optimizations  and two extensions  a.1 pseudocode and proof of r f i nfer the pseudocode for r f i nfer is shown in algorithm 1 we next prove theorem 1 about the optimality of the r f i nfer algorithm  proof  we show that r f i nfer  algorithm 1  is guaranteed to converge to a local maximum of the likelihood l  c  in  3   following the em theory  we can interpret both the e-step and the m-step as maximizing a lower bound on the likelihood  which is l  c  t x c x x qtc  a  log t=1 c=1 ar p  `tc = a  x  y  = o  c  qtc  a  a.2 the fact that this is a lower bound can be proven by jensens inequality  the e-step maximizes this bound with respect to qtc  and the m-step with respect to c in r f i nfer  the e-step is identical to the standard e-step of em  but we use a custom m-step that is specific to our model  so it suffices to prove that the m-step in r f i nfer indeed maximizes o  c   when maximizing with respect to c  we can ignore terms that do not depend on c expanding o  c  using eq  3  and removing irrelevant terms yields max o  c  = max c c = t x c x x qtc  a  t=1 c=1 ar max  c  o   o  o x x details of change point detection the change point detection procedure works as follows  first  before any data arrives  choose the threshold as described in section 3.3 then  the change point detection is run after each time the r f i nfer algorithm runs  which also provides the null hypothesis  for each object o  we compute o  t  = l  c0  t  0max  l  c0  t0  + l  ct0  t   t  0  t  if o  t  <  then there is no change point for o otherwise  if o  t   then we flag a change point at the time t0 that achieved the maximum in eq  6   moreover  we disregard the data from 0    t0 in all subsequent calls to the change point algorithm  so we do not flag the same change point more than once  in implementation  this procedure incurs little extra cost beyond the computation in the r f i nfer algorithm  recall that in the mstep of the r f i nfer algorithm  the co-location strength wco for each object o and container c is a sum over all time steps of a quantity that we call the point evidence of co-location  denoted x x eco  t  = qtc  a  log p  ytro |`to = a   log p  ytc |`to = a  o|  o  c  c wc  o   o  o=1 where c  o  denotes the container of object o  and ytc =  ytrc |r   in this last equation  notice that each containment decision c  o  that we are maximizing over appears in only one term of the summation  this means that we can find the global maximum p by maximizing each term independently  i.e  maxc o  c  = o maxc0 wc0  o  this is exactly what is computed in lines 1220 of r f i nfer  ar rr p the cumulative evidence of co-location is eco  t  = tt0 =1 eco  t0   the point evidence of object o and container c at each time t is 334 memorized so it can be re-used to calculate l  c0  t0  and l  ct0  t   p l  c0  t0  = eco  t0  and l  ct0  t  = tt=t0 eco  t   if the change pt 0 0 point is detected at time t  we use the wco = t=t0 eco  t  as the new strength of co-location to get the new container for object o all the computation above is simply the sum of the point evidence memorized from the computation of the r f i nfer algorithm  hence change point detection incurs little extra overhead  a.3 implementation and optimizations in this section  we sketch the main data structures and optimizations that we use to implement the r f i nfer algorithm including the change point detection extension  data structures  we use a series of tables   1  the read rate table  size  r r  stores the read rates  r  r    2  two history tables  one for container readings x  size  t r c   and one for object readings y  size  t r o   3  the posterior probability table  t c r  stores the posterior distribution qtc  a  over container locations   4  the weight table  c o  stores the co-location strengths wco   5  finally  the containment table  vector of length o  stores the container inferred for each object  although the history and posterior probability tables grow with time  in section 4 we describe a history truncation method that reduces the memory requirement without sacrificing accuracy  finally  many of these tables  especially the history tables  are sparse  i.e  most cells are 0  and so can be easily compressed to save memory  optimizations  we further employ several optimizations to improve inference efficiency  recall from appendix a.1 that both the e-step and m-step of our algorithm have the complexity o  t cor2   the e-step can be easily improved as each object is typically read in only a small number of locations and each container contains a small number of items  since both of those are bounded independently of r and o  the e-step can be improved by a factor of or2  according to eq  4    then the e-step requires only o  t c  time  regarding the m-step  it can be easily improved by a factor of r2  according to eq  5   again because each object is typically read in only a small number of locations  next  we propose an optimization  called candidate pruning  to improve the m-step further to o  t o   eliminating the factor of c the idea is that in line 15  when computing the container that is most strongly co-located with a given object  it is probably safe to consider only containers that have been observed frequently with the object  so as a heuristic  we restrict the set of candidate containers to those that were most frequently co-located during the first several epochs  when testing for change points  we also include as candidates the most frequently co-located containers from recent epochs  our experimental results show that candidate pruning is effective at reducing the time cost without affecting the accuracy  we further propose a memorization technique that avoids unnecessary computation  if the set of objects in a container did not change in the previous em iteration  then the location probabilities and co-location strengths for that container can not change at the current iteration  so we can simply re-use the old values without any extra work  this optimization does not introduce any error  moreover  we can also use the history truncation method  described in section 4 to truncate the history of size t  to a small critical region whose size is independent of t  all of the above optimizations combined finally reduce the complexities of the e-step and the m-step to o  c  and o  o   respectively  is typically true according to the epc tag data standard2  relaxing this assumption is possible  first  if the containers do not have tags  we simply omit the nodes in the graphical model that correspond to the readings from the container tags  along with the corresponding terms in eq  4  and in line 6 of algorithm 1 alternatively  if all containers and objects have tags  but we do not know which are which  we treat all tags  for containers and objects  as if they were object tags ; we use latent and evidence variables in the graphical model to denote the true and observed locations of these objects  respectively  then  we use another copy of latent variables  `tc  to represent real containers and use them to encode containment relationships with objects  eq  4  and algorithm 1 are modified the same as above  we do not further optimize these cases because they are rare given the wide adoption of the epc tag data standard  hierarchical containment  just as objects are grouped into containers  containers may themselves be stored in larger containers  such as pallets  we can extend our model and algorithms to arbitrarily nested containment hierarchies  intuitively by adding latent variables for the pallet locations whose values are imputed using em in a similar way as the container locations  since common types of queries such as those listed in section 1 rely mainly on the immediate containers of items that are subject to shipping and packaging regulations  we defer a detailed study of hierarchical containment to future work when new applications requiring so emerge  b discussion of query processing we next describe our distributed query processing approach more  first  we illustrate our approach using query q1 in section 2 a monitoring query is split into local and global processing based on the labels of input streams of each query block  for instance  for query 1  the inner query block takes two streams r and t  neither of them is labeled as global  so this block is treated as local processing only  however  the output stream s of the inner query block is labeled as global  and hence the outer query block consuming s is considered for global processing  for global query processing  the computation state is recognized and partitioned for individual objects  revisit q1  the outer query block employs pattern matching on the stream produced by the inner block  an automatonbased query processor  1  defines the query state to be   i  the current automaton state   ii  the minimum set of values needed for future automaton evaluation  e.g  the tag id and the time of its first exposure to room temperature for q1  and  iii  the values that the query returns  e.g  the tag id and the sequence of temperature readings for q1  partitioning the query state for objects is simply based on the tag id  then as an object leaves one site for another  we perform state migration by shipping the query state for this object to the next site or writing the query state into the tags memory  second  we propose to exploit stable containment to share query states across objects  our centroid-based sharing technique works as follows  let qo denote the query state for object o we choose the most representative query state  the centroid  of all qo s based on a distance function that counts the number of bytes that differ in the query state of two objects  the centroid selection problem has a o  n2  complexity  but since each case contains a limited number of objects  e.g  20-50  this computation cost is modest on a modern computer  given the centroid  we compress the query states of other objects based on the distance to the centroid  c additional experiments in this section  we describe additional experimental results beyond the key results presented in the main body of the paper  a.4 extensions to the r f i nfer algorithm missing container tags  we have assumed that we know a priori which tags are container tags and which tags are object tags  which 2 335 http  //www.epcglobalinc.org/standards/tds/ 10 table 2  parameters used for generating rfid streams  c.1 6 4 2 6 4 2 0 0.6 0.65 0.7 0.75 0.8 0.85 read rate 0.9 0.95 1 0 600  a  basic algorithm 1200 1800 2400 trace length 3000 3600  b  history truncation figure 6  experimental results for single-site inference t1  rr=0.85  or=0.25  represents the case of high read rates  an average of 0.85 across readers  and limited overlap rates  an average of 0.25 for shelf readers using low power  t2  rr=0.85  or=0.5  is case of high read rates and significant overlap rates  using high power   an average of 0.5 t3  rr=0.7  or=0.25  involves lower read rates due to added environmental noise  i.e  a metal bar placed on each shelf that is 1/3 the length of the shelf  t4  rr=0.7  or=0.5  further has higher overlap rates  t5 to t8 extend t1 to t4  respectively  with containment changes  when all 20 cases were placed on shelves  3 items were moved from one case to another and 1 item was simply removed  causing containment changes in 35 % of the cases  we also obtained traces with varied tag orientations but observed little impact of this factor  this verifies that squiggle tags are orientation-insensitive when used with circularly-polarized antennas  experimental setup we developed a simulator using csim to emulate an rfid-based enterprise supply chain  the parameters are shown in table 2 each supply chain arranges n warehouses in a single-source directed acyclic graph  dag   pallets of cases are injected at the source  and then move through a sequence of warehouses with a scheduled delay in each warehouse and scheduled transit time between two warehouses  until they reach final destinations  our simulator guarantees that in a period of time  pallets arrive at a warehouse and depart from it at the same rate  i.e  the system is in steady state   within a warehouse  pallets first arrive at the entry door and are read by the reader there  they are then unpacked  by default  each warehouse has a reader at the conveyor belt that scans the cases one at a time  the cases are then placed onto shelves and scanned by the shelf readers  after a period of stay  cases are removed from the shelves and repackaged  the assembled pallets are finally read at the exit door and dispatched to subsequent warehouses in a roundrobin fashion  in the simulation  all readers have a read rate rr for its location  uniformly sampled from  0.6  1  unless stated otherwise  there is significant overlap between adjacent shelf readers  a shelf reader can read objects in a nearby location with probability or uniformly sampled from  0.2  0.8   finally  to stress test our containment change detection algorithm  our simulator can inject anomalies that randomly pick an item and place it in a different case  with the frequency specified by the parameter f a our evaluation uses the following metrics  error rate  %   to measure accuracy  we compare the inference results with the ground truth and compute the error rate  f-measure  for change point detection  we evaluate the accuracy of the reported changes  we use precision to capture the percentage of reported changes that are consistent with the ground truth  and recall to capture the percentage of changes in the ground truth that are reported by our algorithm  we combine them into f -measure = 2 precision recall/  precision + recall   running cost  we report the time taken to evaluate a trace using a single-threaded implementation running on a server with an intel xeon 3ghz cpu and running java hotspot 64-bit server vm 1.6 with maximum heap size 1.5gb  c.2 containment  h  containment  cr  8 containment  w1200  error rate  %  value  s  used 1  10 1 every 60 seconds 5 20  0.6  1   default 0.8  0.2  0.8   default 0.5 1 every second 1 every 10 seconds 1 every 10  120 seconds error rate  %  parameter number of warehouses  n  frequency of pallet injection  fixed  cases per pallet  fixed  items per case  fixed  main read rate of readers  rr  overlap rate for shelf readers  or  non-shelf reader frequency  fixed  shelf reader frequency  fixed  frequency of anomalies  f a  10 containment location 8 c.3 alternative method for comparison we describe the design of the s murf method used as a baseline for comparison in both our lab experiment and simulations  this method first uses s murf  11  to smooth raw readings of objects to estimate their locations individually  the adaptive window used in s murf is further stored for containment inference and change detection  within the adaptive window for each item  at a particular time t  if the most frequently co-located case before time t is the same as that after time t  then there is no containment change  and the most frequently co-located case is chosen to be the true container  otherwise  we further check if none of the top-k co-located cases before time t is in the set of top-k co-located cases after t if so  we report a containment change for this item at time t  and pick the case that is most co-located with the item in the period from t to the present  note that the second check is needed because due to the missing readings  the real container may not be the most frequently co-located case  so the case that belongs to both top-k sets between and after t could be the true container  c.4 single-site inference we describe several additional results for our inference methods when run on a single warehouse  basic inference algorithm  we first evaluate the basic algorithm presented in section 3.2 for its sensitivity to various noise factors  read rate  we began with short 1500-second traces and ran inference with all the readings obtained thus far  we first varied the read rate rr from 0.6 to 1 figure 6  a  shows the inference error rates  location inference is highly accurate  with the error rate less than 0.5 % for all read rates  containment inference is more sensitive to the read rate but still achieves an error rate less than 7 % for the 0.6 read rate  the sensitivity of containment inference to the read rate is due to its use of co-location information  the chance of reading both an item and its container reduces quadratically with the read rate  fortunately  the use of history alleviates the problem lab rfid deployment to evaluate our system in real-world settings  we developed an rfid lab with 2 thingmagic mercury5 readers connected to 7 circularly-polarized antennas  20 cases containing 5 items each  and alien squiggle gen 2 class 1 tags attached to all cases and items  we used the 7 antennas to implement 1 entry reader  1 belt reader  4 shelf readers  and 1 exit reader  cases with contained items transitioned through the readers in that order  receiving 5 interrogations from each nonshelf reader and dozens from a shelf reader  the shelf readers had overlapping read ranges as they were placed close to each other  using our lab setup  we created 8 traces with distinct characteristics  by varying the environmental noise  overlap among readers  and tag orientations  336 table 3  f-measures  %  of containment change detection using dif table 4  f-measures  %  and time costs  sec  of change point detection ferent values and our offline method  threshold 10 20 30 40 50 60 70 rr=0.6 64 71 75 85 89 89 87 rr=0.7 85 88 90 92 94 96 94 rr=0.8 92 97 98 97 97 97 96 rr=0.9 83 98 97 97 97 97 97 with different recent history sizes  h  for different read rates  rr   recent history size h 300 400 500 600 700 800 900 f-m  %  46 67 81 87 88 86 92 rr=0.6 time  s  205 235 293 385 418 497 556 f-m  %  72 91 90 90 93 93 94 rr=0.7 time  s  182 229 288 344 403 469 523 f-m  %  73 92 94 97 93 95 96 rr=0.8 time  s  182 220 283 341 395 446 490 f-m  %  82 94 95 98 93 95 96 rr=0.9 time  s  172 207 258 322 381 436 458 80 87 93 95 96 90 87 91 95 96 100 87 90 93 95 and keeps the error rate low  the high accuracy of location inference is due to the effect of smoothing over containment  once we understand containment correctly  the location of an object can be revealed by the readings of any other object  s  in the container  overlap rate  we also varied the overlap rate or from 0.2 to 0.8 while fixing the read rate at 0.7 results show that neither location nor containment inference is sensitive to the overlap rate  the error rate of containment inference is flat at 2.3 % and that of location inference is flat at 0.08 %  this is because overall an object is read more by the reader closest to it than by other readers farther away  container capacity  we also varied the container capacity from 5 to 100 items while keeping the read rate rr and overlap rate or at default values  the inference accuracy remains the same as the above with 20 items per container  this is mainly because for a given item  we calculate the weight of this item with its candidate container based on the co-location history ; the weight calculation remains the same regardless of other items in the container  hence  our inference algorithm is not sensitive to the container capacity  history truncation method  in addition to the total inference time that we presented in figure 5  b   we also computed the error rates of the three methods  figure 6  b  shows the error rates of containment inference using the above methods  the error rates of location inference are always low in the 0.05 % to 1 % range  hence omitted   again  we observe the naive window-based truncation to be inaccurate  its error rate increases for longer traces because our simulation generates the belt readings useful for containment inference in the first half of the warehouse setup  in contrast  using the full history or the cr method makes inference not very sensitive to the trace length  with the cr method being somewhat better due to the elimination of noisy data from inference  containment change detection  we first examine the effect of the threshold for change point detection  we consider fixed values in a range as well as our offline method as described in section 3.3 we created traces with varied read rates  table 3 shows the f-measure for these traces as takes various fixed values  the bold numbers indicate the f-measures of the chosen by our offline sampling algorithm  as can be seen  the best fixed threshold that gives the optimal f-measure varies across traces  our chosen threshold always approximates the optimal value within 2 % across all read rates  we further study the tradeoff between accuracy and efficiency  the change point detection algorithm requires a recent history  whose size is h   besides the critical region in the past  to detect containment changes  since we run inference with the default frequency of once every 300 seconds  h has to be at least 300 seconds to include all new observations for inference  however  a longer recent history may be needed to ensure accuracy of change point detection while it may also increase inference cost  we next vary h to study such tradeoff between accuracy and efficiency  we created traces with varied read rates and for each trace  measured f-measures and time costs as different h values were used  table 4 shows the results  the overall trend with all traces is that as h increases  the f-measure improves but the time cost also increases  among different read rates  we see that achieving high accuracy for lower read rates such as 0.6 requires larger sizes of h  while the time cost varies with h consistently across all read rates  this implies two ways to trade off accuracy and efficiency   1  if the application requirement is to achieve best accuracy while keeping up with stream speed  we should use the h sizes that yield the time costs in bold in the table because they allow inference to complete within 300 seconds  the interval before the next inference starts  this way  our algorithm offers above 90 % accuracy for read rates 0.7 and above 80 % for the read rate = 0.6   2  if the requirement is to optimize performance while satisfying certain accuracy  say  85 %  we should use the h sizes that yield the f-measures in bold in the table  in particular  when the read rate is high  we can use a smaller size of h to reduce the time cost and enable more frequent inference  e.g  for read rate=0.8  running inference every 212 seconds can keep up with the stream speed   overall  for the common read rates between 0.7 and 0.9  the h size of 500 seconds achieves over 90 % accuracy while running at stream speed  summary   i  our inference algorithm is highly accurate for various noisy traces with stable containment  7 % error rate for containment inference and around 0.5 % for location inference   the critical region method can significantly reduce inference cost for long traces while further improving accuracy   ii  our results are stable for various read rates  overlap rates between readers  container capacities  and history lengths  with change point detection  our algorithm can achieve 85 % accuracy given read rates 0.7 while keeping up with stream speed  by using a relatively small recent history size   these results are confirmed using both real lab traces with various noise factors and simulations with different containment change frequencies  c.5 distributed inference we highlight the communication costs of the distributed and centralized approaches in the table below  while omitting other results in the interest of space   for the centralized approach  we assume that all raw data is shipped to a central location for inference with simple gzip compression of data  as can be seen  our cr methods offer 3 orders of magnitude reduction in communication costs  table 5  communication costs  bytes  of a centralized approach and three state migration methods for distributed inference  centralized none cr rr=0.6 125,895,500 0 225,890 rr=0.7 145,858,950 0 223,790 rr=0.8 166,746,235 0 225,890 rr=0.9 187,589,810 0 225,890 summary  our results show that distributed inference using the cr methods have accuracy that is close to the centralized approach  while incurring significantly lower communication costs  our scalability results  in section 5.3  show that our distributed inference system can scale to millions of objects over multiple sites while keeping up with the speed of rfid streams at each site  337 